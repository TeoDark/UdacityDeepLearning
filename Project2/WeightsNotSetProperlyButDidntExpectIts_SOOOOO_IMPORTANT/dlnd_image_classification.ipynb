{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 1 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 9 Name: truck\n",
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 6 Max Value: 202\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n",
      "\n",
      "Stats of batch 3:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n",
      "First 20 Labels: [8, 5, 0, 6, 9, 2, 8, 3, 6, 2, 7, 4, 6, 9, 0, 0, 7, 3, 7, 2]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 6 Max Value: 250\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 0 Name: airplane\n",
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 7 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n",
      "\n",
      "Stats of batch 5:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1014, 1: 1014, 2: 952, 3: 1016, 4: 997, 5: 1025, 6: 980, 7: 977, 8: 1003, 9: 1022}\n",
      "First 20 Labels: [1, 8, 5, 1, 5, 7, 4, 3, 8, 2, 7, 2, 0, 1, 5, 9, 6, 2, 0, 8]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 5 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f960e453780>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 14\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id+1, sample_id)\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id+2, sample_id)\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id+3, sample_id)\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id+4, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function - hurray for np.arrays easy deviding by sclar!\n",
    "    return x / 255.0\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#why it's called map? I was lost for few moments becouse I thought I forget about something.\n",
    "#It's ouside just to prevent encoding badly becouse some label might not be in input x?\n",
    "soCalledMap = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function - implementation from One-Hot Encoding lesson\n",
    "    labels = np.array(soCalledMap)\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(soCalledMap)\n",
    "    return lb.transform(x)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #x = tf.placeholder(tf.float32, shape=[None, image_shape[0],image_shape[1],image_shape[2]], name=\"x\")\n",
    "    x = tf.placeholder(tf.float32, shape=([None]+(list(image_shape))), name='x') # can i do it easier?\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(tf.float32, shape= [None , n_classes], name='y')\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    \n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer (2,2)\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution (4,4)\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool (2,2)\n",
    "    :param pool_strides: Stride 2-D Tuple for pool (2,2)\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply Max Pooling - based on example from \"TensorFlow Convolution Layer\" lesson \n",
    "    color_channels = x_tensor.get_shape().as_list()[3] #why tere was not mentioning this? x_tensor.get_shape()[3] wont do :<\n",
    "    filter_size_width = conv_ksize[0] \n",
    "    filter_size_height = conv_ksize[1] \n",
    "    strideArray = [filter_size_height, filter_size_width, color_channels, conv_num_outputs]\n",
    "\n",
    "    weight = tf.Variable(tf.truncated_normal(strideArray))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "    conv_layerConvolution = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv_layerBiased = tf.nn.bias_add(conv_layerConvolution, bias)\n",
    "    conv_layerActivated = tf.nn.relu(conv_layerBiased)\n",
    "\n",
    "    # Apply Max Pooling - based on example from \"TensorFlow Max Pooling\" lesson \n",
    "    # conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    ksizeArray = [1,pool_ksize[0],pool_ksize[1],1]\n",
    "    kStridnesArray = [1,pool_strides[0],pool_strides[1],1]\n",
    "    maxPoolingLayer=tf.nn.max_pool(conv_layerActivated,ksizeArray,kStridnesArray,padding='SAME')\n",
    "        \n",
    "    return maxPoolingLayer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    size = (x_tensor.get_shape().as_list())\n",
    "    #batchSize = size[0]   -why using that causes error? I have no idea - just used -1 as suggeted in: https://discussions.udacity.com/t/shape-when-flattening-the-tensor-from-4d-to-2d/225217/8 \n",
    "    # BUT EXPLANATION WOULD BE GREAT!\n",
    "    flattenedSize=1\n",
    "    for i in size[1:]:\n",
    "        flattenedSize*=i\n",
    "    \n",
    "    return tf.reshape(x_tensor,[-1,flattenedSize])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    inputSize = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable(tf.truncated_normal([inputSize, num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    fullConnetedLayer = tf.add(tf.matmul(x_tensor,weight),bias)\n",
    "    \n",
    "    #I applied activation because in next one we sould not and I dont see any more diffrences\n",
    "    fullConnetedLayerActivated = tf.nn.relu(fullConnetedLayer)\n",
    "    \n",
    "    return fullConnetedLayerActivated\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    inputSize = x_tensor.get_shape().as_list()[1]\n",
    "    weight = tf.Variable(tf.truncated_normal([inputSize, num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    fullConnetedLayer = tf.add(tf.matmul(x_tensor,weight),bias)\n",
    "    return fullConnetedLayer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 80 #just guessing, shoud it not be: [(input_height - filter_height + 2 * P)/S + 1]^2 * D ?\n",
    "    conv_ksize = (2,2)\n",
    "    conv_strides =(4,4)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    \n",
    "    \n",
    "    firstConvolution = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #secondConvolution = conv2d_maxpool(firstConvolution, 128, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    #thirdConvolution = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "\n",
    "    flattened = flatten(firstConvolution)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    firstFullyConnected = fully_conn(flattened, 60)\n",
    "    firstFullyConnectedWithDropOut = tf.nn.dropout(firstFullyConnected, keep_prob)\n",
    "    \n",
    "    #secondFullyConnected = fully_conn(firstFullyConnectedWithDropOut, 10)\n",
    "    #secondFullyConnectedWithDropOut = tf.nn.dropout(secondFullyConnected, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    outputLeyer = output(firstFullyConnectedWithDropOut, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return outputLeyer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss, acc  = session.run([cost, accuracy], feed_dict={x: valid_features,y: valid_labels, keep_prob: 1.0})\n",
    "    print('loss:',loss,'accuracy:',acc)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 700\n",
    "batch_size = 514\n",
    "keep_probability = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 87.9266 accuracy: 0.1358\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 52.8386 accuracy: 0.1272\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 45.8051 accuracy: 0.1298\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 41.5538 accuracy: 0.1348\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 38.2574 accuracy: 0.1494\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 35.5159 accuracy: 0.1572\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 33.1521 accuracy: 0.1608\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 31.1344 accuracy: 0.1668\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 29.3759 accuracy: 0.1726\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 27.7957 accuracy: 0.182\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 26.3104 accuracy: 0.1888\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 24.9347 accuracy: 0.1942\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 23.6971 accuracy: 0.1974\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 22.5654 accuracy: 0.203\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 21.5279 accuracy: 0.2078\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 20.5548 accuracy: 0.2166\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 19.6735 accuracy: 0.2238\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 18.834 accuracy: 0.2252\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 17.9587 accuracy: 0.2278\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 17.1529 accuracy: 0.2322\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 16.4092 accuracy: 0.2376\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 15.7079 accuracy: 0.235\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 15.0961 accuracy: 0.2388\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 14.5686 accuracy: 0.236\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 14.0969 accuracy: 0.2348\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 13.6839 accuracy: 0.2358\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 13.3024 accuracy: 0.2404\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 12.948 accuracy: 0.244\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 12.6013 accuracy: 0.2476\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 12.2828 accuracy: 0.2484\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 11.9845 accuracy: 0.2516\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 11.7028 accuracy: 0.252\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 11.4443 accuracy: 0.2556\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 11.1924 accuracy: 0.2594\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 10.9511 accuracy: 0.2598\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 10.7229 accuracy: 0.2622\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 10.5027 accuracy: 0.2624\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 10.2908 accuracy: 0.265\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 10.0875 accuracy: 0.2666\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 9.89565 accuracy: 0.2694\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 9.71443 accuracy: 0.2704\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 9.54153 accuracy: 0.2696\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 9.37106 accuracy: 0.2714\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 9.20948 accuracy: 0.274\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 9.05202 accuracy: 0.2748\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 8.89346 accuracy: 0.275\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 8.74317 accuracy: 0.2736\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 8.60167 accuracy: 0.2758\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 8.45719 accuracy: 0.2784\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 8.31614 accuracy: 0.2788\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 8.18094 accuracy: 0.2798\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 8.04654 accuracy: 0.2818\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 7.91913 accuracy: 0.2834\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 7.79713 accuracy: 0.2838\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 7.68491 accuracy: 0.2848\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 7.57471 accuracy: 0.2844\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 7.47146 accuracy: 0.2864\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 7.37037 accuracy: 0.2872\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 7.27465 accuracy: 0.2884\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 7.17701 accuracy: 0.2892\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 7.08343 accuracy: 0.2906\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 6.99392 accuracy: 0.2912\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 6.90976 accuracy: 0.2918\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 6.82904 accuracy: 0.2918\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 6.74491 accuracy: 0.2908\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 6.6692 accuracy: 0.2922\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 6.59226 accuracy: 0.2926\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 6.51624 accuracy: 0.2942\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 6.43819 accuracy: 0.2948\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 6.36523 accuracy: 0.2936\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 6.29744 accuracy: 0.296\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 6.22483 accuracy: 0.2972\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 6.15753 accuracy: 0.2956\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 6.08885 accuracy: 0.2972\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 6.02145 accuracy: 0.2978\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 5.95932 accuracy: 0.299\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 5.89604 accuracy: 0.3\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 5.83548 accuracy: 0.2996\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 5.78103 accuracy: 0.3014\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 5.71889 accuracy: 0.3024\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 5.65829 accuracy: 0.305\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 5.60704 accuracy: 0.3048\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 5.55385 accuracy: 0.3066\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 5.50167 accuracy: 0.307\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 5.44841 accuracy: 0.307\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 5.39938 accuracy: 0.3082\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 5.35119 accuracy: 0.3098\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 5.30431 accuracy: 0.311\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 5.25877 accuracy: 0.3124\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 5.21196 accuracy: 0.3132\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 5.16951 accuracy: 0.3142\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 5.12362 accuracy: 0.3136\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 5.08117 accuracy: 0.3126\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 5.0387 accuracy: 0.3132\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 4.99669 accuracy: 0.315\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 4.95551 accuracy: 0.3158\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 4.91624 accuracy: 0.3164\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 4.87704 accuracy: 0.3166\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 4.83917 accuracy: 0.3164\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 4.802 accuracy: 0.315\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 4.76456 accuracy: 0.3158\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 4.72788 accuracy: 0.3154\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 4.69116 accuracy: 0.3154\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 4.65581 accuracy: 0.315\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 4.62218 accuracy: 0.3142\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 4.58536 accuracy: 0.3146\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 4.55219 accuracy: 0.3154\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 4.52114 accuracy: 0.3154\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 4.48298 accuracy: 0.3156\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 4.45091 accuracy: 0.3166\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss: 4.41894 accuracy: 0.3176\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss: 4.39151 accuracy: 0.3168\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss: 4.36116 accuracy: 0.3168\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss: 4.33072 accuracy: 0.3168\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss: 4.30075 accuracy: 0.3178\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss: 4.27457 accuracy: 0.3174\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss: 4.24411 accuracy: 0.318\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss: 4.22138 accuracy: 0.3188\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss: 4.19458 accuracy: 0.3192\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss: 4.16496 accuracy: 0.3192\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss: 4.13947 accuracy: 0.3218\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss: 4.11422 accuracy: 0.3228\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss: 4.09113 accuracy: 0.324\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss: 4.0644 accuracy: 0.325\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss: 4.03995 accuracy: 0.3256\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss: 4.01571 accuracy: 0.326\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss: 3.99268 accuracy: 0.3272\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss: 3.96779 accuracy: 0.3278\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss: 3.94468 accuracy: 0.329\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss: 3.92054 accuracy: 0.33\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss: 3.90209 accuracy: 0.3302\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss: 3.87965 accuracy: 0.3318\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss: 3.8568 accuracy: 0.3318\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss: 3.8365 accuracy: 0.3322\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss: 3.81434 accuracy: 0.334\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss: 3.79839 accuracy: 0.3326\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss: 3.77935 accuracy: 0.3332\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss: 3.75679 accuracy: 0.3328\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss: 3.73838 accuracy: 0.3322\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss: 3.7219 accuracy: 0.3328\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss: 3.70107 accuracy: 0.3334\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss: 3.68408 accuracy: 0.3344\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss: 3.66808 accuracy: 0.3342\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss: 3.64847 accuracy: 0.3352\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss: 3.63316 accuracy: 0.3354\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss: 3.61399 accuracy: 0.3352\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss: 3.59751 accuracy: 0.3362\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss: 3.58145 accuracy: 0.3364\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss: 3.562 accuracy: 0.3358\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss: 3.55009 accuracy: 0.3372\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss: 3.53785 accuracy: 0.3378\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss: 3.52061 accuracy: 0.3386\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss: 3.50352 accuracy: 0.3378\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss: 3.48786 accuracy: 0.3386\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss: 3.47194 accuracy: 0.3392\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss: 3.45649 accuracy: 0.3402\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss: 3.44375 accuracy: 0.3414\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss: 3.43114 accuracy: 0.3406\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss: 3.41616 accuracy: 0.3418\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss: 3.40111 accuracy: 0.3428\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss: 3.38924 accuracy: 0.3434\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss: 3.37581 accuracy: 0.3438\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss: 3.36229 accuracy: 0.3444\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss: 3.34834 accuracy: 0.344\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss: 3.33331 accuracy: 0.3446\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss: 3.31998 accuracy: 0.3446\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss: 3.30754 accuracy: 0.3448\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss: 3.29655 accuracy: 0.3454\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss: 3.28273 accuracy: 0.345\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss: 3.27203 accuracy: 0.3448\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss: 3.25985 accuracy: 0.3448\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss: 3.24812 accuracy: 0.3448\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss: 3.23687 accuracy: 0.3456\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss: 3.22724 accuracy: 0.3444\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss: 3.21242 accuracy: 0.346\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss: 3.19815 accuracy: 0.3462\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss: 3.19113 accuracy: 0.3464\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss: 3.17804 accuracy: 0.3462\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss: 3.16868 accuracy: 0.3466\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss: 3.15937 accuracy: 0.3466\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss: 3.14675 accuracy: 0.3472\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss: 3.1409 accuracy: 0.3484\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss: 3.12689 accuracy: 0.348\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss: 3.11756 accuracy: 0.3496\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss: 3.10665 accuracy: 0.35\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss: 3.09729 accuracy: 0.351\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss: 3.085 accuracy: 0.3502\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss: 3.07579 accuracy: 0.3516\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss: 3.06751 accuracy: 0.3512\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss: 3.06282 accuracy: 0.3512\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss: 3.05125 accuracy: 0.352\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss: 3.04573 accuracy: 0.3514\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss: 3.03614 accuracy: 0.3528\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss: 3.02706 accuracy: 0.3528\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss: 3.01812 accuracy: 0.3528\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss: 3.01055 accuracy: 0.353\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss: 2.99895 accuracy: 0.3534\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss: 2.99256 accuracy: 0.3532\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss: 2.98175 accuracy: 0.353\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss: 2.97699 accuracy: 0.355\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss: 2.96999 accuracy: 0.356\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss: 2.96204 accuracy: 0.3558\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss: 2.95415 accuracy: 0.357\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss: 2.9459 accuracy: 0.3568\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss: 2.94183 accuracy: 0.3564\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss: 2.92884 accuracy: 0.3566\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss: 2.92152 accuracy: 0.3584\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss: 2.91023 accuracy: 0.3584\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss: 2.90919 accuracy: 0.359\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss: 2.90498 accuracy: 0.3584\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss: 2.89884 accuracy: 0.3586\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss: 2.88743 accuracy: 0.3592\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss: 2.88029 accuracy: 0.3608\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss: 2.87074 accuracy: 0.3608\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss: 2.86409 accuracy: 0.3606\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss: 2.85972 accuracy: 0.3616\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss: 2.85375 accuracy: 0.3604\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss: 2.84617 accuracy: 0.3612\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss: 2.84333 accuracy: 0.3586\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss: 2.83924 accuracy: 0.3612\n",
      "Epoch 221, CIFAR-10 Batch 1:  loss: 2.83206 accuracy: 0.3624\n",
      "Epoch 222, CIFAR-10 Batch 1:  loss: 2.82918 accuracy: 0.3616\n",
      "Epoch 223, CIFAR-10 Batch 1:  loss: 2.8202 accuracy: 0.364\n",
      "Epoch 224, CIFAR-10 Batch 1:  loss: 2.81294 accuracy: 0.364\n",
      "Epoch 225, CIFAR-10 Batch 1:  loss: 2.80547 accuracy: 0.3642\n",
      "Epoch 226, CIFAR-10 Batch 1:  loss: 2.79885 accuracy: 0.3636\n",
      "Epoch 227, CIFAR-10 Batch 1:  loss: 2.79236 accuracy: 0.3642\n",
      "Epoch 228, CIFAR-10 Batch 1:  loss: 2.7879 accuracy: 0.3644\n",
      "Epoch 229, CIFAR-10 Batch 1:  loss: 2.7829 accuracy: 0.3644\n",
      "Epoch 230, CIFAR-10 Batch 1:  loss: 2.77693 accuracy: 0.3644\n",
      "Epoch 231, CIFAR-10 Batch 1:  loss: 2.77434 accuracy: 0.364\n",
      "Epoch 232, CIFAR-10 Batch 1:  loss: 2.77016 accuracy: 0.365\n",
      "Epoch 233, CIFAR-10 Batch 1:  loss: 2.76189 accuracy: 0.3648\n",
      "Epoch 234, CIFAR-10 Batch 1:  loss: 2.7573 accuracy: 0.3642\n",
      "Epoch 235, CIFAR-10 Batch 1:  loss: 2.75232 accuracy: 0.366\n",
      "Epoch 236, CIFAR-10 Batch 1:  loss: 2.74933 accuracy: 0.3674\n",
      "Epoch 237, CIFAR-10 Batch 1:  loss: 2.74645 accuracy: 0.367\n",
      "Epoch 238, CIFAR-10 Batch 1:  loss: 2.74281 accuracy: 0.366\n",
      "Epoch 239, CIFAR-10 Batch 1:  loss: 2.735 accuracy: 0.366\n",
      "Epoch 240, CIFAR-10 Batch 1:  loss: 2.73194 accuracy: 0.3676\n",
      "Epoch 241, CIFAR-10 Batch 1:  loss: 2.72521 accuracy: 0.3668\n",
      "Epoch 242, CIFAR-10 Batch 1:  loss: 2.72096 accuracy: 0.3676\n",
      "Epoch 243, CIFAR-10 Batch 1:  loss: 2.71614 accuracy: 0.3688\n",
      "Epoch 244, CIFAR-10 Batch 1:  loss: 2.71217 accuracy: 0.3682\n",
      "Epoch 245, CIFAR-10 Batch 1:  loss: 2.70929 accuracy: 0.3678\n",
      "Epoch 246, CIFAR-10 Batch 1:  loss: 2.70322 accuracy: 0.3684\n",
      "Epoch 247, CIFAR-10 Batch 1:  loss: 2.69555 accuracy: 0.3688\n",
      "Epoch 248, CIFAR-10 Batch 1:  loss: 2.69248 accuracy: 0.3688\n",
      "Epoch 249, CIFAR-10 Batch 1:  loss: 2.68861 accuracy: 0.37\n",
      "Epoch 250, CIFAR-10 Batch 1:  loss: 2.6847 accuracy: 0.3718\n",
      "Epoch 251, CIFAR-10 Batch 1:  loss: 2.68207 accuracy: 0.3696\n",
      "Epoch 252, CIFAR-10 Batch 1:  loss: 2.67762 accuracy: 0.3684\n",
      "Epoch 253, CIFAR-10 Batch 1:  loss: 2.67831 accuracy: 0.3694\n",
      "Epoch 254, CIFAR-10 Batch 1:  loss: 2.67623 accuracy: 0.3714\n",
      "Epoch 255, CIFAR-10 Batch 1:  loss: 2.67115 accuracy: 0.3724\n",
      "Epoch 256, CIFAR-10 Batch 1:  loss: 2.66659 accuracy: 0.3726\n",
      "Epoch 257, CIFAR-10 Batch 1:  loss: 2.66068 accuracy: 0.3722\n",
      "Epoch 258, CIFAR-10 Batch 1:  loss: 2.65567 accuracy: 0.3726\n",
      "Epoch 259, CIFAR-10 Batch 1:  loss: 2.65261 accuracy: 0.3722\n",
      "Epoch 260, CIFAR-10 Batch 1:  loss: 2.64519 accuracy: 0.3732\n",
      "Epoch 261, CIFAR-10 Batch 1:  loss: 2.63786 accuracy: 0.3734\n",
      "Epoch 262, CIFAR-10 Batch 1:  loss: 2.63309 accuracy: 0.3734\n",
      "Epoch 263, CIFAR-10 Batch 1:  loss: 2.63579 accuracy: 0.3736\n",
      "Epoch 264, CIFAR-10 Batch 1:  loss: 2.63641 accuracy: 0.3736\n",
      "Epoch 265, CIFAR-10 Batch 1:  loss: 2.63283 accuracy: 0.374\n",
      "Epoch 266, CIFAR-10 Batch 1:  loss: 2.62622 accuracy: 0.3746\n",
      "Epoch 267, CIFAR-10 Batch 1:  loss: 2.62357 accuracy: 0.3744\n",
      "Epoch 268, CIFAR-10 Batch 1:  loss: 2.62785 accuracy: 0.3758\n",
      "Epoch 269, CIFAR-10 Batch 1:  loss: 2.6215 accuracy: 0.3758\n",
      "Epoch 270, CIFAR-10 Batch 1:  loss: 2.61749 accuracy: 0.3766\n",
      "Epoch 271, CIFAR-10 Batch 1:  loss: 2.61226 accuracy: 0.3778\n",
      "Epoch 272, CIFAR-10 Batch 1:  loss: 2.60474 accuracy: 0.377\n",
      "Epoch 273, CIFAR-10 Batch 1:  loss: 2.60784 accuracy: 0.3768\n",
      "Epoch 274, CIFAR-10 Batch 1:  loss: 2.60416 accuracy: 0.3768\n",
      "Epoch 275, CIFAR-10 Batch 1:  loss: 2.60115 accuracy: 0.376\n",
      "Epoch 276, CIFAR-10 Batch 1:  loss: 2.59748 accuracy: 0.3776\n",
      "Epoch 277, CIFAR-10 Batch 1:  loss: 2.59126 accuracy: 0.3786\n",
      "Epoch 278, CIFAR-10 Batch 1:  loss: 2.59073 accuracy: 0.3796\n",
      "Epoch 279, CIFAR-10 Batch 1:  loss: 2.5873 accuracy: 0.3784\n",
      "Epoch 280, CIFAR-10 Batch 1:  loss: 2.58196 accuracy: 0.3768\n",
      "Epoch 281, CIFAR-10 Batch 1:  loss: 2.58212 accuracy: 0.3776\n",
      "Epoch 282, CIFAR-10 Batch 1:  loss: 2.57792 accuracy: 0.3816\n",
      "Epoch 283, CIFAR-10 Batch 1:  loss: 2.57672 accuracy: 0.3798\n",
      "Epoch 284, CIFAR-10 Batch 1:  loss: 2.57583 accuracy: 0.3806\n",
      "Epoch 285, CIFAR-10 Batch 1:  loss: 2.57195 accuracy: 0.3802\n",
      "Epoch 286, CIFAR-10 Batch 1:  loss: 2.56956 accuracy: 0.382\n",
      "Epoch 287, CIFAR-10 Batch 1:  loss: 2.56775 accuracy: 0.3808\n",
      "Epoch 288, CIFAR-10 Batch 1:  loss: 2.56446 accuracy: 0.3806\n",
      "Epoch 289, CIFAR-10 Batch 1:  loss: 2.5573 accuracy: 0.3816\n",
      "Epoch 290, CIFAR-10 Batch 1:  loss: 2.56449 accuracy: 0.3804\n",
      "Epoch 291, CIFAR-10 Batch 1:  loss: 2.56479 accuracy: 0.3788\n",
      "Epoch 292, CIFAR-10 Batch 1:  loss: 2.56223 accuracy: 0.38\n",
      "Epoch 293, CIFAR-10 Batch 1:  loss: 2.5604 accuracy: 0.3802\n",
      "Epoch 294, CIFAR-10 Batch 1:  loss: 2.55904 accuracy: 0.3798\n",
      "Epoch 295, CIFAR-10 Batch 1:  loss: 2.56489 accuracy: 0.3802\n",
      "Epoch 296, CIFAR-10 Batch 1:  loss: 2.56172 accuracy: 0.3802\n",
      "Epoch 297, CIFAR-10 Batch 1:  loss: 2.55676 accuracy: 0.3826\n",
      "Epoch 298, CIFAR-10 Batch 1:  loss: 2.5522 accuracy: 0.3834\n",
      "Epoch 299, CIFAR-10 Batch 1:  loss: 2.54551 accuracy: 0.3836\n",
      "Epoch 300, CIFAR-10 Batch 1:  loss: 2.53603 accuracy: 0.3842\n",
      "Epoch 301, CIFAR-10 Batch 1:  loss: 2.53184 accuracy: 0.384\n",
      "Epoch 302, CIFAR-10 Batch 1:  loss: 2.53046 accuracy: 0.3842\n",
      "Epoch 303, CIFAR-10 Batch 1:  loss: 2.52924 accuracy: 0.3834\n",
      "Epoch 304, CIFAR-10 Batch 1:  loss: 2.53202 accuracy: 0.3826\n",
      "Epoch 305, CIFAR-10 Batch 1:  loss: 2.53816 accuracy: 0.384\n",
      "Epoch 306, CIFAR-10 Batch 1:  loss: 2.53492 accuracy: 0.383\n",
      "Epoch 307, CIFAR-10 Batch 1:  loss: 2.53226 accuracy: 0.3836\n",
      "Epoch 308, CIFAR-10 Batch 1:  loss: 2.53898 accuracy: 0.3838\n",
      "Epoch 309, CIFAR-10 Batch 1:  loss: 2.53258 accuracy: 0.3848\n",
      "Epoch 310, CIFAR-10 Batch 1:  loss: 2.53467 accuracy: 0.3856\n",
      "Epoch 311, CIFAR-10 Batch 1:  loss: 2.53037 accuracy: 0.3858\n",
      "Epoch 312, CIFAR-10 Batch 1:  loss: 2.52306 accuracy: 0.3888\n",
      "Epoch 313, CIFAR-10 Batch 1:  loss: 2.52003 accuracy: 0.387\n",
      "Epoch 314, CIFAR-10 Batch 1:  loss: 2.51385 accuracy: 0.3878\n",
      "Epoch 315, CIFAR-10 Batch 1:  loss: 2.51484 accuracy: 0.3874\n",
      "Epoch 316, CIFAR-10 Batch 1:  loss: 2.51602 accuracy: 0.3886\n",
      "Epoch 317, CIFAR-10 Batch 1:  loss: 2.51672 accuracy: 0.3892\n",
      "Epoch 318, CIFAR-10 Batch 1:  loss: 2.51728 accuracy: 0.3874\n",
      "Epoch 319, CIFAR-10 Batch 1:  loss: 2.52517 accuracy: 0.3882\n",
      "Epoch 320, CIFAR-10 Batch 1:  loss: 2.52215 accuracy: 0.3886\n",
      "Epoch 321, CIFAR-10 Batch 1:  loss: 2.51951 accuracy: 0.388\n",
      "Epoch 322, CIFAR-10 Batch 1:  loss: 2.51753 accuracy: 0.3896\n",
      "Epoch 323, CIFAR-10 Batch 1:  loss: 2.51423 accuracy: 0.3894\n",
      "Epoch 324, CIFAR-10 Batch 1:  loss: 2.51005 accuracy: 0.3916\n",
      "Epoch 325, CIFAR-10 Batch 1:  loss: 2.50671 accuracy: 0.391\n",
      "Epoch 326, CIFAR-10 Batch 1:  loss: 2.50702 accuracy: 0.3904\n",
      "Epoch 327, CIFAR-10 Batch 1:  loss: 2.50123 accuracy: 0.3906\n",
      "Epoch 328, CIFAR-10 Batch 1:  loss: 2.49823 accuracy: 0.3898\n",
      "Epoch 329, CIFAR-10 Batch 1:  loss: 2.49135 accuracy: 0.3906\n",
      "Epoch 330, CIFAR-10 Batch 1:  loss: 2.49722 accuracy: 0.389\n",
      "Epoch 331, CIFAR-10 Batch 1:  loss: 2.50835 accuracy: 0.3904\n",
      "Epoch 332, CIFAR-10 Batch 1:  loss: 2.50639 accuracy: 0.3892\n",
      "Epoch 333, CIFAR-10 Batch 1:  loss: 2.50668 accuracy: 0.3906\n",
      "Epoch 334, CIFAR-10 Batch 1:  loss: 2.5124 accuracy: 0.3902\n",
      "Epoch 335, CIFAR-10 Batch 1:  loss: 2.50201 accuracy: 0.3914\n",
      "Epoch 336, CIFAR-10 Batch 1:  loss: 2.50618 accuracy: 0.3906\n",
      "Epoch 337, CIFAR-10 Batch 1:  loss: 2.51018 accuracy: 0.3918\n",
      "Epoch 338, CIFAR-10 Batch 1:  loss: 2.51082 accuracy: 0.3928\n",
      "Epoch 339, CIFAR-10 Batch 1:  loss: 2.50441 accuracy: 0.3932\n",
      "Epoch 340, CIFAR-10 Batch 1:  loss: 2.49877 accuracy: 0.392\n",
      "Epoch 341, CIFAR-10 Batch 1:  loss: 2.49296 accuracy: 0.391\n",
      "Epoch 342, CIFAR-10 Batch 1:  loss: 2.48512 accuracy: 0.3914\n",
      "Epoch 343, CIFAR-10 Batch 1:  loss: 2.48575 accuracy: 0.393\n",
      "Epoch 344, CIFAR-10 Batch 1:  loss: 2.47705 accuracy: 0.3946\n",
      "Epoch 345, CIFAR-10 Batch 1:  loss: 2.48045 accuracy: 0.3962\n",
      "Epoch 346, CIFAR-10 Batch 1:  loss: 2.47837 accuracy: 0.3966\n",
      "Epoch 347, CIFAR-10 Batch 1:  loss: 2.48494 accuracy: 0.3954\n",
      "Epoch 348, CIFAR-10 Batch 1:  loss: 2.48975 accuracy: 0.396\n",
      "Epoch 349, CIFAR-10 Batch 1:  loss: 2.49132 accuracy: 0.3966\n",
      "Epoch 350, CIFAR-10 Batch 1:  loss: 2.49043 accuracy: 0.3966\n",
      "Epoch 351, CIFAR-10 Batch 1:  loss: 2.49614 accuracy: 0.396\n",
      "Epoch 352, CIFAR-10 Batch 1:  loss: 2.49405 accuracy: 0.3978\n",
      "Epoch 353, CIFAR-10 Batch 1:  loss: 2.49727 accuracy: 0.3966\n",
      "Epoch 354, CIFAR-10 Batch 1:  loss: 2.50677 accuracy: 0.3974\n",
      "Epoch 355, CIFAR-10 Batch 1:  loss: 2.51018 accuracy: 0.3982\n",
      "Epoch 356, CIFAR-10 Batch 1:  loss: 2.509 accuracy: 0.3976\n",
      "Epoch 357, CIFAR-10 Batch 1:  loss: 2.50632 accuracy: 0.3984\n",
      "Epoch 358, CIFAR-10 Batch 1:  loss: 2.50355 accuracy: 0.398\n",
      "Epoch 359, CIFAR-10 Batch 1:  loss: 2.49475 accuracy: 0.398\n",
      "Epoch 360, CIFAR-10 Batch 1:  loss: 2.49331 accuracy: 0.398\n",
      "Epoch 361, CIFAR-10 Batch 1:  loss: 2.4947 accuracy: 0.3972\n",
      "Epoch 362, CIFAR-10 Batch 1:  loss: 2.48961 accuracy: 0.3986\n",
      "Epoch 363, CIFAR-10 Batch 1:  loss: 2.48241 accuracy: 0.399\n",
      "Epoch 364, CIFAR-10 Batch 1:  loss: 2.49209 accuracy: 0.3992\n",
      "Epoch 365, CIFAR-10 Batch 1:  loss: 2.49147 accuracy: 0.399\n",
      "Epoch 366, CIFAR-10 Batch 1:  loss: 2.49428 accuracy: 0.399\n",
      "Epoch 367, CIFAR-10 Batch 1:  loss: 2.49449 accuracy: 0.401\n",
      "Epoch 368, CIFAR-10 Batch 1:  loss: 2.49147 accuracy: 0.3984\n",
      "Epoch 369, CIFAR-10 Batch 1:  loss: 2.49303 accuracy: 0.4\n",
      "Epoch 370, CIFAR-10 Batch 1:  loss: 2.47761 accuracy: 0.3992\n",
      "Epoch 371, CIFAR-10 Batch 1:  loss: 2.48475 accuracy: 0.4\n",
      "Epoch 372, CIFAR-10 Batch 1:  loss: 2.48677 accuracy: 0.4002\n",
      "Epoch 373, CIFAR-10 Batch 1:  loss: 2.4896 accuracy: 0.4004\n",
      "Epoch 374, CIFAR-10 Batch 1:  loss: 2.49173 accuracy: 0.3976\n",
      "Epoch 375, CIFAR-10 Batch 1:  loss: 2.49476 accuracy: 0.397\n",
      "Epoch 376, CIFAR-10 Batch 1:  loss: 2.50306 accuracy: 0.3982\n",
      "Epoch 377, CIFAR-10 Batch 1:  loss: 2.50717 accuracy: 0.3992\n",
      "Epoch 378, CIFAR-10 Batch 1:  loss: 2.51064 accuracy: 0.4004\n",
      "Epoch 379, CIFAR-10 Batch 1:  loss: 2.51487 accuracy: 0.4014\n",
      "Epoch 380, CIFAR-10 Batch 1:  loss: 2.52144 accuracy: 0.4004\n",
      "Epoch 381, CIFAR-10 Batch 1:  loss: 2.52762 accuracy: 0.4038\n",
      "Epoch 382, CIFAR-10 Batch 1:  loss: 2.51944 accuracy: 0.4034\n",
      "Epoch 383, CIFAR-10 Batch 1:  loss: 2.51447 accuracy: 0.402\n",
      "Epoch 384, CIFAR-10 Batch 1:  loss: 2.50803 accuracy: 0.4012\n",
      "Epoch 385, CIFAR-10 Batch 1:  loss: 2.50359 accuracy: 0.4018\n",
      "Epoch 386, CIFAR-10 Batch 1:  loss: 2.50373 accuracy: 0.4024\n",
      "Epoch 387, CIFAR-10 Batch 1:  loss: 2.49463 accuracy: 0.4014\n",
      "Epoch 388, CIFAR-10 Batch 1:  loss: 2.49327 accuracy: 0.4034\n",
      "Epoch 389, CIFAR-10 Batch 1:  loss: 2.49734 accuracy: 0.4032\n",
      "Epoch 390, CIFAR-10 Batch 1:  loss: 2.49227 accuracy: 0.4032\n",
      "Epoch 391, CIFAR-10 Batch 1:  loss: 2.50298 accuracy: 0.4026\n",
      "Epoch 392, CIFAR-10 Batch 1:  loss: 2.5007 accuracy: 0.403\n",
      "Epoch 393, CIFAR-10 Batch 1:  loss: 2.50251 accuracy: 0.4028\n",
      "Epoch 394, CIFAR-10 Batch 1:  loss: 2.51052 accuracy: 0.4042\n",
      "Epoch 395, CIFAR-10 Batch 1:  loss: 2.51739 accuracy: 0.4008\n",
      "Epoch 396, CIFAR-10 Batch 1:  loss: 2.51908 accuracy: 0.4006\n",
      "Epoch 397, CIFAR-10 Batch 1:  loss: 2.52053 accuracy: 0.402\n",
      "Epoch 398, CIFAR-10 Batch 1:  loss: 2.52185 accuracy: 0.402\n",
      "Epoch 399, CIFAR-10 Batch 1:  loss: 2.53664 accuracy: 0.4026\n",
      "Epoch 400, CIFAR-10 Batch 1:  loss: 2.54079 accuracy: 0.403\n",
      "Epoch 401, CIFAR-10 Batch 1:  loss: 2.53104 accuracy: 0.403\n",
      "Epoch 402, CIFAR-10 Batch 1:  loss: 2.52724 accuracy: 0.4048\n",
      "Epoch 403, CIFAR-10 Batch 1:  loss: 2.5172 accuracy: 0.4048\n",
      "Epoch 404, CIFAR-10 Batch 1:  loss: 2.51516 accuracy: 0.4054\n",
      "Epoch 405, CIFAR-10 Batch 1:  loss: 2.5197 accuracy: 0.4054\n",
      "Epoch 406, CIFAR-10 Batch 1:  loss: 2.52078 accuracy: 0.4054\n",
      "Epoch 407, CIFAR-10 Batch 1:  loss: 2.52912 accuracy: 0.405\n",
      "Epoch 408, CIFAR-10 Batch 1:  loss: 2.51515 accuracy: 0.4036\n",
      "Epoch 409, CIFAR-10 Batch 1:  loss: 2.52152 accuracy: 0.4038\n",
      "Epoch 410, CIFAR-10 Batch 1:  loss: 2.51804 accuracy: 0.4044\n",
      "Epoch 411, CIFAR-10 Batch 1:  loss: 2.52444 accuracy: 0.4026\n",
      "Epoch 412, CIFAR-10 Batch 1:  loss: 2.51001 accuracy: 0.4048\n",
      "Epoch 413, CIFAR-10 Batch 1:  loss: 2.51701 accuracy: 0.4024\n",
      "Epoch 414, CIFAR-10 Batch 1:  loss: 2.51712 accuracy: 0.403\n",
      "Epoch 415, CIFAR-10 Batch 1:  loss: 2.51821 accuracy: 0.4054\n",
      "Epoch 416, CIFAR-10 Batch 1:  loss: 2.52679 accuracy: 0.4066\n",
      "Epoch 417, CIFAR-10 Batch 1:  loss: 2.53629 accuracy: 0.4054\n",
      "Epoch 418, CIFAR-10 Batch 1:  loss: 2.54143 accuracy: 0.4046\n",
      "Epoch 419, CIFAR-10 Batch 1:  loss: 2.55135 accuracy: 0.4048\n",
      "Epoch 420, CIFAR-10 Batch 1:  loss: 2.55157 accuracy: 0.4048\n",
      "Epoch 421, CIFAR-10 Batch 1:  loss: 2.55609 accuracy: 0.4042\n",
      "Epoch 422, CIFAR-10 Batch 1:  loss: 2.55703 accuracy: 0.405\n",
      "Epoch 423, CIFAR-10 Batch 1:  loss: 2.56125 accuracy: 0.4034\n",
      "Epoch 424, CIFAR-10 Batch 1:  loss: 2.57292 accuracy: 0.4026\n",
      "Epoch 425, CIFAR-10 Batch 1:  loss: 2.56856 accuracy: 0.4022\n",
      "Epoch 426, CIFAR-10 Batch 1:  loss: 2.57099 accuracy: 0.4024\n",
      "Epoch 427, CIFAR-10 Batch 1:  loss: 2.57052 accuracy: 0.4052\n",
      "Epoch 428, CIFAR-10 Batch 1:  loss: 2.57278 accuracy: 0.4048\n",
      "Epoch 429, CIFAR-10 Batch 1:  loss: 2.55809 accuracy: 0.4076\n",
      "Epoch 430, CIFAR-10 Batch 1:  loss: 2.57206 accuracy: 0.4062\n",
      "Epoch 431, CIFAR-10 Batch 1:  loss: 2.56818 accuracy: 0.406\n",
      "Epoch 432, CIFAR-10 Batch 1:  loss: 2.56581 accuracy: 0.406\n",
      "Epoch 433, CIFAR-10 Batch 1:  loss: 2.56629 accuracy: 0.407\n",
      "Epoch 434, CIFAR-10 Batch 1:  loss: 2.55832 accuracy: 0.408\n",
      "Epoch 435, CIFAR-10 Batch 1:  loss: 2.5602 accuracy: 0.406\n",
      "Epoch 436, CIFAR-10 Batch 1:  loss: 2.56478 accuracy: 0.4088\n",
      "Epoch 437, CIFAR-10 Batch 1:  loss: 2.56932 accuracy: 0.4084\n",
      "Epoch 438, CIFAR-10 Batch 1:  loss: 2.56646 accuracy: 0.4092\n",
      "Epoch 439, CIFAR-10 Batch 1:  loss: 2.56679 accuracy: 0.4082\n",
      "Epoch 440, CIFAR-10 Batch 1:  loss: 2.57649 accuracy: 0.411\n",
      "Epoch 441, CIFAR-10 Batch 1:  loss: 2.57641 accuracy: 0.411\n",
      "Epoch 442, CIFAR-10 Batch 1:  loss: 2.58854 accuracy: 0.4108\n",
      "Epoch 443, CIFAR-10 Batch 1:  loss: 2.59583 accuracy: 0.4078\n",
      "Epoch 444, CIFAR-10 Batch 1:  loss: 2.59338 accuracy: 0.4068\n",
      "Epoch 445, CIFAR-10 Batch 1:  loss: 2.59849 accuracy: 0.4068\n",
      "Epoch 446, CIFAR-10 Batch 1:  loss: 2.60686 accuracy: 0.4066\n",
      "Epoch 447, CIFAR-10 Batch 1:  loss: 2.60763 accuracy: 0.4072\n",
      "Epoch 448, CIFAR-10 Batch 1:  loss: 2.6229 accuracy: 0.4074\n",
      "Epoch 449, CIFAR-10 Batch 1:  loss: 2.61001 accuracy: 0.4096\n",
      "Epoch 450, CIFAR-10 Batch 1:  loss: 2.61335 accuracy: 0.4068\n",
      "Epoch 451, CIFAR-10 Batch 1:  loss: 2.60289 accuracy: 0.4082\n",
      "Epoch 452, CIFAR-10 Batch 1:  loss: 2.60677 accuracy: 0.4074\n",
      "Epoch 453, CIFAR-10 Batch 1:  loss: 2.61374 accuracy: 0.4076\n",
      "Epoch 454, CIFAR-10 Batch 1:  loss: 2.61511 accuracy: 0.4102\n",
      "Epoch 455, CIFAR-10 Batch 1:  loss: 2.61458 accuracy: 0.4082\n",
      "Epoch 456, CIFAR-10 Batch 1:  loss: 2.62141 accuracy: 0.4086\n",
      "Epoch 457, CIFAR-10 Batch 1:  loss: 2.62485 accuracy: 0.4074\n",
      "Epoch 458, CIFAR-10 Batch 1:  loss: 2.62231 accuracy: 0.41\n",
      "Epoch 459, CIFAR-10 Batch 1:  loss: 2.62347 accuracy: 0.4096\n",
      "Epoch 460, CIFAR-10 Batch 1:  loss: 2.61648 accuracy: 0.4108\n",
      "Epoch 461, CIFAR-10 Batch 1:  loss: 2.61639 accuracy: 0.4118\n",
      "Epoch 462, CIFAR-10 Batch 1:  loss: 2.61361 accuracy: 0.4122\n",
      "Epoch 463, CIFAR-10 Batch 1:  loss: 2.60872 accuracy: 0.4128\n",
      "Epoch 464, CIFAR-10 Batch 1:  loss: 2.62026 accuracy: 0.4106\n",
      "Epoch 465, CIFAR-10 Batch 1:  loss: 2.61823 accuracy: 0.41\n",
      "Epoch 466, CIFAR-10 Batch 1:  loss: 2.61608 accuracy: 0.4104\n",
      "Epoch 467, CIFAR-10 Batch 1:  loss: 2.6176 accuracy: 0.4104\n",
      "Epoch 468, CIFAR-10 Batch 1:  loss: 2.6404 accuracy: 0.4118\n",
      "Epoch 469, CIFAR-10 Batch 1:  loss: 2.6499 accuracy: 0.412\n",
      "Epoch 470, CIFAR-10 Batch 1:  loss: 2.63696 accuracy: 0.4104\n",
      "Epoch 471, CIFAR-10 Batch 1:  loss: 2.64845 accuracy: 0.4122\n",
      "Epoch 472, CIFAR-10 Batch 1:  loss: 2.63057 accuracy: 0.415\n",
      "Epoch 473, CIFAR-10 Batch 1:  loss: 2.64581 accuracy: 0.4128\n",
      "Epoch 474, CIFAR-10 Batch 1:  loss: 2.65153 accuracy: 0.4132\n",
      "Epoch 475, CIFAR-10 Batch 1:  loss: 2.67065 accuracy: 0.4134\n",
      "Epoch 476, CIFAR-10 Batch 1:  loss: 2.66094 accuracy: 0.413\n",
      "Epoch 477, CIFAR-10 Batch 1:  loss: 2.67154 accuracy: 0.4118\n",
      "Epoch 478, CIFAR-10 Batch 1:  loss: 2.68139 accuracy: 0.413\n",
      "Epoch 479, CIFAR-10 Batch 1:  loss: 2.68792 accuracy: 0.413\n",
      "Epoch 480, CIFAR-10 Batch 1:  loss: 2.69578 accuracy: 0.4138\n",
      "Epoch 481, CIFAR-10 Batch 1:  loss: 2.69963 accuracy: 0.4154\n",
      "Epoch 482, CIFAR-10 Batch 1:  loss: 2.70504 accuracy: 0.4146\n",
      "Epoch 483, CIFAR-10 Batch 1:  loss: 2.70059 accuracy: 0.4118\n",
      "Epoch 484, CIFAR-10 Batch 1:  loss: 2.69763 accuracy: 0.4108\n",
      "Epoch 485, CIFAR-10 Batch 1:  loss: 2.70552 accuracy: 0.4102\n",
      "Epoch 486, CIFAR-10 Batch 1:  loss: 2.7009 accuracy: 0.4098\n",
      "Epoch 487, CIFAR-10 Batch 1:  loss: 2.71017 accuracy: 0.4118\n",
      "Epoch 488, CIFAR-10 Batch 1:  loss: 2.6978 accuracy: 0.4138\n",
      "Epoch 489, CIFAR-10 Batch 1:  loss: 2.69837 accuracy: 0.4144\n",
      "Epoch 490, CIFAR-10 Batch 1:  loss: 2.69689 accuracy: 0.4122\n",
      "Epoch 491, CIFAR-10 Batch 1:  loss: 2.68855 accuracy: 0.4152\n",
      "Epoch 492, CIFAR-10 Batch 1:  loss: 2.67784 accuracy: 0.4166\n",
      "Epoch 493, CIFAR-10 Batch 1:  loss: 2.68789 accuracy: 0.4168\n",
      "Epoch 494, CIFAR-10 Batch 1:  loss: 2.69205 accuracy: 0.4178\n",
      "Epoch 495, CIFAR-10 Batch 1:  loss: 2.68385 accuracy: 0.4172\n",
      "Epoch 496, CIFAR-10 Batch 1:  loss: 2.68696 accuracy: 0.4166\n",
      "Epoch 497, CIFAR-10 Batch 1:  loss: 2.69547 accuracy: 0.4164\n",
      "Epoch 498, CIFAR-10 Batch 1:  loss: 2.71998 accuracy: 0.4168\n",
      "Epoch 499, CIFAR-10 Batch 1:  loss: 2.72162 accuracy: 0.4168\n",
      "Epoch 500, CIFAR-10 Batch 1:  loss: 2.72766 accuracy: 0.416\n",
      "Epoch 501, CIFAR-10 Batch 1:  loss: 2.73778 accuracy: 0.4158\n",
      "Epoch 502, CIFAR-10 Batch 1:  loss: 2.73727 accuracy: 0.4164\n",
      "Epoch 503, CIFAR-10 Batch 1:  loss: 2.73341 accuracy: 0.4154\n",
      "Epoch 504, CIFAR-10 Batch 1:  loss: 2.72214 accuracy: 0.4154\n",
      "Epoch 505, CIFAR-10 Batch 1:  loss: 2.72428 accuracy: 0.4156\n",
      "Epoch 506, CIFAR-10 Batch 1:  loss: 2.74459 accuracy: 0.4176\n",
      "Epoch 507, CIFAR-10 Batch 1:  loss: 2.7527 accuracy: 0.4158\n",
      "Epoch 508, CIFAR-10 Batch 1:  loss: 2.75339 accuracy: 0.416\n",
      "Epoch 509, CIFAR-10 Batch 1:  loss: 2.75431 accuracy: 0.4166\n",
      "Epoch 510, CIFAR-10 Batch 1:  loss: 2.76811 accuracy: 0.417\n",
      "Epoch 511, CIFAR-10 Batch 1:  loss: 2.78527 accuracy: 0.4176\n",
      "Epoch 512, CIFAR-10 Batch 1:  loss: 2.79268 accuracy: 0.4188\n",
      "Epoch 513, CIFAR-10 Batch 1:  loss: 2.78669 accuracy: 0.4186\n",
      "Epoch 514, CIFAR-10 Batch 1:  loss: 2.78022 accuracy: 0.4174\n",
      "Epoch 515, CIFAR-10 Batch 1:  loss: 2.77478 accuracy: 0.4174\n",
      "Epoch 516, CIFAR-10 Batch 1:  loss: 2.7809 accuracy: 0.4172\n",
      "Epoch 517, CIFAR-10 Batch 1:  loss: 2.7994 accuracy: 0.4154\n",
      "Epoch 518, CIFAR-10 Batch 1:  loss: 2.815 accuracy: 0.418\n",
      "Epoch 519, CIFAR-10 Batch 1:  loss: 2.81443 accuracy: 0.416\n",
      "Epoch 520, CIFAR-10 Batch 1:  loss: 2.80696 accuracy: 0.4162\n",
      "Epoch 521, CIFAR-10 Batch 1:  loss: 2.8027 accuracy: 0.4168\n",
      "Epoch 522, CIFAR-10 Batch 1:  loss: 2.79037 accuracy: 0.4174\n",
      "Epoch 523, CIFAR-10 Batch 1:  loss: 2.79435 accuracy: 0.4198\n",
      "Epoch 524, CIFAR-10 Batch 1:  loss: 2.79322 accuracy: 0.4196\n",
      "Epoch 525, CIFAR-10 Batch 1:  loss: 2.8171 accuracy: 0.4196\n",
      "Epoch 526, CIFAR-10 Batch 1:  loss: 2.80455 accuracy: 0.4182\n",
      "Epoch 527, CIFAR-10 Batch 1:  loss: 2.80262 accuracy: 0.4172\n",
      "Epoch 528, CIFAR-10 Batch 1:  loss: 2.81755 accuracy: 0.4194\n",
      "Epoch 529, CIFAR-10 Batch 1:  loss: 2.79268 accuracy: 0.418\n",
      "Epoch 530, CIFAR-10 Batch 1:  loss: 2.77069 accuracy: 0.418\n",
      "Epoch 531, CIFAR-10 Batch 1:  loss: 2.77635 accuracy: 0.4196\n",
      "Epoch 532, CIFAR-10 Batch 1:  loss: 2.7691 accuracy: 0.4196\n",
      "Epoch 533, CIFAR-10 Batch 1:  loss: 2.79545 accuracy: 0.4208\n",
      "Epoch 534, CIFAR-10 Batch 1:  loss: 2.80482 accuracy: 0.4174\n",
      "Epoch 535, CIFAR-10 Batch 1:  loss: 2.81055 accuracy: 0.417\n",
      "Epoch 536, CIFAR-10 Batch 1:  loss: 2.80509 accuracy: 0.4196\n",
      "Epoch 537, CIFAR-10 Batch 1:  loss: 2.82552 accuracy: 0.4206\n",
      "Epoch 538, CIFAR-10 Batch 1:  loss: 2.80664 accuracy: 0.42\n",
      "Epoch 539, CIFAR-10 Batch 1:  loss: 2.80392 accuracy: 0.4206\n",
      "Epoch 540, CIFAR-10 Batch 1:  loss: 2.81906 accuracy: 0.4212\n",
      "Epoch 541, CIFAR-10 Batch 1:  loss: 2.83447 accuracy: 0.422\n",
      "Epoch 542, CIFAR-10 Batch 1:  loss: 2.85409 accuracy: 0.418\n",
      "Epoch 543, CIFAR-10 Batch 1:  loss: 2.84254 accuracy: 0.4208\n",
      "Epoch 544, CIFAR-10 Batch 1:  loss: 2.87025 accuracy: 0.4194\n",
      "Epoch 545, CIFAR-10 Batch 1:  loss: 2.87031 accuracy: 0.42\n",
      "Epoch 546, CIFAR-10 Batch 1:  loss: 2.87605 accuracy: 0.419\n",
      "Epoch 547, CIFAR-10 Batch 1:  loss: 2.86673 accuracy: 0.4212\n",
      "Epoch 548, CIFAR-10 Batch 1:  loss: 2.88272 accuracy: 0.4178\n",
      "Epoch 549, CIFAR-10 Batch 1:  loss: 2.88103 accuracy: 0.4176\n",
      "Epoch 550, CIFAR-10 Batch 1:  loss: 2.93276 accuracy: 0.4184\n",
      "Epoch 551, CIFAR-10 Batch 1:  loss: 2.90194 accuracy: 0.419\n",
      "Epoch 552, CIFAR-10 Batch 1:  loss: 2.91013 accuracy: 0.4186\n",
      "Epoch 553, CIFAR-10 Batch 1:  loss: 2.91762 accuracy: 0.4172\n",
      "Epoch 554, CIFAR-10 Batch 1:  loss: 2.91273 accuracy: 0.4152\n",
      "Epoch 555, CIFAR-10 Batch 1:  loss: 2.91458 accuracy: 0.4174\n",
      "Epoch 556, CIFAR-10 Batch 1:  loss: 2.92046 accuracy: 0.4172\n",
      "Epoch 557, CIFAR-10 Batch 1:  loss: 2.91753 accuracy: 0.4174\n",
      "Epoch 558, CIFAR-10 Batch 1:  loss: 2.89908 accuracy: 0.4174\n",
      "Epoch 559, CIFAR-10 Batch 1:  loss: 2.89463 accuracy: 0.4202\n",
      "Epoch 560, CIFAR-10 Batch 1:  loss: 2.90773 accuracy: 0.4194\n",
      "Epoch 561, CIFAR-10 Batch 1:  loss: 2.91853 accuracy: 0.4202\n",
      "Epoch 562, CIFAR-10 Batch 1:  loss: 2.92868 accuracy: 0.417\n",
      "Epoch 563, CIFAR-10 Batch 1:  loss: 2.92636 accuracy: 0.4196\n",
      "Epoch 564, CIFAR-10 Batch 1:  loss: 2.93381 accuracy: 0.4194\n",
      "Epoch 565, CIFAR-10 Batch 1:  loss: 2.94413 accuracy: 0.421\n",
      "Epoch 566, CIFAR-10 Batch 1:  loss: 2.94132 accuracy: 0.4206\n",
      "Epoch 567, CIFAR-10 Batch 1:  loss: 2.95075 accuracy: 0.4182\n",
      "Epoch 568, CIFAR-10 Batch 1:  loss: 2.93837 accuracy: 0.4216\n",
      "Epoch 569, CIFAR-10 Batch 1:  loss: 2.94711 accuracy: 0.421\n",
      "Epoch 570, CIFAR-10 Batch 1:  loss: 2.98064 accuracy: 0.4208\n",
      "Epoch 571, CIFAR-10 Batch 1:  loss: 2.9762 accuracy: 0.4204\n",
      "Epoch 572, CIFAR-10 Batch 1:  loss: 2.96268 accuracy: 0.423\n",
      "Epoch 573, CIFAR-10 Batch 1:  loss: 2.96431 accuracy: 0.422\n",
      "Epoch 574, CIFAR-10 Batch 1:  loss: 2.95436 accuracy: 0.419\n",
      "Epoch 575, CIFAR-10 Batch 1:  loss: 3.00566 accuracy: 0.4168\n",
      "Epoch 576, CIFAR-10 Batch 1:  loss: 3.01992 accuracy: 0.421\n",
      "Epoch 577, CIFAR-10 Batch 1:  loss: 3.01379 accuracy: 0.4198\n",
      "Epoch 578, CIFAR-10 Batch 1:  loss: 3.00871 accuracy: 0.4234\n",
      "Epoch 579, CIFAR-10 Batch 1:  loss: 2.99025 accuracy: 0.421\n",
      "Epoch 580, CIFAR-10 Batch 1:  loss: 2.98192 accuracy: 0.4242\n",
      "Epoch 581, CIFAR-10 Batch 1:  loss: 2.98887 accuracy: 0.4216\n",
      "Epoch 582, CIFAR-10 Batch 1:  loss: 2.97701 accuracy: 0.421\n",
      "Epoch 583, CIFAR-10 Batch 1:  loss: 2.97696 accuracy: 0.419\n",
      "Epoch 584, CIFAR-10 Batch 1:  loss: 2.97233 accuracy: 0.4202\n",
      "Epoch 585, CIFAR-10 Batch 1:  loss: 2.96313 accuracy: 0.4208\n",
      "Epoch 586, CIFAR-10 Batch 1:  loss: 2.99497 accuracy: 0.418\n",
      "Epoch 587, CIFAR-10 Batch 1:  loss: 3.01199 accuracy: 0.4166\n",
      "Epoch 588, CIFAR-10 Batch 1:  loss: 3.01999 accuracy: 0.4172\n",
      "Epoch 589, CIFAR-10 Batch 1:  loss: 3.03456 accuracy: 0.4192\n",
      "Epoch 590, CIFAR-10 Batch 1:  loss: 3.02676 accuracy: 0.4204\n",
      "Epoch 591, CIFAR-10 Batch 1:  loss: 3.01591 accuracy: 0.4194\n",
      "Epoch 592, CIFAR-10 Batch 1:  loss: 3.0106 accuracy: 0.4216\n",
      "Epoch 593, CIFAR-10 Batch 1:  loss: 3.02678 accuracy: 0.4206\n",
      "Epoch 594, CIFAR-10 Batch 1:  loss: 3.01169 accuracy: 0.421\n",
      "Epoch 595, CIFAR-10 Batch 1:  loss: 3.02714 accuracy: 0.4204\n",
      "Epoch 596, CIFAR-10 Batch 1:  loss: 3.05612 accuracy: 0.4192\n",
      "Epoch 597, CIFAR-10 Batch 1:  loss: 3.0828 accuracy: 0.42\n",
      "Epoch 598, CIFAR-10 Batch 1:  loss: 3.0676 accuracy: 0.4212\n",
      "Epoch 599, CIFAR-10 Batch 1:  loss: 3.08186 accuracy: 0.4204\n",
      "Epoch 600, CIFAR-10 Batch 1:  loss: 3.0655 accuracy: 0.422\n",
      "Epoch 601, CIFAR-10 Batch 1:  loss: 3.08789 accuracy: 0.4188\n",
      "Epoch 602, CIFAR-10 Batch 1:  loss: 3.05842 accuracy: 0.422\n",
      "Epoch 603, CIFAR-10 Batch 1:  loss: 3.06967 accuracy: 0.4218\n",
      "Epoch 604, CIFAR-10 Batch 1:  loss: 3.0835 accuracy: 0.4224\n",
      "Epoch 605, CIFAR-10 Batch 1:  loss: 3.10902 accuracy: 0.4196\n",
      "Epoch 606, CIFAR-10 Batch 1:  loss: 3.1067 accuracy: 0.4194\n",
      "Epoch 607, CIFAR-10 Batch 1:  loss: 3.11083 accuracy: 0.4194\n",
      "Epoch 608, CIFAR-10 Batch 1:  loss: 3.10019 accuracy: 0.4196\n",
      "Epoch 609, CIFAR-10 Batch 1:  loss: 3.09089 accuracy: 0.4202\n",
      "Epoch 610, CIFAR-10 Batch 1:  loss: 3.11854 accuracy: 0.4172\n",
      "Epoch 611, CIFAR-10 Batch 1:  loss: 3.12678 accuracy: 0.419\n",
      "Epoch 612, CIFAR-10 Batch 1:  loss: 3.13974 accuracy: 0.418\n",
      "Epoch 613, CIFAR-10 Batch 1:  loss: 3.16068 accuracy: 0.4178\n",
      "Epoch 614, CIFAR-10 Batch 1:  loss: 3.16881 accuracy: 0.4172\n",
      "Epoch 615, CIFAR-10 Batch 1:  loss: 3.17373 accuracy: 0.4168\n",
      "Epoch 616, CIFAR-10 Batch 1:  loss: 3.17764 accuracy: 0.4158\n",
      "Epoch 617, CIFAR-10 Batch 1:  loss: 3.17653 accuracy: 0.4184\n",
      "Epoch 618, CIFAR-10 Batch 1:  loss: 3.20432 accuracy: 0.4166\n",
      "Epoch 619, CIFAR-10 Batch 1:  loss: 3.20112 accuracy: 0.4188\n",
      "Epoch 620, CIFAR-10 Batch 1:  loss: 3.19195 accuracy: 0.4202\n",
      "Epoch 621, CIFAR-10 Batch 1:  loss: 3.17916 accuracy: 0.4184\n",
      "Epoch 622, CIFAR-10 Batch 1:  loss: 3.15804 accuracy: 0.4206\n",
      "Epoch 623, CIFAR-10 Batch 1:  loss: 3.17717 accuracy: 0.4194\n",
      "Epoch 624, CIFAR-10 Batch 1:  loss: 3.17887 accuracy: 0.421\n",
      "Epoch 625, CIFAR-10 Batch 1:  loss: 3.16656 accuracy: 0.4208\n",
      "Epoch 626, CIFAR-10 Batch 1:  loss: 3.17759 accuracy: 0.419\n",
      "Epoch 627, CIFAR-10 Batch 1:  loss: 3.18514 accuracy: 0.4196\n",
      "Epoch 628, CIFAR-10 Batch 1:  loss: 3.1671 accuracy: 0.418\n",
      "Epoch 629, CIFAR-10 Batch 1:  loss: 3.17249 accuracy: 0.4186\n",
      "Epoch 630, CIFAR-10 Batch 1:  loss: 3.1886 accuracy: 0.4196\n",
      "Epoch 631, CIFAR-10 Batch 1:  loss: 3.1774 accuracy: 0.4186\n",
      "Epoch 632, CIFAR-10 Batch 1:  loss: 3.15369 accuracy: 0.4188\n",
      "Epoch 633, CIFAR-10 Batch 1:  loss: 3.14669 accuracy: 0.4218\n",
      "Epoch 634, CIFAR-10 Batch 1:  loss: 3.15161 accuracy: 0.4212\n",
      "Epoch 635, CIFAR-10 Batch 1:  loss: 3.1771 accuracy: 0.4188\n",
      "Epoch 636, CIFAR-10 Batch 1:  loss: 3.16338 accuracy: 0.4186\n",
      "Epoch 637, CIFAR-10 Batch 1:  loss: 3.17796 accuracy: 0.4186\n",
      "Epoch 638, CIFAR-10 Batch 1:  loss: 3.23497 accuracy: 0.4164\n",
      "Epoch 639, CIFAR-10 Batch 1:  loss: 3.22414 accuracy: 0.4162\n",
      "Epoch 640, CIFAR-10 Batch 1:  loss: 3.24416 accuracy: 0.415\n",
      "Epoch 641, CIFAR-10 Batch 1:  loss: 3.23273 accuracy: 0.416\n",
      "Epoch 642, CIFAR-10 Batch 1:  loss: 3.22558 accuracy: 0.4172\n",
      "Epoch 643, CIFAR-10 Batch 1:  loss: 3.21907 accuracy: 0.4154\n",
      "Epoch 644, CIFAR-10 Batch 1:  loss: 3.23233 accuracy: 0.4152\n",
      "Epoch 645, CIFAR-10 Batch 1:  loss: 3.2433 accuracy: 0.4156\n",
      "Epoch 646, CIFAR-10 Batch 1:  loss: 3.27953 accuracy: 0.4136\n",
      "Epoch 647, CIFAR-10 Batch 1:  loss: 3.29377 accuracy: 0.4122\n",
      "Epoch 648, CIFAR-10 Batch 1:  loss: 3.30331 accuracy: 0.4118\n",
      "Epoch 649, CIFAR-10 Batch 1:  loss: 3.29942 accuracy: 0.4126\n",
      "Epoch 650, CIFAR-10 Batch 1:  loss: 3.29264 accuracy: 0.4126\n",
      "Epoch 651, CIFAR-10 Batch 1:  loss: 3.30328 accuracy: 0.4132\n",
      "Epoch 652, CIFAR-10 Batch 1:  loss: 3.29998 accuracy: 0.4142\n",
      "Epoch 653, CIFAR-10 Batch 1:  loss: 3.29923 accuracy: 0.4136\n",
      "Epoch 654, CIFAR-10 Batch 1:  loss: 3.30118 accuracy: 0.413\n",
      "Epoch 655, CIFAR-10 Batch 1:  loss: 3.27197 accuracy: 0.4156\n",
      "Epoch 656, CIFAR-10 Batch 1:  loss: 3.24287 accuracy: 0.4138\n",
      "Epoch 657, CIFAR-10 Batch 1:  loss: 3.27849 accuracy: 0.4162\n",
      "Epoch 658, CIFAR-10 Batch 1:  loss: 3.28917 accuracy: 0.414\n",
      "Epoch 659, CIFAR-10 Batch 1:  loss: 3.30181 accuracy: 0.414\n",
      "Epoch 660, CIFAR-10 Batch 1:  loss: 3.28873 accuracy: 0.4146\n",
      "Epoch 661, CIFAR-10 Batch 1:  loss: 3.27496 accuracy: 0.4164\n",
      "Epoch 662, CIFAR-10 Batch 1:  loss: 3.2729 accuracy: 0.4182\n",
      "Epoch 663, CIFAR-10 Batch 1:  loss: 3.29299 accuracy: 0.4168\n",
      "Epoch 664, CIFAR-10 Batch 1:  loss: 3.29014 accuracy: 0.4176\n",
      "Epoch 665, CIFAR-10 Batch 1:  loss: 3.29572 accuracy: 0.4168\n",
      "Epoch 666, CIFAR-10 Batch 1:  loss: 3.30551 accuracy: 0.4194\n",
      "Epoch 667, CIFAR-10 Batch 1:  loss: 3.35092 accuracy: 0.4164\n",
      "Epoch 668, CIFAR-10 Batch 1:  loss: 3.37475 accuracy: 0.4124\n",
      "Epoch 669, CIFAR-10 Batch 1:  loss: 3.38367 accuracy: 0.4114\n",
      "Epoch 670, CIFAR-10 Batch 1:  loss: 3.36784 accuracy: 0.4128\n",
      "Epoch 671, CIFAR-10 Batch 1:  loss: 3.3734 accuracy: 0.4154\n",
      "Epoch 672, CIFAR-10 Batch 1:  loss: 3.36053 accuracy: 0.4142\n",
      "Epoch 673, CIFAR-10 Batch 1:  loss: 3.36971 accuracy: 0.4158\n",
      "Epoch 674, CIFAR-10 Batch 1:  loss: 3.33835 accuracy: 0.4152\n",
      "Epoch 675, CIFAR-10 Batch 1:  loss: 3.34505 accuracy: 0.4138\n",
      "Epoch 676, CIFAR-10 Batch 1:  loss: 3.38484 accuracy: 0.4146\n",
      "Epoch 677, CIFAR-10 Batch 1:  loss: 3.40786 accuracy: 0.4146\n",
      "Epoch 678, CIFAR-10 Batch 1:  loss: 3.39334 accuracy: 0.4156\n",
      "Epoch 679, CIFAR-10 Batch 1:  loss: 3.38923 accuracy: 0.4182\n",
      "Epoch 680, CIFAR-10 Batch 1:  loss: 3.41835 accuracy: 0.4148\n",
      "Epoch 681, CIFAR-10 Batch 1:  loss: 3.37335 accuracy: 0.4154\n",
      "Epoch 682, CIFAR-10 Batch 1:  loss: 3.40813 accuracy: 0.4154\n",
      "Epoch 683, CIFAR-10 Batch 1:  loss: 3.39987 accuracy: 0.4146\n",
      "Epoch 684, CIFAR-10 Batch 1:  loss: 3.42773 accuracy: 0.413\n",
      "Epoch 685, CIFAR-10 Batch 1:  loss: 3.42474 accuracy: 0.4116\n",
      "Epoch 686, CIFAR-10 Batch 1:  loss: 3.40349 accuracy: 0.4144\n",
      "Epoch 687, CIFAR-10 Batch 1:  loss: 3.4229 accuracy: 0.4142\n",
      "Epoch 688, CIFAR-10 Batch 1:  loss: 3.40556 accuracy: 0.4176\n",
      "Epoch 689, CIFAR-10 Batch 1:  loss: 3.40359 accuracy: 0.4178\n",
      "Epoch 690, CIFAR-10 Batch 1:  loss: 3.42101 accuracy: 0.4156\n",
      "Epoch 691, CIFAR-10 Batch 1:  loss: 3.40768 accuracy: 0.4176\n",
      "Epoch 692, CIFAR-10 Batch 1:  loss: 3.41941 accuracy: 0.4146\n",
      "Epoch 693, CIFAR-10 Batch 1:  loss: 3.43184 accuracy: 0.4146\n",
      "Epoch 694, CIFAR-10 Batch 1:  loss: 3.43089 accuracy: 0.4164\n",
      "Epoch 695, CIFAR-10 Batch 1:  loss: 3.43302 accuracy: 0.4148\n",
      "Epoch 696, CIFAR-10 Batch 1:  loss: 3.42883 accuracy: 0.4164\n",
      "Epoch 697, CIFAR-10 Batch 1:  loss: 3.43227 accuracy: 0.4154\n",
      "Epoch 698, CIFAR-10 Batch 1:  loss: 3.43983 accuracy: 0.4144\n",
      "Epoch 699, CIFAR-10 Batch 1:  loss: 3.44068 accuracy: 0.4132\n",
      "Epoch 700, CIFAR-10 Batch 1:  loss: 3.44003 accuracy: 0.4156\n",
      "Epoch 701, CIFAR-10 Batch 1:  loss: 3.47508 accuracy: 0.4156\n",
      "Epoch 702, CIFAR-10 Batch 1:  loss: 3.49718 accuracy: 0.4156\n",
      "Epoch 703, CIFAR-10 Batch 1:  loss: 3.51702 accuracy: 0.414\n",
      "Epoch 704, CIFAR-10 Batch 1:  loss: 3.54187 accuracy: 0.414\n",
      "Epoch 705, CIFAR-10 Batch 1:  loss: 3.55682 accuracy: 0.4126\n",
      "Epoch 706, CIFAR-10 Batch 1:  loss: 3.5353 accuracy: 0.4104\n",
      "Epoch 707, CIFAR-10 Batch 1:  loss: 3.51349 accuracy: 0.412\n",
      "Epoch 708, CIFAR-10 Batch 1:  loss: 3.51052 accuracy: 0.4142\n",
      "Epoch 709, CIFAR-10 Batch 1:  loss: 3.50385 accuracy: 0.4162\n",
      "Epoch 710, CIFAR-10 Batch 1:  loss: 3.50805 accuracy: 0.415\n",
      "Epoch 711, CIFAR-10 Batch 1:  loss: 3.42919 accuracy: 0.4174\n",
      "Epoch 712, CIFAR-10 Batch 1:  loss: 3.47331 accuracy: 0.4164\n",
      "Epoch 713, CIFAR-10 Batch 1:  loss: 3.50772 accuracy: 0.4166\n",
      "Epoch 714, CIFAR-10 Batch 1:  loss: 3.55102 accuracy: 0.414\n",
      "Epoch 715, CIFAR-10 Batch 1:  loss: 3.56954 accuracy: 0.4138\n",
      "Epoch 716, CIFAR-10 Batch 1:  loss: 3.60085 accuracy: 0.4116\n",
      "Epoch 717, CIFAR-10 Batch 1:  loss: 3.63172 accuracy: 0.4092\n",
      "Epoch 718, CIFAR-10 Batch 1:  loss: 3.60844 accuracy: 0.411\n",
      "Epoch 719, CIFAR-10 Batch 1:  loss: 3.60317 accuracy: 0.4126\n",
      "Epoch 720, CIFAR-10 Batch 1:  loss: 3.61499 accuracy: 0.411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-0ebd1bbc35ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_preprocess_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtrain_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {:>2}, CIFAR-10 Batch {}:  '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-177-bbaa3b3502f8>\u001b[0m in \u001b[0;36mtrain_neural_network\u001b[0;34m(session, optimizer, keep_probability, feature_batch, label_batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/dl/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 189.54 accuracy: 0.0722\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 115.19 accuracy: 0.0806\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 79.1307 accuracy: 0.093\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 65.5205 accuracy: 0.1018\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 56.0532 accuracy: 0.1124\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 49.4108 accuracy: 0.13\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 44.544 accuracy: 0.1398\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 40.809 accuracy: 0.1532\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 38.0053 accuracy: 0.1642\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 35.6773 accuracy: 0.1794\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 33.7545 accuracy: 0.1832\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 32.0134 accuracy: 0.1858\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 30.6549 accuracy: 0.1916\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 29.4352 accuracy: 0.1966\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 28.1044 accuracy: 0.2048\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 27.0757 accuracy: 0.2104\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 25.9482 accuracy: 0.2172\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 25.0549 accuracy: 0.2178\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 24.3141 accuracy: 0.217\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 23.377 accuracy: 0.226\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 22.5752 accuracy: 0.2312\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 21.8096 accuracy: 0.2346\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 21.2567 accuracy: 0.2306\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 20.6661 accuracy: 0.2394\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 19.9858 accuracy: 0.2454\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 19.3506 accuracy: 0.255\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 18.9675 accuracy: 0.249\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 18.5094 accuracy: 0.248\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 17.9595 accuracy: 0.2516\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 17.5368 accuracy: 0.2586\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 17.1268 accuracy: 0.2682\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 16.8701 accuracy: 0.2588\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 16.5467 accuracy: 0.2612\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 16.0426 accuracy: 0.2666\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 15.7218 accuracy: 0.276\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 15.4612 accuracy: 0.2798\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 15.1893 accuracy: 0.2706\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 15.0041 accuracy: 0.2738\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 14.5216 accuracy: 0.2752\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 14.2945 accuracy: 0.2794\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 14.0566 accuracy: 0.2874\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 13.8694 accuracy: 0.2758\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 13.7618 accuracy: 0.2826\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 13.299 accuracy: 0.286\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 13.1447 accuracy: 0.2884\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 12.9225 accuracy: 0.295\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 12.7865 accuracy: 0.2858\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 12.7151 accuracy: 0.2934\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 12.2694 accuracy: 0.2938\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 12.1632 accuracy: 0.296\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 11.9663 accuracy: 0.3018\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 11.8977 accuracy: 0.2918\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 11.773 accuracy: 0.3014\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 11.389 accuracy: 0.3012\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 11.2826 accuracy: 0.3014\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 11.1138 accuracy: 0.3052\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 11.1451 accuracy: 0.289\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 10.8713 accuracy: 0.3066\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 10.611 accuracy: 0.305\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 10.5092 accuracy: 0.3026\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 10.3661 accuracy: 0.3066\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 10.4534 accuracy: 0.2886\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 10.1172 accuracy: 0.3124\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 9.93148 accuracy: 0.3132\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 9.84203 accuracy: 0.307\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 9.69595 accuracy: 0.313\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 9.80092 accuracy: 0.2968\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 9.46567 accuracy: 0.3168\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 9.3284 accuracy: 0.313\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 9.24673 accuracy: 0.3138\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 9.0979 accuracy: 0.3186\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 9.19228 accuracy: 0.3052\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 8.85824 accuracy: 0.3204\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 8.7687 accuracy: 0.3226\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 8.70736 accuracy: 0.3176\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 8.55887 accuracy: 0.3272\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 8.63004 accuracy: 0.3102\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 8.34205 accuracy: 0.3244\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 8.26128 accuracy: 0.3294\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 8.23135 accuracy: 0.3224\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 8.09278 accuracy: 0.3316\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 8.13909 accuracy: 0.317\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 7.88936 accuracy: 0.3294\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 7.81379 accuracy: 0.3326\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 7.80431 accuracy: 0.3264\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 7.66708 accuracy: 0.3336\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 7.70473 accuracy: 0.3252\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 7.48059 accuracy: 0.335\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 7.41868 accuracy: 0.3348\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 7.42053 accuracy: 0.3296\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 7.28826 accuracy: 0.3332\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 7.3222 accuracy: 0.3284\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 7.12859 accuracy: 0.3384\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 7.07526 accuracy: 0.3346\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 7.07097 accuracy: 0.3278\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 6.94337 accuracy: 0.3356\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 6.96133 accuracy: 0.3324\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 6.8067 accuracy: 0.342\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 6.76253 accuracy: 0.3378\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 6.75191 accuracy: 0.3374\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 6.62623 accuracy: 0.3406\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 6.63539 accuracy: 0.3368\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 6.51825 accuracy: 0.3428\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 6.46857 accuracy: 0.3424\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 6.45668 accuracy: 0.3422\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 6.32591 accuracy: 0.3442\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 6.3538 accuracy: 0.338\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 6.25568 accuracy: 0.3466\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 6.19583 accuracy: 0.345\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 6.18679 accuracy: 0.3436\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 6.05598 accuracy: 0.3486\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 6.09413 accuracy: 0.3406\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 6.00223 accuracy: 0.3492\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 5.93638 accuracy: 0.3508\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 5.93913 accuracy: 0.3476\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 5.8162 accuracy: 0.3502\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 5.85027 accuracy: 0.3458\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 5.76851 accuracy: 0.351\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 5.70085 accuracy: 0.3586\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 5.72135 accuracy: 0.345\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 5.59516 accuracy: 0.3524\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 5.62938 accuracy: 0.3472\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 5.54654 accuracy: 0.3538\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 5.48218 accuracy: 0.3572\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 5.52209 accuracy: 0.3472\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 5.38875 accuracy: 0.3536\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 5.42282 accuracy: 0.3482\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 5.34478 accuracy: 0.3526\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 5.28014 accuracy: 0.3572\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 5.33992 accuracy: 0.3498\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 5.19258 accuracy: 0.3558\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 5.22845 accuracy: 0.3502\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 5.15262 accuracy: 0.3558\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 5.09556 accuracy: 0.3604\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 5.16428 accuracy: 0.3488\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 5.01082 accuracy: 0.3562\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 5.04407 accuracy: 0.3516\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 4.97305 accuracy: 0.3574\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 4.92291 accuracy: 0.3618\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 4.9904 accuracy: 0.3506\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 4.84307 accuracy: 0.36\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 4.87227 accuracy: 0.3536\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 4.80513 accuracy: 0.3584\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 4.7541 accuracy: 0.359\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 4.82126 accuracy: 0.3504\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 4.68599 accuracy: 0.3626\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 4.70954 accuracy: 0.3538\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 4.64675 accuracy: 0.3584\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 4.59464 accuracy: 0.3632\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 4.66258 accuracy: 0.3502\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 4.53734 accuracy: 0.3622\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 4.55375 accuracy: 0.3566\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 4.49649 accuracy: 0.3606\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 4.44965 accuracy: 0.3636\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 4.50834 accuracy: 0.3542\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 4.39821 accuracy: 0.366\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 4.40899 accuracy: 0.3584\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 4.35642 accuracy: 0.362\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 4.31017 accuracy: 0.3636\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 4.36321 accuracy: 0.3552\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 4.26847 accuracy: 0.3684\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 4.27333 accuracy: 0.3582\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 4.22666 accuracy: 0.3624\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 4.17972 accuracy: 0.3646\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 4.22548 accuracy: 0.3598\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 4.14998 accuracy: 0.368\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 4.14565 accuracy: 0.358\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 4.10666 accuracy: 0.3634\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 4.05913 accuracy: 0.3668\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 4.09874 accuracy: 0.3642\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 4.03874 accuracy: 0.3706\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 4.02661 accuracy: 0.3582\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 3.9895 accuracy: 0.3646\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 3.9413 accuracy: 0.3702\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 3.97657 accuracy: 0.367\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 3.92923 accuracy: 0.3746\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 3.91027 accuracy: 0.3588\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 3.88066 accuracy: 0.3664\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 3.8313 accuracy: 0.3732\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 3.86394 accuracy: 0.368\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 3.82775 accuracy: 0.3796\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 3.80181 accuracy: 0.362\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 3.77413 accuracy: 0.3672\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 3.72992 accuracy: 0.3752\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 3.75793 accuracy: 0.3726\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 3.73532 accuracy: 0.3808\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 3.70659 accuracy: 0.3632\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 3.67568 accuracy: 0.3682\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 3.63672 accuracy: 0.377\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 3.65866 accuracy: 0.3746\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 3.64843 accuracy: 0.3822\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 3.61606 accuracy: 0.3648\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 3.58141 accuracy: 0.3704\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 3.54892 accuracy: 0.3786\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 3.56651 accuracy: 0.376\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 3.56657 accuracy: 0.384\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 3.53206 accuracy: 0.368\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 3.4946 accuracy: 0.3728\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 3.46553 accuracy: 0.3812\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 3.47931 accuracy: 0.379\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 3.48877 accuracy: 0.3846\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 3.45412 accuracy: 0.3696\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 3.41047 accuracy: 0.3748\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 3.38579 accuracy: 0.3834\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 3.39801 accuracy: 0.3808\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 3.41264 accuracy: 0.3862\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 3.3785 accuracy: 0.371\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 3.33156 accuracy: 0.3762\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 3.30889 accuracy: 0.3842\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 3.3208 accuracy: 0.3834\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 3.33813 accuracy: 0.3862\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 3.30499 accuracy: 0.3736\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 3.25762 accuracy: 0.3782\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 3.23663 accuracy: 0.3876\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 3.24966 accuracy: 0.3834\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 3.26879 accuracy: 0.3862\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 3.23538 accuracy: 0.3748\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 3.18867 accuracy: 0.3802\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 3.17015 accuracy: 0.3908\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 3.17819 accuracy: 0.385\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 3.2012 accuracy: 0.3862\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 3.1685 accuracy: 0.3778\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 3.12217 accuracy: 0.3808\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 3.1066 accuracy: 0.3924\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 3.11256 accuracy: 0.3876\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 3.13769 accuracy: 0.388\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 3.10687 accuracy: 0.3786\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 3.05985 accuracy: 0.384\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 3.0457 accuracy: 0.3938\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 3.05165 accuracy: 0.3902\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 3.0782 accuracy: 0.3894\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 3.04798 accuracy: 0.38\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 3.00057 accuracy: 0.3872\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 2.98687 accuracy: 0.3964\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 2.99222 accuracy: 0.3936\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 3.02009 accuracy: 0.3918\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 2.99072 accuracy: 0.3826\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 2.94432 accuracy: 0.3898\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 2.93327 accuracy: 0.3978\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 2.93496 accuracy: 0.394\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 2.96398 accuracy: 0.3924\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 2.93754 accuracy: 0.383\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 2.89125 accuracy: 0.3896\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 2.88126 accuracy: 0.4006\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 2.88116 accuracy: 0.3952\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 2.91051 accuracy: 0.3948\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 2.88645 accuracy: 0.3822\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 2.84012 accuracy: 0.3918\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 2.82953 accuracy: 0.4032\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 2.83091 accuracy: 0.3968\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 2.86247 accuracy: 0.397\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 2.83769 accuracy: 0.384\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 2.791 accuracy: 0.3938\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 2.78144 accuracy: 0.4044\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 2.78018 accuracy: 0.4002\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 2.81208 accuracy: 0.3974\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 2.79091 accuracy: 0.3852\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 2.74529 accuracy: 0.3944\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 2.73495 accuracy: 0.4062\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 2.7339 accuracy: 0.4016\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 2.76698 accuracy: 0.3974\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 2.74824 accuracy: 0.386\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 2.7024 accuracy: 0.3952\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 2.692 accuracy: 0.4078\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 2.69165 accuracy: 0.4048\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 2.72534 accuracy: 0.3988\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 2.70643 accuracy: 0.3884\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 2.66167 accuracy: 0.396\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 2.65076 accuracy: 0.4086\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 2.65013 accuracy: 0.4052\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 2.68568 accuracy: 0.3996\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 2.66544 accuracy: 0.3908\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 2.62221 accuracy: 0.3968\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 2.60995 accuracy: 0.4084\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 2.61055 accuracy: 0.406\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 2.64611 accuracy: 0.4032\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 2.62715 accuracy: 0.3898\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 2.58489 accuracy: 0.3966\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 2.57107 accuracy: 0.41\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 2.57181 accuracy: 0.4084\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 2.6071 accuracy: 0.4044\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 2.59007 accuracy: 0.3916\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 2.54866 accuracy: 0.3966\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 2.53436 accuracy: 0.41\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 2.53677 accuracy: 0.41\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 2.57165 accuracy: 0.405\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 2.55352 accuracy: 0.3936\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 2.51471 accuracy: 0.3982\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 2.50048 accuracy: 0.4122\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 2.5029 accuracy: 0.409\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 2.53837 accuracy: 0.4054\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 2.51698 accuracy: 0.3948\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 2.47983 accuracy: 0.3996\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 2.46561 accuracy: 0.414\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 2.47133 accuracy: 0.4106\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 2.50657 accuracy: 0.407\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 2.48395 accuracy: 0.3964\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 2.44804 accuracy: 0.4012\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 2.43289 accuracy: 0.4154\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 2.43907 accuracy: 0.4106\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 2.47721 accuracy: 0.4098\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 2.45109 accuracy: 0.3968\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 2.4169 accuracy: 0.4036\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 2.40102 accuracy: 0.4158\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 2.40718 accuracy: 0.412\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 2.44824 accuracy: 0.4104\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 2.41803 accuracy: 0.3996\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 2.38772 accuracy: 0.4038\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 2.36967 accuracy: 0.4166\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 2.37754 accuracy: 0.4132\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 2.41847 accuracy: 0.4098\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 2.38736 accuracy: 0.4014\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 2.36067 accuracy: 0.4046\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 2.34134 accuracy: 0.417\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 2.35122 accuracy: 0.4156\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 2.39163 accuracy: 0.4132\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 2.3581 accuracy: 0.4032\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 2.33404 accuracy: 0.4054\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 2.31346 accuracy: 0.4184\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 2.32372 accuracy: 0.417\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 2.36307 accuracy: 0.415\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 2.32973 accuracy: 0.4056\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 2.30623 accuracy: 0.4084\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 2.28653 accuracy: 0.4186\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 2.29676 accuracy: 0.4182\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 2.33947 accuracy: 0.4158\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 2.30261 accuracy: 0.4068\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 2.28137 accuracy: 0.4108\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 2.25963 accuracy: 0.4204\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 2.2705 accuracy: 0.4194\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 2.316 accuracy: 0.4168\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 2.27748 accuracy: 0.4086\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 2.25906 accuracy: 0.4102\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 2.23538 accuracy: 0.4222\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 2.24604 accuracy: 0.4204\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 2.29114 accuracy: 0.4168\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 2.25243 accuracy: 0.4104\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 2.2372 accuracy: 0.4116\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 2.2116 accuracy: 0.423\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 2.22224 accuracy: 0.4228\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 2.26879 accuracy: 0.4178\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 2.22662 accuracy: 0.4116\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 2.21389 accuracy: 0.4152\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 2.18956 accuracy: 0.4246\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 2.19954 accuracy: 0.4224\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 2.24936 accuracy: 0.419\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 2.20266 accuracy: 0.4126\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 2.19408 accuracy: 0.4162\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 2.16754 accuracy: 0.4254\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 2.17798 accuracy: 0.4236\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 2.227 accuracy: 0.4202\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 2.18069 accuracy: 0.415\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 2.17308 accuracy: 0.417\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 2.14572 accuracy: 0.4262\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 2.15527 accuracy: 0.4264\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 2.20584 accuracy: 0.422\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 2.15725 accuracy: 0.416\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 2.15253 accuracy: 0.4178\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 2.12742 accuracy: 0.4276\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 2.13676 accuracy: 0.4262\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 2.18622 accuracy: 0.422\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 2.1365 accuracy: 0.4178\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 2.13229 accuracy: 0.4206\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 2.10678 accuracy: 0.4282\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 2.11722 accuracy: 0.4274\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 2.16522 accuracy: 0.4252\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 2.11501 accuracy: 0.4188\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 2.11272 accuracy: 0.4212\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 2.08895 accuracy: 0.4282\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 2.09736 accuracy: 0.4278\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 2.14751 accuracy: 0.426\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 2.09475 accuracy: 0.4218\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 2.09419 accuracy: 0.4226\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 2.07123 accuracy: 0.4302\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 2.07909 accuracy: 0.4286\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 2.1272 accuracy: 0.4246\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 2.07389 accuracy: 0.425\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 2.07595 accuracy: 0.424\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 2.05276 accuracy: 0.431\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 2.0597 accuracy: 0.4292\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 2.10902 accuracy: 0.426\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 2.05542 accuracy: 0.4258\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 2.06094 accuracy: 0.4242\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 2.03741 accuracy: 0.4324\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 2.04271 accuracy: 0.4296\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 2.09145 accuracy: 0.4266\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 2.03603 accuracy: 0.4276\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 2.03979 accuracy: 0.4268\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 2.02106 accuracy: 0.4328\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 2.02685 accuracy: 0.4306\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 2.07441 accuracy: 0.4278\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 2.01937 accuracy: 0.4274\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 2.02361 accuracy: 0.4282\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 2.00971 accuracy: 0.4342\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 2.00842 accuracy: 0.43\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 2.05617 accuracy: 0.4298\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 1.99986 accuracy: 0.4284\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 2.0055 accuracy: 0.43\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 1.98962 accuracy: 0.436\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 1.99428 accuracy: 0.431\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 2.03972 accuracy: 0.4308\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 1.98215 accuracy: 0.4298\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 1.98936 accuracy: 0.4326\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 1.97299 accuracy: 0.4372\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 1.97944 accuracy: 0.4328\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 2.02252 accuracy: 0.4328\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 1.9667 accuracy: 0.4312\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 1.97514 accuracy: 0.4336\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 1.96038 accuracy: 0.4372\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 1.96359 accuracy: 0.4366\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 2.00752 accuracy: 0.4342\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 1.95127 accuracy: 0.4326\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 1.96017 accuracy: 0.4322\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 1.94575 accuracy: 0.4386\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 1.94866 accuracy: 0.4374\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 1.99211 accuracy: 0.4346\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 1.93598 accuracy: 0.4332\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 1.9443 accuracy: 0.4326\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 1.92988 accuracy: 0.4388\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 1.93435 accuracy: 0.4408\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 1.97762 accuracy: 0.4352\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 1.91993 accuracy: 0.4364\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 1.92998 accuracy: 0.434\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 1.91844 accuracy: 0.438\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 1.92209 accuracy: 0.4426\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 1.96475 accuracy: 0.4356\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 1.90625 accuracy: 0.4374\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 1.91727 accuracy: 0.4352\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 1.90523 accuracy: 0.4388\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 1.90759 accuracy: 0.4426\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 1.94834 accuracy: 0.4378\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 1.89291 accuracy: 0.4386\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 1.905 accuracy: 0.4346\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 1.89135 accuracy: 0.4392\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 1.89644 accuracy: 0.4426\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 1.93585 accuracy: 0.4398\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 1.87963 accuracy: 0.4402\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 1.89083 accuracy: 0.437\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 1.87706 accuracy: 0.4402\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 1.88516 accuracy: 0.4424\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 1.92381 accuracy: 0.4402\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 1.86719 accuracy: 0.4412\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 1.87879 accuracy: 0.4394\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 1.86717 accuracy: 0.4408\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 1.87516 accuracy: 0.443\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 1.91276 accuracy: 0.44\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 1.85624 accuracy: 0.4434\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 1.86812 accuracy: 0.4404\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 1.85623 accuracy: 0.4402\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 1.86231 accuracy: 0.445\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 1.89908 accuracy: 0.4408\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 1.84578 accuracy: 0.4448\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 1.85521 accuracy: 0.4428\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 1.84547 accuracy: 0.4406\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 1.85105 accuracy: 0.445\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 1.88848 accuracy: 0.4404\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 1.83233 accuracy: 0.4466\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 1.84533 accuracy: 0.444\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 1.83347 accuracy: 0.443\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 1.84068 accuracy: 0.4458\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 1.87616 accuracy: 0.4418\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 1.82127 accuracy: 0.4476\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 1.83345 accuracy: 0.445\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 1.82385 accuracy: 0.4432\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 1.8316 accuracy: 0.4474\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 1.86543 accuracy: 0.4424\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 1.8097 accuracy: 0.4466\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 1.82095 accuracy: 0.4446\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 1.81348 accuracy: 0.4436\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 1.82025 accuracy: 0.4478\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 1.85659 accuracy: 0.4444\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 1.8023 accuracy: 0.4486\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 1.81377 accuracy: 0.4462\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 1.80327 accuracy: 0.4454\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 1.81231 accuracy: 0.4478\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 1.84559 accuracy: 0.445\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 1.78942 accuracy: 0.4492\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 1.80274 accuracy: 0.4484\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 1.79394 accuracy: 0.4454\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 1.80198 accuracy: 0.4488\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 1.83552 accuracy: 0.445\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 1.77921 accuracy: 0.4504\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 1.79379 accuracy: 0.4484\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 1.78625 accuracy: 0.4444\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 1.79229 accuracy: 0.4512\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 1.82778 accuracy: 0.446\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 1.76938 accuracy: 0.4518\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 1.78155 accuracy: 0.45\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 1.77583 accuracy: 0.4448\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 1.78184 accuracy: 0.4522\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 1.81749 accuracy: 0.4478\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 1.75872 accuracy: 0.4526\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 1.77345 accuracy: 0.4498\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 1.76666 accuracy: 0.4454\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 1.77354 accuracy: 0.4542\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 1.80911 accuracy: 0.4492\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 1.75204 accuracy: 0.4534\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 1.76512 accuracy: 0.4512\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 1.75886 accuracy: 0.446\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 1.76802 accuracy: 0.4544\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss: 1.80151 accuracy: 0.449\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss: 1.74578 accuracy: 0.454\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss: 1.75879 accuracy: 0.4524\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss: 1.74839 accuracy: 0.45\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss: 1.76283 accuracy: 0.4542\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss: 1.79038 accuracy: 0.4494\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss: 1.73517 accuracy: 0.4548\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss: 1.74843 accuracy: 0.4532\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss: 1.74007 accuracy: 0.45\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss: 1.75401 accuracy: 0.4562\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss: 1.78193 accuracy: 0.4496\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss: 1.72629 accuracy: 0.4556\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss: 1.74026 accuracy: 0.4536\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss: 1.73176 accuracy: 0.452\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss: 1.74987 accuracy: 0.4578\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss: 1.7755 accuracy: 0.4496\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss: 1.7186 accuracy: 0.4562\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss: 1.73146 accuracy: 0.4546\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss: 1.72342 accuracy: 0.4548\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss: 1.74292 accuracy: 0.457\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss: 1.76823 accuracy: 0.451\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss: 1.71105 accuracy: 0.4574\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss: 1.7236 accuracy: 0.4564\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss: 1.71451 accuracy: 0.455\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss: 1.73312 accuracy: 0.4602\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss: 1.76032 accuracy: 0.4514\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss: 1.70307 accuracy: 0.4578\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss: 1.71722 accuracy: 0.4576\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss: 1.70771 accuracy: 0.4548\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss: 1.72667 accuracy: 0.4598\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss: 1.75382 accuracy: 0.4526\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss: 1.69784 accuracy: 0.4598\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss: 1.70917 accuracy: 0.4568\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss: 1.70146 accuracy: 0.4564\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss: 1.72068 accuracy: 0.459\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss: 1.74787 accuracy: 0.4524\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss: 1.68941 accuracy: 0.4616\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss: 1.70304 accuracy: 0.4572\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss: 1.69317 accuracy: 0.4574\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss: 1.71529 accuracy: 0.459\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss: 1.73946 accuracy: 0.4538\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss: 1.68206 accuracy: 0.4628\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss: 1.69375 accuracy: 0.4598\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss: 1.68338 accuracy: 0.4576\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss: 1.70706 accuracy: 0.4614\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss: 1.73452 accuracy: 0.4548\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss: 1.67753 accuracy: 0.4638\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss: 1.68945 accuracy: 0.4604\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss: 1.68047 accuracy: 0.459\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss: 1.70156 accuracy: 0.462\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss: 1.728 accuracy: 0.4558\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss: 1.66948 accuracy: 0.4658\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss: 1.68387 accuracy: 0.4606\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss: 1.67395 accuracy: 0.4592\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss: 1.69291 accuracy: 0.4626\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss: 1.71905 accuracy: 0.4564\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss: 1.6632 accuracy: 0.466\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss: 1.67759 accuracy: 0.4612\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss: 1.66834 accuracy: 0.4598\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss: 1.68664 accuracy: 0.4634\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss: 1.71588 accuracy: 0.4578\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss: 1.65874 accuracy: 0.4662\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss: 1.67112 accuracy: 0.4628\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss: 1.66366 accuracy: 0.4608\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss: 1.67928 accuracy: 0.4632\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss: 1.70636 accuracy: 0.461\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss: 1.65217 accuracy: 0.4678\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss: 1.66414 accuracy: 0.4644\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss: 1.65706 accuracy: 0.4628\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss: 1.67363 accuracy: 0.4644\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss: 1.70221 accuracy: 0.4604\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss: 1.64555 accuracy: 0.4678\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss: 1.65885 accuracy: 0.464\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss: 1.65115 accuracy: 0.464\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss: 1.66732 accuracy: 0.4646\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss: 1.69676 accuracy: 0.4626\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss: 1.64097 accuracy: 0.4696\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss: 1.65504 accuracy: 0.4674\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss: 1.64685 accuracy: 0.4652\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss: 1.66248 accuracy: 0.466\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss: 1.69121 accuracy: 0.4618\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss: 1.63486 accuracy: 0.4704\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss: 1.64878 accuracy: 0.4692\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss: 1.64132 accuracy: 0.4652\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss: 1.65599 accuracy: 0.4676\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss: 1.68389 accuracy: 0.463\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss: 1.63105 accuracy: 0.4706\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss: 1.64303 accuracy: 0.4696\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss: 1.63621 accuracy: 0.466\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss: 1.65113 accuracy: 0.468\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss: 1.67825 accuracy: 0.4638\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss: 1.62662 accuracy: 0.4724\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss: 1.63908 accuracy: 0.4696\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss: 1.62963 accuracy: 0.4676\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss: 1.64506 accuracy: 0.4688\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss: 1.6777 accuracy: 0.4662\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss: 1.62085 accuracy: 0.4718\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss: 1.63446 accuracy: 0.4702\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss: 1.62483 accuracy: 0.4688\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss: 1.64044 accuracy: 0.4698\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss: 1.67277 accuracy: 0.4682\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss: 1.61684 accuracy: 0.4758\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss: 1.62794 accuracy: 0.47\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss: 1.61976 accuracy: 0.469\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss: 1.63776 accuracy: 0.4688\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss: 1.66832 accuracy: 0.4688\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss: 1.61304 accuracy: 0.4756\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss: 1.62359 accuracy: 0.47\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss: 1.6156 accuracy: 0.4716\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss: 1.63213 accuracy: 0.4722\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss: 1.66422 accuracy: 0.4702\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss: 1.61027 accuracy: 0.4782\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss: 1.62169 accuracy: 0.4718\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss: 1.61114 accuracy: 0.4722\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss: 1.6275 accuracy: 0.4708\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss: 1.65955 accuracy: 0.4716\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss: 1.60507 accuracy: 0.479\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss: 1.61565 accuracy: 0.4726\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss: 1.60717 accuracy: 0.4734\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss: 1.62529 accuracy: 0.472\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss: 1.65426 accuracy: 0.4714\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss: 1.60116 accuracy: 0.4794\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss: 1.61434 accuracy: 0.4736\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss: 1.60404 accuracy: 0.4738\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss: 1.61898 accuracy: 0.4742\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss: 1.64804 accuracy: 0.4722\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss: 1.5979 accuracy: 0.4792\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss: 1.60879 accuracy: 0.4758\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss: 1.60031 accuracy: 0.4752\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss: 1.61781 accuracy: 0.4736\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss: 1.64088 accuracy: 0.473\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss: 1.59628 accuracy: 0.481\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss: 1.60526 accuracy: 0.4766\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss: 1.59393 accuracy: 0.4772\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss: 1.61231 accuracy: 0.4756\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss: 1.63619 accuracy: 0.4734\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss: 1.5905 accuracy: 0.4824\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss: 1.59955 accuracy: 0.4756\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss: 1.59144 accuracy: 0.4798\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss: 1.60468 accuracy: 0.4774\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss: 1.62856 accuracy: 0.4736\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss: 1.58869 accuracy: 0.4828\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss: 1.59914 accuracy: 0.4766\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss: 1.58919 accuracy: 0.4786\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss: 1.60604 accuracy: 0.477\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss: 1.62453 accuracy: 0.4754\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss: 1.58191 accuracy: 0.4842\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss: 1.59366 accuracy: 0.4768\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss: 1.58481 accuracy: 0.4804\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss: 1.6036 accuracy: 0.478\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss: 1.62224 accuracy: 0.475\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss: 1.58283 accuracy: 0.4836\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss: 1.59068 accuracy: 0.4766\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss: 1.58427 accuracy: 0.4814\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss: 1.59913 accuracy: 0.4768\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss: 1.61645 accuracy: 0.4748\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss: 1.58162 accuracy: 0.4848\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss: 1.58818 accuracy: 0.4776\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss: 1.58277 accuracy: 0.4838\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss: 1.59415 accuracy: 0.4774\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss: 1.61264 accuracy: 0.4764\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss: 1.57641 accuracy: 0.4862\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss: 1.5809 accuracy: 0.4806\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss: 1.5791 accuracy: 0.4842\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss: 1.5913 accuracy: 0.478\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss: 1.61 accuracy: 0.4772\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss: 1.57283 accuracy: 0.486\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss: 1.57967 accuracy: 0.4792\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss: 1.57501 accuracy: 0.4864\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss: 1.58491 accuracy: 0.4796\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss: 1.60792 accuracy: 0.4778\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss: 1.56863 accuracy: 0.487\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss: 1.57591 accuracy: 0.4792\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss: 1.57137 accuracy: 0.4862\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss: 1.58237 accuracy: 0.4808\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss: 1.6041 accuracy: 0.4792\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss: 1.56583 accuracy: 0.4872\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss: 1.57546 accuracy: 0.4794\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss: 1.56663 accuracy: 0.4864\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss: 1.57964 accuracy: 0.4804\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss: 1.60171 accuracy: 0.4782\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss: 1.55904 accuracy: 0.4884\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss: 1.57125 accuracy: 0.4816\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss: 1.56304 accuracy: 0.488\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss: 1.57542 accuracy: 0.4824\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss: 1.60306 accuracy: 0.478\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss: 1.55362 accuracy: 0.4902\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss: 1.57013 accuracy: 0.4806\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss: 1.56219 accuracy: 0.4874\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss: 1.57278 accuracy: 0.4822\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss: 1.59802 accuracy: 0.4796\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss: 1.55141 accuracy: 0.4886\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss: 1.57192 accuracy: 0.4794\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss: 1.55795 accuracy: 0.4888\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss: 1.57001 accuracy: 0.484\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss: 1.59433 accuracy: 0.4792\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss: 1.54704 accuracy: 0.4888\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss: 1.56745 accuracy: 0.4822\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss: 1.55876 accuracy: 0.4888\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss: 1.5654 accuracy: 0.4846\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss: 1.5907 accuracy: 0.4804\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss: 1.5417 accuracy: 0.4898\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss: 1.56494 accuracy: 0.4824\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss: 1.55516 accuracy: 0.4904\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss: 1.56205 accuracy: 0.4882\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss: 1.5897 accuracy: 0.4804\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss: 1.54405 accuracy: 0.49\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss: 1.56231 accuracy: 0.4822\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss: 1.54954 accuracy: 0.4894\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss: 1.56056 accuracy: 0.487\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss: 1.58806 accuracy: 0.4794\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss: 1.54042 accuracy: 0.4914\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss: 1.55966 accuracy: 0.4838\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss: 1.55221 accuracy: 0.4896\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss: 1.55842 accuracy: 0.4872\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss: 1.58495 accuracy: 0.4802\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss: 1.53816 accuracy: 0.4902\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss: 1.55662 accuracy: 0.484\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss: 1.54996 accuracy: 0.4896\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss: 1.5534 accuracy: 0.4876\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss: 1.5831 accuracy: 0.4806\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss: 1.53087 accuracy: 0.4904\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss: 1.55383 accuracy: 0.4844\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss: 1.54695 accuracy: 0.4908\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss: 1.55075 accuracy: 0.4882\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss: 1.58047 accuracy: 0.4816\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss: 1.53169 accuracy: 0.49\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss: 1.54997 accuracy: 0.486\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss: 1.5454 accuracy: 0.4912\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss: 1.55026 accuracy: 0.4902\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss: 1.57626 accuracy: 0.4808\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss: 1.52942 accuracy: 0.4906\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss: 1.54728 accuracy: 0.4868\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss: 1.54326 accuracy: 0.4916\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss: 1.54763 accuracy: 0.4892\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss: 1.5772 accuracy: 0.48\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss: 1.52614 accuracy: 0.4894\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss: 1.54444 accuracy: 0.4876\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss: 1.53963 accuracy: 0.4918\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss: 1.54721 accuracy: 0.4902\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss: 1.57547 accuracy: 0.4808\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss: 1.52316 accuracy: 0.4918\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss: 1.53866 accuracy: 0.489\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss: 1.53774 accuracy: 0.4928\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss: 1.54451 accuracy: 0.4904\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss: 1.5698 accuracy: 0.4826\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss: 1.5202 accuracy: 0.4936\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss: 1.5375 accuracy: 0.4874\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss: 1.53494 accuracy: 0.4928\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss: 1.54045 accuracy: 0.4922\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss: 1.56731 accuracy: 0.4824\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss: 1.51923 accuracy: 0.4904\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss: 1.53813 accuracy: 0.4886\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss: 1.5338 accuracy: 0.4936\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss: 1.53815 accuracy: 0.4926\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss: 1.56805 accuracy: 0.4806\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss: 1.51357 accuracy: 0.491\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss: 1.53629 accuracy: 0.4894\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss: 1.5288 accuracy: 0.4946\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss: 1.53478 accuracy: 0.495\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss: 1.56819 accuracy: 0.4812\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss: 1.51499 accuracy: 0.4928\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss: 1.53417 accuracy: 0.4902\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss: 1.52644 accuracy: 0.4934\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss: 1.53039 accuracy: 0.494\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss: 1.56297 accuracy: 0.4832\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss: 1.51331 accuracy: 0.495\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss: 1.527 accuracy: 0.4912\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss: 1.52702 accuracy: 0.493\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss: 1.53414 accuracy: 0.4954\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss: 1.55898 accuracy: 0.482\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss: 1.51141 accuracy: 0.494\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss: 1.52636 accuracy: 0.491\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss: 1.52138 accuracy: 0.4944\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss: 1.53127 accuracy: 0.4956\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss: 1.55542 accuracy: 0.4838\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss: 1.50846 accuracy: 0.495\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss: 1.5294 accuracy: 0.4924\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss: 1.51382 accuracy: 0.496\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss: 1.52881 accuracy: 0.4954\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss: 1.55378 accuracy: 0.484\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss: 1.50687 accuracy: 0.4944\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss: 1.52681 accuracy: 0.4904\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss: 1.51117 accuracy: 0.4946\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss: 1.52511 accuracy: 0.4958\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss: 1.55454 accuracy: 0.486\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss: 1.50379 accuracy: 0.4952\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss: 1.52324 accuracy: 0.4936\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss: 1.51393 accuracy: 0.4958\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss: 1.52457 accuracy: 0.496\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss: 1.55291 accuracy: 0.4846\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss: 1.50314 accuracy: 0.4964\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss: 1.52349 accuracy: 0.4934\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss: 1.5163 accuracy: 0.4972\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss: 1.52335 accuracy: 0.4952\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss: 1.54979 accuracy: 0.4854\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss: 1.49857 accuracy: 0.4972\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss: 1.51964 accuracy: 0.493\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss: 1.51067 accuracy: 0.4982\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss: 1.5202 accuracy: 0.4978\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss: 1.5501 accuracy: 0.486\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss: 1.49568 accuracy: 0.4982\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss: 1.51784 accuracy: 0.4948\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss: 1.51316 accuracy: 0.498\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss: 1.5192 accuracy: 0.4978\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss: 1.54358 accuracy: 0.4866\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss: 1.49418 accuracy: 0.4976\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss: 1.51834 accuracy: 0.496\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss: 1.51081 accuracy: 0.4986\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss: 1.51734 accuracy: 0.4988\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss: 1.5413 accuracy: 0.4888\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss: 1.49296 accuracy: 0.498\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss: 1.5151 accuracy: 0.4964\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss: 1.50828 accuracy: 0.4992\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss: 1.51655 accuracy: 0.4994\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss: 1.54118 accuracy: 0.4896\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss: 1.49136 accuracy: 0.4994\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss: 1.51482 accuracy: 0.495\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss: 1.50701 accuracy: 0.499\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss: 1.51308 accuracy: 0.4994\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss: 1.53817 accuracy: 0.4904\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss: 1.48834 accuracy: 0.5014\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss: 1.51267 accuracy: 0.4952\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss: 1.50575 accuracy: 0.4996\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss: 1.51289 accuracy: 0.501\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss: 1.53574 accuracy: 0.493\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss: 1.48888 accuracy: 0.5004\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss: 1.51052 accuracy: 0.4954\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss: 1.50328 accuracy: 0.5\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss: 1.5074 accuracy: 0.5016\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss: 1.53018 accuracy: 0.494\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss: 1.48864 accuracy: 0.502\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss: 1.50937 accuracy: 0.497\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss: 1.50264 accuracy: 0.5006\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss: 1.50725 accuracy: 0.5022\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss: 1.52771 accuracy: 0.4938\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss: 1.48598 accuracy: 0.5034\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss: 1.50731 accuracy: 0.4974\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss: 1.49412 accuracy: 0.503\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss: 1.50844 accuracy: 0.5002\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss: 1.52635 accuracy: 0.4958\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss: 1.48473 accuracy: 0.504\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss: 1.5118 accuracy: 0.4978\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss: 1.49679 accuracy: 0.5016\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss: 1.50812 accuracy: 0.5014\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss: 1.5262 accuracy: 0.4968\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss: 1.48351 accuracy: 0.5042\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss: 1.50951 accuracy: 0.4986\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss: 1.49652 accuracy: 0.502\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss: 1.50655 accuracy: 0.5024\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss: 1.52304 accuracy: 0.4964\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss: 1.47891 accuracy: 0.5054\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss: 1.51171 accuracy: 0.4998\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss: 1.49069 accuracy: 0.5018\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss: 1.5053 accuracy: 0.5032\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss: 1.52132 accuracy: 0.4974\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss: 1.47709 accuracy: 0.5034\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss: 1.50842 accuracy: 0.5018\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss: 1.49277 accuracy: 0.5032\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss: 1.50195 accuracy: 0.5032\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss: 1.52377 accuracy: 0.4968\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss: 1.47258 accuracy: 0.5056\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss: 1.50134 accuracy: 0.501\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss: 1.4895 accuracy: 0.5042\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss: 1.50462 accuracy: 0.5024\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss: 1.52241 accuracy: 0.4974\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss: 1.47667 accuracy: 0.5042\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss: 1.50406 accuracy: 0.5042\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss: 1.48959 accuracy: 0.5038\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss: 1.4994 accuracy: 0.5018\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss: 1.51791 accuracy: 0.499\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss: 1.47919 accuracy: 0.5032\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss: 1.49894 accuracy: 0.5012\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss: 1.49207 accuracy: 0.5022\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss: 1.49886 accuracy: 0.5028\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss: 1.51615 accuracy: 0.4982\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss: 1.47402 accuracy: 0.5078\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss: 1.49526 accuracy: 0.5032\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss: 1.48773 accuracy: 0.5034\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss: 1.49933 accuracy: 0.504\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss: 1.5204 accuracy: 0.499\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss: 1.47192 accuracy: 0.508\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss: 1.49506 accuracy: 0.5014\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss: 1.49285 accuracy: 0.502\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss: 1.49631 accuracy: 0.504\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss: 1.51563 accuracy: 0.4982\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss: 1.471 accuracy: 0.5082\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss: 1.49539 accuracy: 0.502\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss: 1.49327 accuracy: 0.5002\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss: 1.4941 accuracy: 0.5046\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss: 1.5155 accuracy: 0.5006\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss: 1.47101 accuracy: 0.5066\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss: 1.49147 accuracy: 0.5028\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss: 1.48797 accuracy: 0.5008\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss: 1.48971 accuracy: 0.5042\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss: 1.50712 accuracy: 0.5002\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss: 1.47437 accuracy: 0.508\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss: 1.49011 accuracy: 0.5024\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss: 1.4901 accuracy: 0.5008\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss: 1.49005 accuracy: 0.5046\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss: 1.50381 accuracy: 0.5012\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss: 1.47101 accuracy: 0.5094\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss: 1.4891 accuracy: 0.5062\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss: 1.48779 accuracy: 0.5032\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss: 1.49046 accuracy: 0.505\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss: 1.50855 accuracy: 0.5038\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss: 1.47377 accuracy: 0.5066\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss: 1.48896 accuracy: 0.504\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss: 1.48632 accuracy: 0.504\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss: 1.48989 accuracy: 0.5054\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss: 1.50613 accuracy: 0.5034\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss: 1.4689 accuracy: 0.5086\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss: 1.48556 accuracy: 0.5058\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss: 1.48459 accuracy: 0.5042\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss: 1.48763 accuracy: 0.5062\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss: 1.50395 accuracy: 0.5032\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss: 1.46884 accuracy: 0.5088\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss: 1.48693 accuracy: 0.5064\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss: 1.48659 accuracy: 0.5026\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss: 1.48805 accuracy: 0.5066\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss: 1.50521 accuracy: 0.5056\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss: 1.4698 accuracy: 0.5106\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss: 1.48478 accuracy: 0.5078\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss: 1.48551 accuracy: 0.5032\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss: 1.48597 accuracy: 0.5072\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss: 1.50275 accuracy: 0.5062\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss: 1.469 accuracy: 0.5104\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss: 1.48754 accuracy: 0.5078\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss: 1.48519 accuracy: 0.5042\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss: 1.48025 accuracy: 0.5072\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss: 1.49557 accuracy: 0.5062\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss: 1.47131 accuracy: 0.509\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss: 1.48534 accuracy: 0.5086\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss: 1.48072 accuracy: 0.5052\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss: 1.4853 accuracy: 0.507\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss: 1.49923 accuracy: 0.506\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss: 1.46611 accuracy: 0.5098\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss: 1.48333 accuracy: 0.509\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss: 1.47935 accuracy: 0.5062\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss: 1.48362 accuracy: 0.5066\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss: 1.50643 accuracy: 0.508\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss: 1.46778 accuracy: 0.509\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss: 1.48216 accuracy: 0.5086\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss: 1.48129 accuracy: 0.5056\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss: 1.48139 accuracy: 0.5058\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss: 1.49858 accuracy: 0.5086\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss: 1.46679 accuracy: 0.5096\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss: 1.48285 accuracy: 0.5102\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss: 1.4807 accuracy: 0.5054\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss: 1.48338 accuracy: 0.507\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss: 1.49815 accuracy: 0.5108\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss: 1.46815 accuracy: 0.509\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss: 1.48082 accuracy: 0.5062\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss: 1.47607 accuracy: 0.5068\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss: 1.47901 accuracy: 0.5058\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss: 1.49743 accuracy: 0.5104\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss: 1.46275 accuracy: 0.512\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss: 1.483 accuracy: 0.5064\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss: 1.47777 accuracy: 0.5062\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss: 1.4797 accuracy: 0.5064\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss: 1.49097 accuracy: 0.5112\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss: 1.46408 accuracy: 0.5092\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss: 1.4819 accuracy: 0.5072\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss: 1.47818 accuracy: 0.5068\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss: 1.47844 accuracy: 0.5068\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss: 1.49154 accuracy: 0.5124\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss: 1.46339 accuracy: 0.5096\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss: 1.47912 accuracy: 0.51\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss: 1.47977 accuracy: 0.5066\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss: 1.47708 accuracy: 0.5074\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss: 1.49201 accuracy: 0.5124\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss: 1.4659 accuracy: 0.509\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss: 1.48289 accuracy: 0.5104\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss: 1.4808 accuracy: 0.5064\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss: 1.47331 accuracy: 0.5082\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss: 1.48957 accuracy: 0.5136\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss: 1.46407 accuracy: 0.51\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss: 1.47986 accuracy: 0.5082\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss: 1.48046 accuracy: 0.5074\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss: 1.47713 accuracy: 0.5086\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss: 1.48546 accuracy: 0.5142\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss: 1.46478 accuracy: 0.5098\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss: 1.47773 accuracy: 0.5104\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss: 1.48143 accuracy: 0.507\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss: 1.4714 accuracy: 0.5072\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss: 1.48648 accuracy: 0.5124\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss: 1.4637 accuracy: 0.5112\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss: 1.47462 accuracy: 0.5108\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss: 1.48292 accuracy: 0.5064\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss: 1.46482 accuracy: 0.5074\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss: 1.47917 accuracy: 0.5124\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss: 1.46333 accuracy: 0.5102\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss: 1.47685 accuracy: 0.5112\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss: 1.48434 accuracy: 0.5072\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss: 1.46606 accuracy: 0.5086\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss: 1.48241 accuracy: 0.513\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss: 1.45751 accuracy: 0.512\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss: 1.47419 accuracy: 0.5128\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss: 1.48916 accuracy: 0.507\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss: 1.46547 accuracy: 0.509\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss: 1.48282 accuracy: 0.5148\n",
      "Epoch 201, CIFAR-10 Batch 2:  loss: 1.45962 accuracy: 0.511\n",
      "Epoch 201, CIFAR-10 Batch 3:  loss: 1.4725 accuracy: 0.5142\n",
      "Epoch 201, CIFAR-10 Batch 4:  loss: 1.48909 accuracy: 0.5054\n",
      "Epoch 201, CIFAR-10 Batch 5:  loss: 1.46456 accuracy: 0.5104\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss: 1.48393 accuracy: 0.5162\n",
      "Epoch 202, CIFAR-10 Batch 2:  loss: 1.45978 accuracy: 0.5122\n",
      "Epoch 202, CIFAR-10 Batch 3:  loss: 1.46977 accuracy: 0.514\n",
      "Epoch 202, CIFAR-10 Batch 4:  loss: 1.48752 accuracy: 0.5038\n",
      "Epoch 202, CIFAR-10 Batch 5:  loss: 1.46347 accuracy: 0.5108\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss: 1.48794 accuracy: 0.5164\n",
      "Epoch 203, CIFAR-10 Batch 2:  loss: 1.45959 accuracy: 0.5118\n",
      "Epoch 203, CIFAR-10 Batch 3:  loss: 1.47579 accuracy: 0.5146\n",
      "Epoch 203, CIFAR-10 Batch 4:  loss: 1.48952 accuracy: 0.505\n",
      "Epoch 203, CIFAR-10 Batch 5:  loss: 1.45885 accuracy: 0.512\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss: 1.49551 accuracy: 0.517\n",
      "Epoch 204, CIFAR-10 Batch 2:  loss: 1.45856 accuracy: 0.5116\n",
      "Epoch 204, CIFAR-10 Batch 3:  loss: 1.47038 accuracy: 0.5166\n",
      "Epoch 204, CIFAR-10 Batch 4:  loss: 1.48958 accuracy: 0.5056\n",
      "Epoch 204, CIFAR-10 Batch 5:  loss: 1.45892 accuracy: 0.5122\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss: 1.48556 accuracy: 0.5164\n",
      "Epoch 205, CIFAR-10 Batch 2:  loss: 1.46057 accuracy: 0.5122\n",
      "Epoch 205, CIFAR-10 Batch 3:  loss: 1.47051 accuracy: 0.515\n",
      "Epoch 205, CIFAR-10 Batch 4:  loss: 1.48893 accuracy: 0.5042\n",
      "Epoch 205, CIFAR-10 Batch 5:  loss: 1.45736 accuracy: 0.5134\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss: 1.48502 accuracy: 0.5172\n",
      "Epoch 206, CIFAR-10 Batch 2:  loss: 1.46054 accuracy: 0.5126\n",
      "Epoch 206, CIFAR-10 Batch 3:  loss: 1.4711 accuracy: 0.5158\n",
      "Epoch 206, CIFAR-10 Batch 4:  loss: 1.49049 accuracy: 0.5044\n",
      "Epoch 206, CIFAR-10 Batch 5:  loss: 1.45895 accuracy: 0.5146\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss: 1.49085 accuracy: 0.5196\n",
      "Epoch 207, CIFAR-10 Batch 2:  loss: 1.45753 accuracy: 0.5138\n",
      "Epoch 207, CIFAR-10 Batch 3:  loss: 1.47545 accuracy: 0.5148\n",
      "Epoch 207, CIFAR-10 Batch 4:  loss: 1.48658 accuracy: 0.506\n",
      "Epoch 207, CIFAR-10 Batch 5:  loss: 1.46063 accuracy: 0.515\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss: 1.49081 accuracy: 0.518\n",
      "Epoch 208, CIFAR-10 Batch 2:  loss: 1.45297 accuracy: 0.5158\n",
      "Epoch 208, CIFAR-10 Batch 3:  loss: 1.47312 accuracy: 0.5162\n",
      "Epoch 208, CIFAR-10 Batch 4:  loss: 1.48855 accuracy: 0.5064\n",
      "Epoch 208, CIFAR-10 Batch 5:  loss: 1.4569 accuracy: 0.513\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss: 1.48812 accuracy: 0.5188\n",
      "Epoch 209, CIFAR-10 Batch 2:  loss: 1.45349 accuracy: 0.516\n",
      "Epoch 209, CIFAR-10 Batch 3:  loss: 1.47565 accuracy: 0.5162\n",
      "Epoch 209, CIFAR-10 Batch 4:  loss: 1.48842 accuracy: 0.5078\n",
      "Epoch 209, CIFAR-10 Batch 5:  loss: 1.45803 accuracy: 0.5136\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss: 1.48673 accuracy: 0.5194\n",
      "Epoch 210, CIFAR-10 Batch 2:  loss: 1.45283 accuracy: 0.5168\n",
      "Epoch 210, CIFAR-10 Batch 3:  loss: 1.47509 accuracy: 0.5172\n",
      "Epoch 210, CIFAR-10 Batch 4:  loss: 1.47566 accuracy: 0.5088\n",
      "Epoch 210, CIFAR-10 Batch 5:  loss: 1.46237 accuracy: 0.5148\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss: 1.47532 accuracy: 0.5186\n",
      "Epoch 211, CIFAR-10 Batch 2:  loss: 1.45197 accuracy: 0.518\n",
      "Epoch 211, CIFAR-10 Batch 3:  loss: 1.46893 accuracy: 0.5158\n",
      "Epoch 211, CIFAR-10 Batch 4:  loss: 1.47796 accuracy: 0.5096\n",
      "Epoch 211, CIFAR-10 Batch 5:  loss: 1.4608 accuracy: 0.5156\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss: 1.48205 accuracy: 0.521\n",
      "Epoch 212, CIFAR-10 Batch 2:  loss: 1.45237 accuracy: 0.5164\n",
      "Epoch 212, CIFAR-10 Batch 3:  loss: 1.45624 accuracy: 0.519\n",
      "Epoch 212, CIFAR-10 Batch 4:  loss: 1.47679 accuracy: 0.5098\n",
      "Epoch 212, CIFAR-10 Batch 5:  loss: 1.46416 accuracy: 0.515\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss: 1.48096 accuracy: 0.5206\n",
      "Epoch 213, CIFAR-10 Batch 2:  loss: 1.44907 accuracy: 0.5188\n",
      "Epoch 213, CIFAR-10 Batch 3:  loss: 1.45954 accuracy: 0.519\n",
      "Epoch 213, CIFAR-10 Batch 4:  loss: 1.47369 accuracy: 0.5098\n",
      "Epoch 213, CIFAR-10 Batch 5:  loss: 1.46519 accuracy: 0.5162\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss: 1.4805 accuracy: 0.5214\n",
      "Epoch 214, CIFAR-10 Batch 2:  loss: 1.44857 accuracy: 0.5188\n",
      "Epoch 214, CIFAR-10 Batch 3:  loss: 1.45863 accuracy: 0.5188\n",
      "Epoch 214, CIFAR-10 Batch 4:  loss: 1.47216 accuracy: 0.511\n",
      "Epoch 214, CIFAR-10 Batch 5:  loss: 1.46678 accuracy: 0.5162\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss: 1.47865 accuracy: 0.5218\n",
      "Epoch 215, CIFAR-10 Batch 2:  loss: 1.44539 accuracy: 0.5184\n",
      "Epoch 215, CIFAR-10 Batch 3:  loss: 1.46174 accuracy: 0.5188\n",
      "Epoch 215, CIFAR-10 Batch 4:  loss: 1.47586 accuracy: 0.5118\n",
      "Epoch 215, CIFAR-10 Batch 5:  loss: 1.45941 accuracy: 0.5156\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss: 1.47084 accuracy: 0.518\n",
      "Epoch 216, CIFAR-10 Batch 2:  loss: 1.44644 accuracy: 0.5204\n",
      "Epoch 216, CIFAR-10 Batch 3:  loss: 1.45782 accuracy: 0.518\n",
      "Epoch 216, CIFAR-10 Batch 4:  loss: 1.47441 accuracy: 0.5128\n",
      "Epoch 216, CIFAR-10 Batch 5:  loss: 1.46129 accuracy: 0.5156\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss: 1.47464 accuracy: 0.5228\n",
      "Epoch 217, CIFAR-10 Batch 2:  loss: 1.44154 accuracy: 0.5184\n",
      "Epoch 217, CIFAR-10 Batch 3:  loss: 1.46069 accuracy: 0.5208\n",
      "Epoch 217, CIFAR-10 Batch 4:  loss: 1.4652 accuracy: 0.5134\n",
      "Epoch 217, CIFAR-10 Batch 5:  loss: 1.45978 accuracy: 0.515\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss: 1.47031 accuracy: 0.5184\n",
      "Epoch 218, CIFAR-10 Batch 2:  loss: 1.44596 accuracy: 0.5222\n",
      "Epoch 218, CIFAR-10 Batch 3:  loss: 1.45674 accuracy: 0.519\n",
      "Epoch 218, CIFAR-10 Batch 4:  loss: 1.4671 accuracy: 0.5134\n",
      "Epoch 218, CIFAR-10 Batch 5:  loss: 1.46648 accuracy: 0.5174\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss: 1.47352 accuracy: 0.5232\n",
      "Epoch 219, CIFAR-10 Batch 2:  loss: 1.43086 accuracy: 0.5204\n",
      "Epoch 219, CIFAR-10 Batch 3:  loss: 1.44926 accuracy: 0.5204\n",
      "Epoch 219, CIFAR-10 Batch 4:  loss: 1.46573 accuracy: 0.512\n",
      "Epoch 219, CIFAR-10 Batch 5:  loss: 1.46368 accuracy: 0.5174\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss: 1.46786 accuracy: 0.5186\n",
      "Epoch 220, CIFAR-10 Batch 2:  loss: 1.43929 accuracy: 0.5232\n",
      "Epoch 220, CIFAR-10 Batch 3:  loss: 1.45238 accuracy: 0.5184\n",
      "Epoch 220, CIFAR-10 Batch 4:  loss: 1.46131 accuracy: 0.5146\n",
      "Epoch 220, CIFAR-10 Batch 5:  loss: 1.46816 accuracy: 0.5174\n",
      "Epoch 221, CIFAR-10 Batch 1:  loss: 1.47049 accuracy: 0.5212\n",
      "Epoch 221, CIFAR-10 Batch 2:  loss: 1.43379 accuracy: 0.5238\n",
      "Epoch 221, CIFAR-10 Batch 3:  loss: 1.45337 accuracy: 0.5198\n",
      "Epoch 221, CIFAR-10 Batch 4:  loss: 1.45929 accuracy: 0.5154\n",
      "Epoch 221, CIFAR-10 Batch 5:  loss: 1.46508 accuracy: 0.5166\n",
      "Epoch 222, CIFAR-10 Batch 1:  loss: 1.46453 accuracy: 0.5182\n",
      "Epoch 222, CIFAR-10 Batch 2:  loss: 1.43405 accuracy: 0.5248\n",
      "Epoch 222, CIFAR-10 Batch 3:  loss: 1.44279 accuracy: 0.519\n",
      "Epoch 222, CIFAR-10 Batch 4:  loss: 1.45976 accuracy: 0.5168\n",
      "Epoch 222, CIFAR-10 Batch 5:  loss: 1.46807 accuracy: 0.5202\n",
      "Epoch 223, CIFAR-10 Batch 1:  loss: 1.46789 accuracy: 0.5198\n",
      "Epoch 223, CIFAR-10 Batch 2:  loss: 1.42881 accuracy: 0.5228\n",
      "Epoch 223, CIFAR-10 Batch 3:  loss: 1.44913 accuracy: 0.521\n",
      "Epoch 223, CIFAR-10 Batch 4:  loss: 1.45354 accuracy: 0.5148\n",
      "Epoch 223, CIFAR-10 Batch 5:  loss: 1.45989 accuracy: 0.5188\n",
      "Epoch 224, CIFAR-10 Batch 1:  loss: 1.46927 accuracy: 0.5206\n",
      "Epoch 224, CIFAR-10 Batch 2:  loss: 1.42912 accuracy: 0.5222\n",
      "Epoch 224, CIFAR-10 Batch 3:  loss: 1.44392 accuracy: 0.5208\n",
      "Epoch 224, CIFAR-10 Batch 4:  loss: 1.45969 accuracy: 0.5164\n",
      "Epoch 224, CIFAR-10 Batch 5:  loss: 1.46072 accuracy: 0.518\n",
      "Epoch 225, CIFAR-10 Batch 1:  loss: 1.47304 accuracy: 0.5204\n",
      "Epoch 225, CIFAR-10 Batch 2:  loss: 1.4276 accuracy: 0.5254\n",
      "Epoch 225, CIFAR-10 Batch 3:  loss: 1.44481 accuracy: 0.5222\n",
      "Epoch 225, CIFAR-10 Batch 4:  loss: 1.45433 accuracy: 0.5174\n",
      "Epoch 225, CIFAR-10 Batch 5:  loss: 1.4666 accuracy: 0.5176\n",
      "Epoch 226, CIFAR-10 Batch 1:  loss: 1.4717 accuracy: 0.5184\n",
      "Epoch 226, CIFAR-10 Batch 2:  loss: 1.42873 accuracy: 0.523\n",
      "Epoch 226, CIFAR-10 Batch 3:  loss: 1.44873 accuracy: 0.521\n",
      "Epoch 226, CIFAR-10 Batch 4:  loss: 1.45732 accuracy: 0.5158\n",
      "Epoch 226, CIFAR-10 Batch 5:  loss: 1.46742 accuracy: 0.5182\n",
      "Epoch 227, CIFAR-10 Batch 1:  loss: 1.47474 accuracy: 0.5182\n",
      "Epoch 227, CIFAR-10 Batch 2:  loss: 1.43034 accuracy: 0.5232\n",
      "Epoch 227, CIFAR-10 Batch 3:  loss: 1.44767 accuracy: 0.5212\n",
      "Epoch 227, CIFAR-10 Batch 4:  loss: 1.45348 accuracy: 0.517\n",
      "Epoch 227, CIFAR-10 Batch 5:  loss: 1.46392 accuracy: 0.5182\n",
      "Epoch 228, CIFAR-10 Batch 1:  loss: 1.47164 accuracy: 0.5208\n",
      "Epoch 228, CIFAR-10 Batch 2:  loss: 1.42686 accuracy: 0.5242\n",
      "Epoch 228, CIFAR-10 Batch 3:  loss: 1.44899 accuracy: 0.5212\n",
      "Epoch 228, CIFAR-10 Batch 4:  loss: 1.45881 accuracy: 0.516\n",
      "Epoch 228, CIFAR-10 Batch 5:  loss: 1.46539 accuracy: 0.518\n",
      "Epoch 229, CIFAR-10 Batch 1:  loss: 1.47409 accuracy: 0.5222\n",
      "Epoch 229, CIFAR-10 Batch 2:  loss: 1.43222 accuracy: 0.5246\n",
      "Epoch 229, CIFAR-10 Batch 3:  loss: 1.4469 accuracy: 0.5212\n",
      "Epoch 229, CIFAR-10 Batch 4:  loss: 1.45633 accuracy: 0.5164\n",
      "Epoch 229, CIFAR-10 Batch 5:  loss: 1.46244 accuracy: 0.5172\n",
      "Epoch 230, CIFAR-10 Batch 1:  loss: 1.4739 accuracy: 0.5198\n",
      "Epoch 230, CIFAR-10 Batch 2:  loss: 1.43054 accuracy: 0.5232\n",
      "Epoch 230, CIFAR-10 Batch 3:  loss: 1.44694 accuracy: 0.5218\n",
      "Epoch 230, CIFAR-10 Batch 4:  loss: 1.4589 accuracy: 0.5156\n",
      "Epoch 230, CIFAR-10 Batch 5:  loss: 1.45991 accuracy: 0.5216\n",
      "Epoch 231, CIFAR-10 Batch 1:  loss: 1.47684 accuracy: 0.521\n",
      "Epoch 231, CIFAR-10 Batch 2:  loss: 1.43046 accuracy: 0.5232\n",
      "Epoch 231, CIFAR-10 Batch 3:  loss: 1.44568 accuracy: 0.5222\n",
      "Epoch 231, CIFAR-10 Batch 4:  loss: 1.45953 accuracy: 0.5194\n",
      "Epoch 231, CIFAR-10 Batch 5:  loss: 1.45651 accuracy: 0.5178\n",
      "Epoch 232, CIFAR-10 Batch 1:  loss: 1.47736 accuracy: 0.521\n",
      "Epoch 232, CIFAR-10 Batch 2:  loss: 1.4294 accuracy: 0.5252\n",
      "Epoch 232, CIFAR-10 Batch 3:  loss: 1.44546 accuracy: 0.5214\n",
      "Epoch 232, CIFAR-10 Batch 4:  loss: 1.46453 accuracy: 0.5172\n",
      "Epoch 232, CIFAR-10 Batch 5:  loss: 1.45857 accuracy: 0.519\n",
      "Epoch 233, CIFAR-10 Batch 1:  loss: 1.48005 accuracy: 0.5214\n",
      "Epoch 233, CIFAR-10 Batch 2:  loss: 1.43027 accuracy: 0.5268\n",
      "Epoch 233, CIFAR-10 Batch 3:  loss: 1.44452 accuracy: 0.5214\n",
      "Epoch 233, CIFAR-10 Batch 4:  loss: 1.4593 accuracy: 0.5186\n",
      "Epoch 233, CIFAR-10 Batch 5:  loss: 1.45737 accuracy: 0.5184\n",
      "Epoch 234, CIFAR-10 Batch 1:  loss: 1.48254 accuracy: 0.5216\n",
      "Epoch 234, CIFAR-10 Batch 2:  loss: 1.43554 accuracy: 0.5264\n",
      "Epoch 234, CIFAR-10 Batch 3:  loss: 1.44729 accuracy: 0.5216\n",
      "Epoch 234, CIFAR-10 Batch 4:  loss: 1.46037 accuracy: 0.5194\n",
      "Epoch 234, CIFAR-10 Batch 5:  loss: 1.45868 accuracy: 0.5186\n",
      "Epoch 235, CIFAR-10 Batch 1:  loss: 1.48011 accuracy: 0.5222\n",
      "Epoch 235, CIFAR-10 Batch 2:  loss: 1.43357 accuracy: 0.5248\n",
      "Epoch 235, CIFAR-10 Batch 3:  loss: 1.44718 accuracy: 0.5234\n",
      "Epoch 235, CIFAR-10 Batch 4:  loss: 1.46912 accuracy: 0.5198\n",
      "Epoch 235, CIFAR-10 Batch 5:  loss: 1.4563 accuracy: 0.5216\n",
      "Epoch 236, CIFAR-10 Batch 1:  loss: 1.47952 accuracy: 0.5212\n",
      "Epoch 236, CIFAR-10 Batch 2:  loss: 1.43385 accuracy: 0.525\n",
      "Epoch 236, CIFAR-10 Batch 3:  loss: 1.45118 accuracy: 0.522\n",
      "Epoch 236, CIFAR-10 Batch 4:  loss: 1.46482 accuracy: 0.5202\n",
      "Epoch 236, CIFAR-10 Batch 5:  loss: 1.45825 accuracy: 0.5198\n",
      "Epoch 237, CIFAR-10 Batch 1:  loss: 1.48375 accuracy: 0.5226\n",
      "Epoch 237, CIFAR-10 Batch 2:  loss: 1.43529 accuracy: 0.5248\n",
      "Epoch 237, CIFAR-10 Batch 3:  loss: 1.44448 accuracy: 0.5222\n",
      "Epoch 237, CIFAR-10 Batch 4:  loss: 1.46119 accuracy: 0.5212\n",
      "Epoch 237, CIFAR-10 Batch 5:  loss: 1.4548 accuracy: 0.521\n",
      "Epoch 238, CIFAR-10 Batch 1:  loss: 1.47742 accuracy: 0.5214\n",
      "Epoch 238, CIFAR-10 Batch 2:  loss: 1.43565 accuracy: 0.5262\n",
      "Epoch 238, CIFAR-10 Batch 3:  loss: 1.45053 accuracy: 0.5222\n",
      "Epoch 238, CIFAR-10 Batch 4:  loss: 1.45777 accuracy: 0.5208\n",
      "Epoch 238, CIFAR-10 Batch 5:  loss: 1.4575 accuracy: 0.5214\n",
      "Epoch 239, CIFAR-10 Batch 1:  loss: 1.48363 accuracy: 0.5222\n",
      "Epoch 239, CIFAR-10 Batch 2:  loss: 1.43464 accuracy: 0.5262\n",
      "Epoch 239, CIFAR-10 Batch 3:  loss: 1.44728 accuracy: 0.5206\n",
      "Epoch 239, CIFAR-10 Batch 4:  loss: 1.4634 accuracy: 0.5222\n",
      "Epoch 239, CIFAR-10 Batch 5:  loss: 1.4561 accuracy: 0.5226\n",
      "Epoch 240, CIFAR-10 Batch 1:  loss: 1.47926 accuracy: 0.523\n",
      "Epoch 240, CIFAR-10 Batch 2:  loss: 1.43507 accuracy: 0.5232\n",
      "Epoch 240, CIFAR-10 Batch 3:  loss: 1.44331 accuracy: 0.5218\n",
      "Epoch 240, CIFAR-10 Batch 4:  loss: 1.45988 accuracy: 0.5222\n",
      "Epoch 240, CIFAR-10 Batch 5:  loss: 1.4596 accuracy: 0.5234\n",
      "Epoch 241, CIFAR-10 Batch 1:  loss: 1.48142 accuracy: 0.5236\n",
      "Epoch 241, CIFAR-10 Batch 2:  loss: 1.43351 accuracy: 0.5232\n",
      "Epoch 241, CIFAR-10 Batch 3:  loss: 1.44614 accuracy: 0.5216\n",
      "Epoch 241, CIFAR-10 Batch 4:  loss: 1.45635 accuracy: 0.5214\n",
      "Epoch 241, CIFAR-10 Batch 5:  loss: 1.45776 accuracy: 0.5232\n",
      "Epoch 242, CIFAR-10 Batch 1:  loss: 1.4795 accuracy: 0.5246\n",
      "Epoch 242, CIFAR-10 Batch 2:  loss: 1.43509 accuracy: 0.5258\n",
      "Epoch 242, CIFAR-10 Batch 3:  loss: 1.44869 accuracy: 0.5228\n",
      "Epoch 242, CIFAR-10 Batch 4:  loss: 1.46144 accuracy: 0.5218\n",
      "Epoch 242, CIFAR-10 Batch 5:  loss: 1.46152 accuracy: 0.5232\n",
      "Epoch 243, CIFAR-10 Batch 1:  loss: 1.47761 accuracy: 0.525\n",
      "Epoch 243, CIFAR-10 Batch 2:  loss: 1.43796 accuracy: 0.526\n",
      "Epoch 243, CIFAR-10 Batch 3:  loss: 1.45042 accuracy: 0.5206\n",
      "Epoch 243, CIFAR-10 Batch 4:  loss: 1.46044 accuracy: 0.522\n",
      "Epoch 243, CIFAR-10 Batch 5:  loss: 1.45371 accuracy: 0.524\n",
      "Epoch 244, CIFAR-10 Batch 1:  loss: 1.47723 accuracy: 0.5226\n",
      "Epoch 244, CIFAR-10 Batch 2:  loss: 1.43813 accuracy: 0.525\n",
      "Epoch 244, CIFAR-10 Batch 3:  loss: 1.44662 accuracy: 0.524\n",
      "Epoch 244, CIFAR-10 Batch 4:  loss: 1.45849 accuracy: 0.5234\n",
      "Epoch 244, CIFAR-10 Batch 5:  loss: 1.45963 accuracy: 0.5244\n",
      "Epoch 245, CIFAR-10 Batch 1:  loss: 1.47798 accuracy: 0.526\n",
      "Epoch 245, CIFAR-10 Batch 2:  loss: 1.43711 accuracy: 0.5244\n",
      "Epoch 245, CIFAR-10 Batch 3:  loss: 1.44775 accuracy: 0.5224\n",
      "Epoch 245, CIFAR-10 Batch 4:  loss: 1.46134 accuracy: 0.5222\n",
      "Epoch 245, CIFAR-10 Batch 5:  loss: 1.4584 accuracy: 0.5264\n",
      "Epoch 246, CIFAR-10 Batch 1:  loss: 1.47918 accuracy: 0.5228\n",
      "Epoch 246, CIFAR-10 Batch 2:  loss: 1.44153 accuracy: 0.5238\n",
      "Epoch 246, CIFAR-10 Batch 3:  loss: 1.44826 accuracy: 0.5212\n",
      "Epoch 246, CIFAR-10 Batch 4:  loss: 1.45697 accuracy: 0.5226\n",
      "Epoch 246, CIFAR-10 Batch 5:  loss: 1.45764 accuracy: 0.5258\n",
      "Epoch 247, CIFAR-10 Batch 1:  loss: 1.47694 accuracy: 0.5244\n",
      "Epoch 247, CIFAR-10 Batch 2:  loss: 1.44238 accuracy: 0.523\n",
      "Epoch 247, CIFAR-10 Batch 3:  loss: 1.44633 accuracy: 0.5224\n",
      "Epoch 247, CIFAR-10 Batch 4:  loss: 1.45978 accuracy: 0.5218\n",
      "Epoch 247, CIFAR-10 Batch 5:  loss: 1.46639 accuracy: 0.5244\n",
      "Epoch 248, CIFAR-10 Batch 1:  loss: 1.48152 accuracy: 0.525\n",
      "Epoch 248, CIFAR-10 Batch 2:  loss: 1.44227 accuracy: 0.5242\n",
      "Epoch 248, CIFAR-10 Batch 3:  loss: 1.44758 accuracy: 0.5232\n",
      "Epoch 248, CIFAR-10 Batch 4:  loss: 1.46072 accuracy: 0.5226\n",
      "Epoch 248, CIFAR-10 Batch 5:  loss: 1.45776 accuracy: 0.5244\n",
      "Epoch 249, CIFAR-10 Batch 1:  loss: 1.47785 accuracy: 0.5244\n",
      "Epoch 249, CIFAR-10 Batch 2:  loss: 1.44111 accuracy: 0.5252\n",
      "Epoch 249, CIFAR-10 Batch 3:  loss: 1.44827 accuracy: 0.5214\n",
      "Epoch 249, CIFAR-10 Batch 4:  loss: 1.46512 accuracy: 0.5244\n",
      "Epoch 249, CIFAR-10 Batch 5:  loss: 1.46068 accuracy: 0.5232\n",
      "Epoch 250, CIFAR-10 Batch 1:  loss: 1.47997 accuracy: 0.5264\n",
      "Epoch 250, CIFAR-10 Batch 2:  loss: 1.44204 accuracy: 0.523\n",
      "Epoch 250, CIFAR-10 Batch 3:  loss: 1.44741 accuracy: 0.5216\n",
      "Epoch 250, CIFAR-10 Batch 4:  loss: 1.46803 accuracy: 0.5226\n",
      "Epoch 250, CIFAR-10 Batch 5:  loss: 1.45535 accuracy: 0.5222\n",
      "Epoch 251, CIFAR-10 Batch 1:  loss: 1.48065 accuracy: 0.524\n",
      "Epoch 251, CIFAR-10 Batch 2:  loss: 1.44079 accuracy: 0.525\n",
      "Epoch 251, CIFAR-10 Batch 3:  loss: 1.445 accuracy: 0.5226\n",
      "Epoch 251, CIFAR-10 Batch 4:  loss: 1.4742 accuracy: 0.5248\n",
      "Epoch 251, CIFAR-10 Batch 5:  loss: 1.45602 accuracy: 0.5248\n",
      "Epoch 252, CIFAR-10 Batch 1:  loss: 1.47854 accuracy: 0.525\n",
      "Epoch 252, CIFAR-10 Batch 2:  loss: 1.43815 accuracy: 0.524\n",
      "Epoch 252, CIFAR-10 Batch 3:  loss: 1.44797 accuracy: 0.5234\n",
      "Epoch 252, CIFAR-10 Batch 4:  loss: 1.47206 accuracy: 0.525\n",
      "Epoch 252, CIFAR-10 Batch 5:  loss: 1.44471 accuracy: 0.5244\n",
      "Epoch 253, CIFAR-10 Batch 1:  loss: 1.47247 accuracy: 0.525\n",
      "Epoch 253, CIFAR-10 Batch 2:  loss: 1.4366 accuracy: 0.5238\n",
      "Epoch 253, CIFAR-10 Batch 3:  loss: 1.4479 accuracy: 0.5228\n",
      "Epoch 253, CIFAR-10 Batch 4:  loss: 1.4679 accuracy: 0.5252\n",
      "Epoch 253, CIFAR-10 Batch 5:  loss: 1.44889 accuracy: 0.5266\n",
      "Epoch 254, CIFAR-10 Batch 1:  loss: 1.47956 accuracy: 0.5278\n",
      "Epoch 254, CIFAR-10 Batch 2:  loss: 1.4306 accuracy: 0.5246\n",
      "Epoch 254, CIFAR-10 Batch 3:  loss: 1.43712 accuracy: 0.5218\n",
      "Epoch 254, CIFAR-10 Batch 4:  loss: 1.47441 accuracy: 0.5242\n",
      "Epoch 254, CIFAR-10 Batch 5:  loss: 1.44297 accuracy: 0.5266\n",
      "Epoch 255, CIFAR-10 Batch 1:  loss: 1.47527 accuracy: 0.5278\n",
      "Epoch 255, CIFAR-10 Batch 2:  loss: 1.4307 accuracy: 0.5266\n",
      "Epoch 255, CIFAR-10 Batch 3:  loss: 1.4419 accuracy: 0.5216\n",
      "Epoch 255, CIFAR-10 Batch 4:  loss: 1.46974 accuracy: 0.525\n",
      "Epoch 255, CIFAR-10 Batch 5:  loss: 1.44508 accuracy: 0.525\n",
      "Epoch 256, CIFAR-10 Batch 1:  loss: 1.47467 accuracy: 0.525\n",
      "Epoch 256, CIFAR-10 Batch 2:  loss: 1.43543 accuracy: 0.5276\n",
      "Epoch 256, CIFAR-10 Batch 3:  loss: 1.44156 accuracy: 0.5252\n",
      "Epoch 256, CIFAR-10 Batch 4:  loss: 1.47455 accuracy: 0.5244\n",
      "Epoch 256, CIFAR-10 Batch 5:  loss: 1.4418 accuracy: 0.5264\n",
      "Epoch 257, CIFAR-10 Batch 1:  loss: 1.47982 accuracy: 0.5246\n",
      "Epoch 257, CIFAR-10 Batch 2:  loss: 1.4366 accuracy: 0.5256\n",
      "Epoch 257, CIFAR-10 Batch 3:  loss: 1.44704 accuracy: 0.5226\n",
      "Epoch 257, CIFAR-10 Batch 4:  loss: 1.46748 accuracy: 0.5266\n",
      "Epoch 257, CIFAR-10 Batch 5:  loss: 1.44428 accuracy: 0.5264\n",
      "Epoch 258, CIFAR-10 Batch 1:  loss: 1.48248 accuracy: 0.527\n",
      "Epoch 258, CIFAR-10 Batch 2:  loss: 1.43803 accuracy: 0.5266\n",
      "Epoch 258, CIFAR-10 Batch 3:  loss: 1.4412 accuracy: 0.5236\n",
      "Epoch 258, CIFAR-10 Batch 4:  loss: 1.4664 accuracy: 0.5274\n",
      "Epoch 258, CIFAR-10 Batch 5:  loss: 1.45156 accuracy: 0.5244\n",
      "Epoch 259, CIFAR-10 Batch 1:  loss: 1.47763 accuracy: 0.5274\n",
      "Epoch 259, CIFAR-10 Batch 2:  loss: 1.43447 accuracy: 0.5258\n",
      "Epoch 259, CIFAR-10 Batch 3:  loss: 1.4393 accuracy: 0.5234\n",
      "Epoch 259, CIFAR-10 Batch 4:  loss: 1.46468 accuracy: 0.528\n",
      "Epoch 259, CIFAR-10 Batch 5:  loss: 1.44568 accuracy: 0.5268\n",
      "Epoch 260, CIFAR-10 Batch 1:  loss: 1.47417 accuracy: 0.5284\n",
      "Epoch 260, CIFAR-10 Batch 2:  loss: 1.44062 accuracy: 0.5244\n",
      "Epoch 260, CIFAR-10 Batch 3:  loss: 1.44782 accuracy: 0.524\n",
      "Epoch 260, CIFAR-10 Batch 4:  loss: 1.47031 accuracy: 0.528\n",
      "Epoch 260, CIFAR-10 Batch 5:  loss: 1.4481 accuracy: 0.5268\n",
      "Epoch 261, CIFAR-10 Batch 1:  loss: 1.48131 accuracy: 0.524\n",
      "Epoch 261, CIFAR-10 Batch 2:  loss: 1.43691 accuracy: 0.5254\n",
      "Epoch 261, CIFAR-10 Batch 3:  loss: 1.44314 accuracy: 0.5238\n",
      "Epoch 261, CIFAR-10 Batch 4:  loss: 1.46272 accuracy: 0.5276\n",
      "Epoch 261, CIFAR-10 Batch 5:  loss: 1.44846 accuracy: 0.5256\n",
      "Epoch 262, CIFAR-10 Batch 1:  loss: 1.47834 accuracy: 0.526\n",
      "Epoch 262, CIFAR-10 Batch 2:  loss: 1.44181 accuracy: 0.5256\n",
      "Epoch 262, CIFAR-10 Batch 3:  loss: 1.44635 accuracy: 0.5234\n",
      "Epoch 262, CIFAR-10 Batch 4:  loss: 1.45659 accuracy: 0.5288\n",
      "Epoch 262, CIFAR-10 Batch 5:  loss: 1.4489 accuracy: 0.5278\n",
      "Epoch 263, CIFAR-10 Batch 1:  loss: 1.48394 accuracy: 0.5238\n",
      "Epoch 263, CIFAR-10 Batch 2:  loss: 1.44054 accuracy: 0.5276\n",
      "Epoch 263, CIFAR-10 Batch 3:  loss: 1.44868 accuracy: 0.5242\n",
      "Epoch 263, CIFAR-10 Batch 4:  loss: 1.46265 accuracy: 0.5298\n",
      "Epoch 263, CIFAR-10 Batch 5:  loss: 1.45064 accuracy: 0.5266\n",
      "Epoch 264, CIFAR-10 Batch 1:  loss: 1.48172 accuracy: 0.5268\n",
      "Epoch 264, CIFAR-10 Batch 2:  loss: 1.44555 accuracy: 0.5252\n",
      "Epoch 264, CIFAR-10 Batch 3:  loss: 1.44682 accuracy: 0.5244\n",
      "Epoch 264, CIFAR-10 Batch 4:  loss: 1.45443 accuracy: 0.5306\n",
      "Epoch 264, CIFAR-10 Batch 5:  loss: 1.45293 accuracy: 0.5294\n",
      "Epoch 265, CIFAR-10 Batch 1:  loss: 1.48212 accuracy: 0.5258\n",
      "Epoch 265, CIFAR-10 Batch 2:  loss: 1.44129 accuracy: 0.5278\n",
      "Epoch 265, CIFAR-10 Batch 3:  loss: 1.44927 accuracy: 0.5272\n",
      "Epoch 265, CIFAR-10 Batch 4:  loss: 1.46309 accuracy: 0.5312\n",
      "Epoch 265, CIFAR-10 Batch 5:  loss: 1.45202 accuracy: 0.528\n",
      "Epoch 266, CIFAR-10 Batch 1:  loss: 1.48815 accuracy: 0.5276\n",
      "Epoch 266, CIFAR-10 Batch 2:  loss: 1.44455 accuracy: 0.5274\n",
      "Epoch 266, CIFAR-10 Batch 3:  loss: 1.44674 accuracy: 0.5254\n",
      "Epoch 266, CIFAR-10 Batch 4:  loss: 1.46593 accuracy: 0.5288\n",
      "Epoch 266, CIFAR-10 Batch 5:  loss: 1.44864 accuracy: 0.5282\n",
      "Epoch 267, CIFAR-10 Batch 1:  loss: 1.48913 accuracy: 0.5258\n",
      "Epoch 267, CIFAR-10 Batch 2:  loss: 1.44292 accuracy: 0.528\n",
      "Epoch 267, CIFAR-10 Batch 3:  loss: 1.44696 accuracy: 0.5264\n",
      "Epoch 267, CIFAR-10 Batch 4:  loss: 1.46493 accuracy: 0.5314\n",
      "Epoch 267, CIFAR-10 Batch 5:  loss: 1.44963 accuracy: 0.5284\n",
      "Epoch 268, CIFAR-10 Batch 1:  loss: 1.49112 accuracy: 0.5238\n",
      "Epoch 268, CIFAR-10 Batch 2:  loss: 1.44631 accuracy: 0.5278\n",
      "Epoch 268, CIFAR-10 Batch 3:  loss: 1.45039 accuracy: 0.5268\n",
      "Epoch 268, CIFAR-10 Batch 4:  loss: 1.46934 accuracy: 0.5312\n",
      "Epoch 268, CIFAR-10 Batch 5:  loss: 1.44343 accuracy: 0.527\n",
      "Epoch 269, CIFAR-10 Batch 1:  loss: 1.48488 accuracy: 0.525\n",
      "Epoch 269, CIFAR-10 Batch 2:  loss: 1.44811 accuracy: 0.5282\n",
      "Epoch 269, CIFAR-10 Batch 3:  loss: 1.44904 accuracy: 0.5266\n",
      "Epoch 269, CIFAR-10 Batch 4:  loss: 1.46565 accuracy: 0.5324\n",
      "Epoch 269, CIFAR-10 Batch 5:  loss: 1.44435 accuracy: 0.5296\n",
      "Epoch 270, CIFAR-10 Batch 1:  loss: 1.48821 accuracy: 0.5252\n",
      "Epoch 270, CIFAR-10 Batch 2:  loss: 1.44847 accuracy: 0.526\n",
      "Epoch 270, CIFAR-10 Batch 3:  loss: 1.44748 accuracy: 0.527\n",
      "Epoch 270, CIFAR-10 Batch 4:  loss: 1.4705 accuracy: 0.5324\n",
      "Epoch 270, CIFAR-10 Batch 5:  loss: 1.44845 accuracy: 0.5282\n",
      "Epoch 271, CIFAR-10 Batch 1:  loss: 1.49164 accuracy: 0.5244\n",
      "Epoch 271, CIFAR-10 Batch 2:  loss: 1.44666 accuracy: 0.5286\n",
      "Epoch 271, CIFAR-10 Batch 3:  loss: 1.44684 accuracy: 0.5272\n",
      "Epoch 271, CIFAR-10 Batch 4:  loss: 1.47481 accuracy: 0.5292\n",
      "Epoch 271, CIFAR-10 Batch 5:  loss: 1.44926 accuracy: 0.5298\n",
      "Epoch 272, CIFAR-10 Batch 1:  loss: 1.4944 accuracy: 0.524\n",
      "Epoch 272, CIFAR-10 Batch 2:  loss: 1.44632 accuracy: 0.528\n",
      "Epoch 272, CIFAR-10 Batch 3:  loss: 1.43953 accuracy: 0.528\n",
      "Epoch 272, CIFAR-10 Batch 4:  loss: 1.46418 accuracy: 0.5304\n",
      "Epoch 272, CIFAR-10 Batch 5:  loss: 1.44978 accuracy: 0.527\n",
      "Epoch 273, CIFAR-10 Batch 1:  loss: 1.48585 accuracy: 0.525\n",
      "Epoch 273, CIFAR-10 Batch 2:  loss: 1.45291 accuracy: 0.5272\n",
      "Epoch 273, CIFAR-10 Batch 3:  loss: 1.44283 accuracy: 0.5276\n",
      "Epoch 273, CIFAR-10 Batch 4:  loss: 1.46464 accuracy: 0.53\n",
      "Epoch 273, CIFAR-10 Batch 5:  loss: 1.45053 accuracy: 0.5274\n",
      "Epoch 274, CIFAR-10 Batch 1:  loss: 1.49186 accuracy: 0.5238\n",
      "Epoch 274, CIFAR-10 Batch 2:  loss: 1.45206 accuracy: 0.5278\n",
      "Epoch 274, CIFAR-10 Batch 3:  loss: 1.44808 accuracy: 0.5268\n",
      "Epoch 274, CIFAR-10 Batch 4:  loss: 1.47684 accuracy: 0.5298\n",
      "Epoch 274, CIFAR-10 Batch 5:  loss: 1.44676 accuracy: 0.5298\n",
      "Epoch 275, CIFAR-10 Batch 1:  loss: 1.48553 accuracy: 0.5244\n",
      "Epoch 275, CIFAR-10 Batch 2:  loss: 1.45335 accuracy: 0.5296\n",
      "Epoch 275, CIFAR-10 Batch 3:  loss: 1.44818 accuracy: 0.5278\n",
      "Epoch 275, CIFAR-10 Batch 4:  loss: 1.47719 accuracy: 0.5308\n",
      "Epoch 275, CIFAR-10 Batch 5:  loss: 1.44457 accuracy: 0.5294\n",
      "Epoch 276, CIFAR-10 Batch 1:  loss: 1.48782 accuracy: 0.526\n",
      "Epoch 276, CIFAR-10 Batch 2:  loss: 1.4526 accuracy: 0.529\n",
      "Epoch 276, CIFAR-10 Batch 3:  loss: 1.44956 accuracy: 0.5276\n",
      "Epoch 276, CIFAR-10 Batch 4:  loss: 1.47248 accuracy: 0.5304\n",
      "Epoch 276, CIFAR-10 Batch 5:  loss: 1.4487 accuracy: 0.5276\n",
      "Epoch 277, CIFAR-10 Batch 1:  loss: 1.48382 accuracy: 0.5242\n",
      "Epoch 277, CIFAR-10 Batch 2:  loss: 1.45739 accuracy: 0.5272\n",
      "Epoch 277, CIFAR-10 Batch 3:  loss: 1.44923 accuracy: 0.527\n",
      "Epoch 277, CIFAR-10 Batch 4:  loss: 1.47822 accuracy: 0.53\n",
      "Epoch 277, CIFAR-10 Batch 5:  loss: 1.44238 accuracy: 0.5292\n",
      "Epoch 278, CIFAR-10 Batch 1:  loss: 1.48792 accuracy: 0.524\n",
      "Epoch 278, CIFAR-10 Batch 2:  loss: 1.45611 accuracy: 0.5282\n",
      "Epoch 278, CIFAR-10 Batch 3:  loss: 1.44895 accuracy: 0.5282\n",
      "Epoch 278, CIFAR-10 Batch 4:  loss: 1.48019 accuracy: 0.5326\n",
      "Epoch 278, CIFAR-10 Batch 5:  loss: 1.4454 accuracy: 0.53\n",
      "Epoch 279, CIFAR-10 Batch 1:  loss: 1.48887 accuracy: 0.525\n",
      "Epoch 279, CIFAR-10 Batch 2:  loss: 1.451 accuracy: 0.5274\n",
      "Epoch 279, CIFAR-10 Batch 3:  loss: 1.45214 accuracy: 0.5294\n",
      "Epoch 279, CIFAR-10 Batch 4:  loss: 1.48793 accuracy: 0.5322\n",
      "Epoch 279, CIFAR-10 Batch 5:  loss: 1.44883 accuracy: 0.5284\n",
      "Epoch 280, CIFAR-10 Batch 1:  loss: 1.49336 accuracy: 0.5246\n",
      "Epoch 280, CIFAR-10 Batch 2:  loss: 1.45778 accuracy: 0.529\n",
      "Epoch 280, CIFAR-10 Batch 3:  loss: 1.45231 accuracy: 0.529\n",
      "Epoch 280, CIFAR-10 Batch 4:  loss: 1.48572 accuracy: 0.5298\n",
      "Epoch 280, CIFAR-10 Batch 5:  loss: 1.44098 accuracy: 0.53\n",
      "Epoch 281, CIFAR-10 Batch 1:  loss: 1.48915 accuracy: 0.5242\n",
      "Epoch 281, CIFAR-10 Batch 2:  loss: 1.46198 accuracy: 0.5308\n",
      "Epoch 281, CIFAR-10 Batch 3:  loss: 1.44899 accuracy: 0.5294\n",
      "Epoch 281, CIFAR-10 Batch 4:  loss: 1.48599 accuracy: 0.5322\n",
      "Epoch 281, CIFAR-10 Batch 5:  loss: 1.44845 accuracy: 0.529\n",
      "Epoch 282, CIFAR-10 Batch 1:  loss: 1.49146 accuracy: 0.5254\n",
      "Epoch 282, CIFAR-10 Batch 2:  loss: 1.4575 accuracy: 0.5316\n",
      "Epoch 282, CIFAR-10 Batch 3:  loss: 1.45346 accuracy: 0.5278\n",
      "Epoch 282, CIFAR-10 Batch 4:  loss: 1.48376 accuracy: 0.533\n",
      "Epoch 282, CIFAR-10 Batch 5:  loss: 1.44178 accuracy: 0.5302\n",
      "Epoch 283, CIFAR-10 Batch 1:  loss: 1.48366 accuracy: 0.527\n",
      "Epoch 283, CIFAR-10 Batch 2:  loss: 1.46333 accuracy: 0.5306\n",
      "Epoch 283, CIFAR-10 Batch 3:  loss: 1.4537 accuracy: 0.529\n",
      "Epoch 283, CIFAR-10 Batch 4:  loss: 1.47783 accuracy: 0.5324\n",
      "Epoch 283, CIFAR-10 Batch 5:  loss: 1.44459 accuracy: 0.5304\n",
      "Epoch 284, CIFAR-10 Batch 1:  loss: 1.48872 accuracy: 0.5254\n",
      "Epoch 284, CIFAR-10 Batch 2:  loss: 1.4606 accuracy: 0.53\n",
      "Epoch 284, CIFAR-10 Batch 3:  loss: 1.45631 accuracy: 0.529\n",
      "Epoch 284, CIFAR-10 Batch 4:  loss: 1.47567 accuracy: 0.532\n",
      "Epoch 284, CIFAR-10 Batch 5:  loss: 1.44611 accuracy: 0.5298\n",
      "Epoch 285, CIFAR-10 Batch 1:  loss: 1.49494 accuracy: 0.5256\n",
      "Epoch 285, CIFAR-10 Batch 2:  loss: 1.47181 accuracy: 0.5294\n",
      "Epoch 285, CIFAR-10 Batch 3:  loss: 1.45418 accuracy: 0.5266\n",
      "Epoch 285, CIFAR-10 Batch 4:  loss: 1.48333 accuracy: 0.532\n",
      "Epoch 285, CIFAR-10 Batch 5:  loss: 1.45318 accuracy: 0.5294\n",
      "Epoch 286, CIFAR-10 Batch 1:  loss: 1.48743 accuracy: 0.527\n",
      "Epoch 286, CIFAR-10 Batch 2:  loss: 1.47121 accuracy: 0.5312\n",
      "Epoch 286, CIFAR-10 Batch 3:  loss: 1.46396 accuracy: 0.5286\n",
      "Epoch 286, CIFAR-10 Batch 4:  loss: 1.48204 accuracy: 0.5324\n",
      "Epoch 286, CIFAR-10 Batch 5:  loss: 1.45216 accuracy: 0.5298\n",
      "Epoch 287, CIFAR-10 Batch 1:  loss: 1.48561 accuracy: 0.5268\n",
      "Epoch 287, CIFAR-10 Batch 2:  loss: 1.46723 accuracy: 0.5294\n",
      "Epoch 287, CIFAR-10 Batch 3:  loss: 1.46047 accuracy: 0.528\n",
      "Epoch 287, CIFAR-10 Batch 4:  loss: 1.47746 accuracy: 0.5346\n",
      "Epoch 287, CIFAR-10 Batch 5:  loss: 1.45234 accuracy: 0.5278\n",
      "Epoch 288, CIFAR-10 Batch 1:  loss: 1.48371 accuracy: 0.5274\n",
      "Epoch 288, CIFAR-10 Batch 2:  loss: 1.47603 accuracy: 0.532\n",
      "Epoch 288, CIFAR-10 Batch 3:  loss: 1.45777 accuracy: 0.5276\n",
      "Epoch 288, CIFAR-10 Batch 4:  loss: 1.48028 accuracy: 0.5328\n",
      "Epoch 288, CIFAR-10 Batch 5:  loss: 1.44118 accuracy: 0.5308\n",
      "Epoch 289, CIFAR-10 Batch 1:  loss: 1.48752 accuracy: 0.527\n",
      "Epoch 289, CIFAR-10 Batch 2:  loss: 1.46863 accuracy: 0.5308\n",
      "Epoch 289, CIFAR-10 Batch 3:  loss: 1.45544 accuracy: 0.5284\n",
      "Epoch 289, CIFAR-10 Batch 4:  loss: 1.47549 accuracy: 0.5336\n",
      "Epoch 289, CIFAR-10 Batch 5:  loss: 1.44983 accuracy: 0.5308\n",
      "Epoch 290, CIFAR-10 Batch 1:  loss: 1.49287 accuracy: 0.5272\n",
      "Epoch 290, CIFAR-10 Batch 2:  loss: 1.46458 accuracy: 0.5312\n",
      "Epoch 290, CIFAR-10 Batch 3:  loss: 1.46186 accuracy: 0.5268\n",
      "Epoch 290, CIFAR-10 Batch 4:  loss: 1.47576 accuracy: 0.534\n",
      "Epoch 290, CIFAR-10 Batch 5:  loss: 1.45502 accuracy: 0.5298\n",
      "Epoch 291, CIFAR-10 Batch 1:  loss: 1.49947 accuracy: 0.527\n",
      "Epoch 291, CIFAR-10 Batch 2:  loss: 1.47317 accuracy: 0.5316\n",
      "Epoch 291, CIFAR-10 Batch 3:  loss: 1.45906 accuracy: 0.5286\n",
      "Epoch 291, CIFAR-10 Batch 4:  loss: 1.4785 accuracy: 0.5328\n",
      "Epoch 291, CIFAR-10 Batch 5:  loss: 1.45508 accuracy: 0.5288\n",
      "Epoch 292, CIFAR-10 Batch 1:  loss: 1.50172 accuracy: 0.5272\n",
      "Epoch 292, CIFAR-10 Batch 2:  loss: 1.46769 accuracy: 0.5316\n",
      "Epoch 292, CIFAR-10 Batch 3:  loss: 1.46431 accuracy: 0.5276\n",
      "Epoch 292, CIFAR-10 Batch 4:  loss: 1.47612 accuracy: 0.5338\n",
      "Epoch 292, CIFAR-10 Batch 5:  loss: 1.45321 accuracy: 0.53\n",
      "Epoch 293, CIFAR-10 Batch 1:  loss: 1.4969 accuracy: 0.5278\n",
      "Epoch 293, CIFAR-10 Batch 2:  loss: 1.47445 accuracy: 0.5332\n",
      "Epoch 293, CIFAR-10 Batch 3:  loss: 1.46946 accuracy: 0.528\n",
      "Epoch 293, CIFAR-10 Batch 4:  loss: 1.473 accuracy: 0.534\n",
      "Epoch 293, CIFAR-10 Batch 5:  loss: 1.45982 accuracy: 0.5308\n",
      "Epoch 294, CIFAR-10 Batch 1:  loss: 1.50106 accuracy: 0.5274\n",
      "Epoch 294, CIFAR-10 Batch 2:  loss: 1.47334 accuracy: 0.5318\n",
      "Epoch 294, CIFAR-10 Batch 3:  loss: 1.4643 accuracy: 0.5312\n",
      "Epoch 294, CIFAR-10 Batch 4:  loss: 1.47586 accuracy: 0.5358\n",
      "Epoch 294, CIFAR-10 Batch 5:  loss: 1.45254 accuracy: 0.5312\n",
      "Epoch 295, CIFAR-10 Batch 1:  loss: 1.50262 accuracy: 0.527\n",
      "Epoch 295, CIFAR-10 Batch 2:  loss: 1.47184 accuracy: 0.533\n",
      "Epoch 295, CIFAR-10 Batch 3:  loss: 1.46146 accuracy: 0.5304\n",
      "Epoch 295, CIFAR-10 Batch 4:  loss: 1.47234 accuracy: 0.535\n",
      "Epoch 295, CIFAR-10 Batch 5:  loss: 1.45216 accuracy: 0.5326\n",
      "Epoch 296, CIFAR-10 Batch 1:  loss: 1.50339 accuracy: 0.5276\n",
      "Epoch 296, CIFAR-10 Batch 2:  loss: 1.4728 accuracy: 0.5338\n",
      "Epoch 296, CIFAR-10 Batch 3:  loss: 1.46943 accuracy: 0.5322\n",
      "Epoch 296, CIFAR-10 Batch 4:  loss: 1.4757 accuracy: 0.5354\n",
      "Epoch 296, CIFAR-10 Batch 5:  loss: 1.45498 accuracy: 0.5314\n",
      "Epoch 297, CIFAR-10 Batch 1:  loss: 1.50297 accuracy: 0.5278\n",
      "Epoch 297, CIFAR-10 Batch 2:  loss: 1.47426 accuracy: 0.5344\n",
      "Epoch 297, CIFAR-10 Batch 3:  loss: 1.4675 accuracy: 0.5292\n",
      "Epoch 297, CIFAR-10 Batch 4:  loss: 1.48037 accuracy: 0.5344\n",
      "Epoch 297, CIFAR-10 Batch 5:  loss: 1.45755 accuracy: 0.532\n",
      "Epoch 298, CIFAR-10 Batch 1:  loss: 1.49692 accuracy: 0.5272\n",
      "Epoch 298, CIFAR-10 Batch 2:  loss: 1.47527 accuracy: 0.5328\n",
      "Epoch 298, CIFAR-10 Batch 3:  loss: 1.46191 accuracy: 0.53\n",
      "Epoch 298, CIFAR-10 Batch 4:  loss: 1.47454 accuracy: 0.5358\n",
      "Epoch 298, CIFAR-10 Batch 5:  loss: 1.46403 accuracy: 0.531\n",
      "Epoch 299, CIFAR-10 Batch 1:  loss: 1.5055 accuracy: 0.5282\n",
      "Epoch 299, CIFAR-10 Batch 2:  loss: 1.46604 accuracy: 0.5346\n",
      "Epoch 299, CIFAR-10 Batch 3:  loss: 1.4613 accuracy: 0.5304\n",
      "Epoch 299, CIFAR-10 Batch 4:  loss: 1.46982 accuracy: 0.5364\n",
      "Epoch 299, CIFAR-10 Batch 5:  loss: 1.46359 accuracy: 0.5326\n",
      "Epoch 300, CIFAR-10 Batch 1:  loss: 1.48928 accuracy: 0.5298\n",
      "Epoch 300, CIFAR-10 Batch 2:  loss: 1.46453 accuracy: 0.5358\n",
      "Epoch 300, CIFAR-10 Batch 3:  loss: 1.46398 accuracy: 0.5284\n",
      "Epoch 300, CIFAR-10 Batch 4:  loss: 1.47963 accuracy: 0.5376\n",
      "Epoch 300, CIFAR-10 Batch 5:  loss: 1.45462 accuracy: 0.5334\n",
      "Epoch 301, CIFAR-10 Batch 1:  loss: 1.48424 accuracy: 0.5308\n",
      "Epoch 301, CIFAR-10 Batch 2:  loss: 1.4728 accuracy: 0.5338\n",
      "Epoch 301, CIFAR-10 Batch 3:  loss: 1.4702 accuracy: 0.531\n",
      "Epoch 301, CIFAR-10 Batch 4:  loss: 1.49201 accuracy: 0.5364\n",
      "Epoch 301, CIFAR-10 Batch 5:  loss: 1.46075 accuracy: 0.5318\n",
      "Epoch 302, CIFAR-10 Batch 1:  loss: 1.5051 accuracy: 0.5278\n",
      "Epoch 302, CIFAR-10 Batch 2:  loss: 1.47578 accuracy: 0.5346\n",
      "Epoch 302, CIFAR-10 Batch 3:  loss: 1.46266 accuracy: 0.5296\n",
      "Epoch 302, CIFAR-10 Batch 4:  loss: 1.48347 accuracy: 0.5346\n",
      "Epoch 302, CIFAR-10 Batch 5:  loss: 1.46494 accuracy: 0.5322\n",
      "Epoch 303, CIFAR-10 Batch 1:  loss: 1.50332 accuracy: 0.5292\n",
      "Epoch 303, CIFAR-10 Batch 2:  loss: 1.48701 accuracy: 0.5336\n",
      "Epoch 303, CIFAR-10 Batch 3:  loss: 1.46015 accuracy: 0.5278\n",
      "Epoch 303, CIFAR-10 Batch 4:  loss: 1.48232 accuracy: 0.5378\n",
      "Epoch 303, CIFAR-10 Batch 5:  loss: 1.46524 accuracy: 0.5334\n",
      "Epoch 304, CIFAR-10 Batch 1:  loss: 1.50201 accuracy: 0.5302\n",
      "Epoch 304, CIFAR-10 Batch 2:  loss: 1.47405 accuracy: 0.5362\n",
      "Epoch 304, CIFAR-10 Batch 3:  loss: 1.46062 accuracy: 0.5308\n",
      "Epoch 304, CIFAR-10 Batch 4:  loss: 1.49214 accuracy: 0.5358\n",
      "Epoch 304, CIFAR-10 Batch 5:  loss: 1.46832 accuracy: 0.5308\n",
      "Epoch 305, CIFAR-10 Batch 1:  loss: 1.50309 accuracy: 0.5294\n",
      "Epoch 305, CIFAR-10 Batch 2:  loss: 1.48805 accuracy: 0.5346\n",
      "Epoch 305, CIFAR-10 Batch 3:  loss: 1.46101 accuracy: 0.5288\n",
      "Epoch 305, CIFAR-10 Batch 4:  loss: 1.48655 accuracy: 0.5364\n",
      "Epoch 305, CIFAR-10 Batch 5:  loss: 1.47849 accuracy: 0.531\n",
      "Epoch 306, CIFAR-10 Batch 1:  loss: 1.50813 accuracy: 0.5302\n",
      "Epoch 306, CIFAR-10 Batch 2:  loss: 1.48245 accuracy: 0.535\n",
      "Epoch 306, CIFAR-10 Batch 3:  loss: 1.46412 accuracy: 0.5294\n",
      "Epoch 306, CIFAR-10 Batch 4:  loss: 1.48763 accuracy: 0.536\n",
      "Epoch 306, CIFAR-10 Batch 5:  loss: 1.47844 accuracy: 0.5314\n",
      "Epoch 307, CIFAR-10 Batch 1:  loss: 1.50237 accuracy: 0.5302\n",
      "Epoch 307, CIFAR-10 Batch 2:  loss: 1.48175 accuracy: 0.5356\n",
      "Epoch 307, CIFAR-10 Batch 3:  loss: 1.45412 accuracy: 0.529\n",
      "Epoch 307, CIFAR-10 Batch 4:  loss: 1.48049 accuracy: 0.5358\n",
      "Epoch 307, CIFAR-10 Batch 5:  loss: 1.4721 accuracy: 0.5338\n",
      "Epoch 308, CIFAR-10 Batch 1:  loss: 1.50781 accuracy: 0.5314\n",
      "Epoch 308, CIFAR-10 Batch 2:  loss: 1.48492 accuracy: 0.5364\n",
      "Epoch 308, CIFAR-10 Batch 3:  loss: 1.46516 accuracy: 0.5306\n",
      "Epoch 308, CIFAR-10 Batch 4:  loss: 1.49193 accuracy: 0.5366\n",
      "Epoch 308, CIFAR-10 Batch 5:  loss: 1.48503 accuracy: 0.532\n",
      "Epoch 309, CIFAR-10 Batch 1:  loss: 1.50535 accuracy: 0.5322\n",
      "Epoch 309, CIFAR-10 Batch 2:  loss: 1.49209 accuracy: 0.5374\n",
      "Epoch 309, CIFAR-10 Batch 3:  loss: 1.45865 accuracy: 0.5318\n",
      "Epoch 309, CIFAR-10 Batch 4:  loss: 1.4847 accuracy: 0.5376\n",
      "Epoch 309, CIFAR-10 Batch 5:  loss: 1.48563 accuracy: 0.5302\n",
      "Epoch 310, CIFAR-10 Batch 1:  loss: 1.50196 accuracy: 0.5318\n",
      "Epoch 310, CIFAR-10 Batch 2:  loss: 1.49423 accuracy: 0.5374\n",
      "Epoch 310, CIFAR-10 Batch 3:  loss: 1.45966 accuracy: 0.531\n",
      "Epoch 310, CIFAR-10 Batch 4:  loss: 1.49666 accuracy: 0.5366\n",
      "Epoch 310, CIFAR-10 Batch 5:  loss: 1.48538 accuracy: 0.5316\n",
      "Epoch 311, CIFAR-10 Batch 1:  loss: 1.50269 accuracy: 0.5326\n",
      "Epoch 311, CIFAR-10 Batch 2:  loss: 1.49442 accuracy: 0.535\n",
      "Epoch 311, CIFAR-10 Batch 3:  loss: 1.45171 accuracy: 0.5304\n",
      "Epoch 311, CIFAR-10 Batch 4:  loss: 1.49649 accuracy: 0.5386\n",
      "Epoch 311, CIFAR-10 Batch 5:  loss: 1.4922 accuracy: 0.5316\n",
      "Epoch 312, CIFAR-10 Batch 1:  loss: 1.50338 accuracy: 0.532\n",
      "Epoch 312, CIFAR-10 Batch 2:  loss: 1.47575 accuracy: 0.5374\n",
      "Epoch 312, CIFAR-10 Batch 3:  loss: 1.4494 accuracy: 0.531\n",
      "Epoch 312, CIFAR-10 Batch 4:  loss: 1.49098 accuracy: 0.5366\n",
      "Epoch 312, CIFAR-10 Batch 5:  loss: 1.48324 accuracy: 0.5302\n",
      "Epoch 313, CIFAR-10 Batch 1:  loss: 1.49826 accuracy: 0.5322\n",
      "Epoch 313, CIFAR-10 Batch 2:  loss: 1.49458 accuracy: 0.539\n",
      "Epoch 313, CIFAR-10 Batch 3:  loss: 1.44751 accuracy: 0.5314\n",
      "Epoch 313, CIFAR-10 Batch 4:  loss: 1.48723 accuracy: 0.539\n",
      "Epoch 313, CIFAR-10 Batch 5:  loss: 1.49498 accuracy: 0.5304\n",
      "Epoch 314, CIFAR-10 Batch 1:  loss: 1.50365 accuracy: 0.5338\n",
      "Epoch 314, CIFAR-10 Batch 2:  loss: 1.48515 accuracy: 0.5378\n",
      "Epoch 314, CIFAR-10 Batch 3:  loss: 1.45573 accuracy: 0.5312\n",
      "Epoch 314, CIFAR-10 Batch 4:  loss: 1.48943 accuracy: 0.5382\n",
      "Epoch 314, CIFAR-10 Batch 5:  loss: 1.49566 accuracy: 0.531\n",
      "Epoch 315, CIFAR-10 Batch 1:  loss: 1.50237 accuracy: 0.5336\n",
      "Epoch 315, CIFAR-10 Batch 2:  loss: 1.48623 accuracy: 0.5364\n",
      "Epoch 315, CIFAR-10 Batch 3:  loss: 1.46408 accuracy: 0.5308\n",
      "Epoch 315, CIFAR-10 Batch 4:  loss: 1.49546 accuracy: 0.5368\n",
      "Epoch 315, CIFAR-10 Batch 5:  loss: 1.48057 accuracy: 0.5318\n",
      "Epoch 316, CIFAR-10 Batch 1:  loss: 1.50462 accuracy: 0.533\n",
      "Epoch 316, CIFAR-10 Batch 2:  loss: 1.48939 accuracy: 0.5366\n",
      "Epoch 316, CIFAR-10 Batch 3:  loss: 1.46429 accuracy: 0.5324\n",
      "Epoch 316, CIFAR-10 Batch 4:  loss: 1.49359 accuracy: 0.5382\n",
      "Epoch 316, CIFAR-10 Batch 5:  loss: 1.49905 accuracy: 0.5286\n",
      "Epoch 317, CIFAR-10 Batch 1:  loss: 1.49689 accuracy: 0.5318\n",
      "Epoch 317, CIFAR-10 Batch 2:  loss: 1.49283 accuracy: 0.539\n",
      "Epoch 317, CIFAR-10 Batch 3:  loss: 1.45392 accuracy: 0.5318\n",
      "Epoch 317, CIFAR-10 Batch 4:  loss: 1.48667 accuracy: 0.5372\n",
      "Epoch 317, CIFAR-10 Batch 5:  loss: 1.47296 accuracy: 0.5312\n",
      "Epoch 318, CIFAR-10 Batch 1:  loss: 1.51303 accuracy: 0.5324\n",
      "Epoch 318, CIFAR-10 Batch 2:  loss: 1.48989 accuracy: 0.537\n",
      "Epoch 318, CIFAR-10 Batch 3:  loss: 1.46444 accuracy: 0.5316\n",
      "Epoch 318, CIFAR-10 Batch 4:  loss: 1.50248 accuracy: 0.5356\n",
      "Epoch 318, CIFAR-10 Batch 5:  loss: 1.50016 accuracy: 0.532\n",
      "Epoch 319, CIFAR-10 Batch 1:  loss: 1.50235 accuracy: 0.5332\n",
      "Epoch 319, CIFAR-10 Batch 2:  loss: 1.47238 accuracy: 0.5414\n",
      "Epoch 319, CIFAR-10 Batch 3:  loss: 1.45996 accuracy: 0.5324\n",
      "Epoch 319, CIFAR-10 Batch 4:  loss: 1.49438 accuracy: 0.5376\n",
      "Epoch 319, CIFAR-10 Batch 5:  loss: 1.47421 accuracy: 0.5338\n",
      "Epoch 320, CIFAR-10 Batch 1:  loss: 1.50608 accuracy: 0.5324\n",
      "Epoch 320, CIFAR-10 Batch 2:  loss: 1.47904 accuracy: 0.5378\n",
      "Epoch 320, CIFAR-10 Batch 3:  loss: 1.46259 accuracy: 0.533\n",
      "Epoch 320, CIFAR-10 Batch 4:  loss: 1.50355 accuracy: 0.5374\n",
      "Epoch 320, CIFAR-10 Batch 5:  loss: 1.50087 accuracy: 0.5298\n",
      "Epoch 321, CIFAR-10 Batch 1:  loss: 1.50509 accuracy: 0.5332\n",
      "Epoch 321, CIFAR-10 Batch 2:  loss: 1.49198 accuracy: 0.5386\n",
      "Epoch 321, CIFAR-10 Batch 3:  loss: 1.46155 accuracy: 0.532\n",
      "Epoch 321, CIFAR-10 Batch 4:  loss: 1.50622 accuracy: 0.5388\n",
      "Epoch 321, CIFAR-10 Batch 5:  loss: 1.47364 accuracy: 0.5314\n",
      "Epoch 322, CIFAR-10 Batch 1:  loss: 1.50866 accuracy: 0.531\n",
      "Epoch 322, CIFAR-10 Batch 2:  loss: 1.49257 accuracy: 0.5376\n",
      "Epoch 322, CIFAR-10 Batch 3:  loss: 1.45787 accuracy: 0.5328\n",
      "Epoch 322, CIFAR-10 Batch 4:  loss: 1.50756 accuracy: 0.54\n",
      "Epoch 322, CIFAR-10 Batch 5:  loss: 1.4972 accuracy: 0.5312\n",
      "Epoch 323, CIFAR-10 Batch 1:  loss: 1.50817 accuracy: 0.5318\n",
      "Epoch 323, CIFAR-10 Batch 2:  loss: 1.49114 accuracy: 0.5362\n",
      "Epoch 323, CIFAR-10 Batch 3:  loss: 1.45755 accuracy: 0.5316\n",
      "Epoch 323, CIFAR-10 Batch 4:  loss: 1.50857 accuracy: 0.5384\n",
      "Epoch 323, CIFAR-10 Batch 5:  loss: 1.47273 accuracy: 0.5312\n",
      "Epoch 324, CIFAR-10 Batch 1:  loss: 1.50941 accuracy: 0.5334\n",
      "Epoch 324, CIFAR-10 Batch 2:  loss: 1.49786 accuracy: 0.5364\n",
      "Epoch 324, CIFAR-10 Batch 3:  loss: 1.45621 accuracy: 0.5342\n",
      "Epoch 324, CIFAR-10 Batch 4:  loss: 1.51457 accuracy: 0.539\n",
      "Epoch 324, CIFAR-10 Batch 5:  loss: 1.48917 accuracy: 0.5318\n",
      "Epoch 325, CIFAR-10 Batch 1:  loss: 1.50057 accuracy: 0.5334\n",
      "Epoch 325, CIFAR-10 Batch 2:  loss: 1.50542 accuracy: 0.5372\n",
      "Epoch 325, CIFAR-10 Batch 3:  loss: 1.4475 accuracy: 0.5346\n",
      "Epoch 325, CIFAR-10 Batch 4:  loss: 1.51506 accuracy: 0.5372\n",
      "Epoch 325, CIFAR-10 Batch 5:  loss: 1.48993 accuracy: 0.5312\n",
      "Epoch 326, CIFAR-10 Batch 1:  loss: 1.5079 accuracy: 0.5334\n",
      "Epoch 326, CIFAR-10 Batch 2:  loss: 1.49724 accuracy: 0.5374\n",
      "Epoch 326, CIFAR-10 Batch 3:  loss: 1.46089 accuracy: 0.5342\n",
      "Epoch 326, CIFAR-10 Batch 4:  loss: 1.51818 accuracy: 0.5366\n",
      "Epoch 326, CIFAR-10 Batch 5:  loss: 1.47506 accuracy: 0.5326\n",
      "Epoch 327, CIFAR-10 Batch 1:  loss: 1.51128 accuracy: 0.5334\n",
      "Epoch 327, CIFAR-10 Batch 2:  loss: 1.51602 accuracy: 0.536\n",
      "Epoch 327, CIFAR-10 Batch 3:  loss: 1.44512 accuracy: 0.533\n",
      "Epoch 327, CIFAR-10 Batch 4:  loss: 1.51667 accuracy: 0.538\n",
      "Epoch 327, CIFAR-10 Batch 5:  loss: 1.48291 accuracy: 0.5356\n",
      "Epoch 328, CIFAR-10 Batch 1:  loss: 1.50988 accuracy: 0.534\n",
      "Epoch 328, CIFAR-10 Batch 2:  loss: 1.51344 accuracy: 0.5376\n",
      "Epoch 328, CIFAR-10 Batch 3:  loss: 1.44992 accuracy: 0.5342\n",
      "Epoch 328, CIFAR-10 Batch 4:  loss: 1.51579 accuracy: 0.5372\n",
      "Epoch 328, CIFAR-10 Batch 5:  loss: 1.48112 accuracy: 0.5364\n",
      "Epoch 329, CIFAR-10 Batch 1:  loss: 1.50873 accuracy: 0.5338\n",
      "Epoch 329, CIFAR-10 Batch 2:  loss: 1.51995 accuracy: 0.5374\n",
      "Epoch 329, CIFAR-10 Batch 3:  loss: 1.45803 accuracy: 0.5348\n",
      "Epoch 329, CIFAR-10 Batch 4:  loss: 1.52082 accuracy: 0.5402\n",
      "Epoch 329, CIFAR-10 Batch 5:  loss: 1.48478 accuracy: 0.533\n",
      "Epoch 330, CIFAR-10 Batch 1:  loss: 1.50865 accuracy: 0.533\n",
      "Epoch 330, CIFAR-10 Batch 2:  loss: 1.51481 accuracy: 0.5386\n",
      "Epoch 330, CIFAR-10 Batch 3:  loss: 1.45438 accuracy: 0.5346\n",
      "Epoch 330, CIFAR-10 Batch 4:  loss: 1.50981 accuracy: 0.5386\n",
      "Epoch 330, CIFAR-10 Batch 5:  loss: 1.48799 accuracy: 0.5334\n",
      "Epoch 331, CIFAR-10 Batch 1:  loss: 1.51625 accuracy: 0.5324\n",
      "Epoch 331, CIFAR-10 Batch 2:  loss: 1.52316 accuracy: 0.5362\n",
      "Epoch 331, CIFAR-10 Batch 3:  loss: 1.45362 accuracy: 0.5358\n",
      "Epoch 331, CIFAR-10 Batch 4:  loss: 1.53305 accuracy: 0.537\n",
      "Epoch 331, CIFAR-10 Batch 5:  loss: 1.48475 accuracy: 0.5358\n",
      "Epoch 332, CIFAR-10 Batch 1:  loss: 1.51871 accuracy: 0.5344\n",
      "Epoch 332, CIFAR-10 Batch 2:  loss: 1.51961 accuracy: 0.538\n",
      "Epoch 332, CIFAR-10 Batch 3:  loss: 1.45219 accuracy: 0.5362\n",
      "Epoch 332, CIFAR-10 Batch 4:  loss: 1.5246 accuracy: 0.5384\n",
      "Epoch 332, CIFAR-10 Batch 5:  loss: 1.48673 accuracy: 0.536\n",
      "Epoch 333, CIFAR-10 Batch 1:  loss: 1.51225 accuracy: 0.5336\n",
      "Epoch 333, CIFAR-10 Batch 2:  loss: 1.52584 accuracy: 0.5378\n",
      "Epoch 333, CIFAR-10 Batch 3:  loss: 1.44755 accuracy: 0.5334\n",
      "Epoch 333, CIFAR-10 Batch 4:  loss: 1.51323 accuracy: 0.5394\n",
      "Epoch 333, CIFAR-10 Batch 5:  loss: 1.48619 accuracy: 0.5342\n",
      "Epoch 334, CIFAR-10 Batch 1:  loss: 1.51598 accuracy: 0.534\n",
      "Epoch 334, CIFAR-10 Batch 2:  loss: 1.52308 accuracy: 0.5376\n",
      "Epoch 334, CIFAR-10 Batch 3:  loss: 1.44917 accuracy: 0.5352\n",
      "Epoch 334, CIFAR-10 Batch 4:  loss: 1.51338 accuracy: 0.54\n",
      "Epoch 334, CIFAR-10 Batch 5:  loss: 1.49253 accuracy: 0.5348\n",
      "Epoch 335, CIFAR-10 Batch 1:  loss: 1.52171 accuracy: 0.5326\n",
      "Epoch 335, CIFAR-10 Batch 2:  loss: 1.52933 accuracy: 0.5384\n",
      "Epoch 335, CIFAR-10 Batch 3:  loss: 1.45739 accuracy: 0.5356\n",
      "Epoch 335, CIFAR-10 Batch 4:  loss: 1.52065 accuracy: 0.5392\n",
      "Epoch 335, CIFAR-10 Batch 5:  loss: 1.48446 accuracy: 0.5386\n",
      "Epoch 336, CIFAR-10 Batch 1:  loss: 1.5228 accuracy: 0.5334\n",
      "Epoch 336, CIFAR-10 Batch 2:  loss: 1.53067 accuracy: 0.5392\n",
      "Epoch 336, CIFAR-10 Batch 3:  loss: 1.44519 accuracy: 0.535\n",
      "Epoch 336, CIFAR-10 Batch 4:  loss: 1.51408 accuracy: 0.5392\n",
      "Epoch 336, CIFAR-10 Batch 5:  loss: 1.49327 accuracy: 0.5366\n",
      "Epoch 337, CIFAR-10 Batch 1:  loss: 1.51309 accuracy: 0.532\n",
      "Epoch 337, CIFAR-10 Batch 2:  loss: 1.54021 accuracy: 0.538\n",
      "Epoch 337, CIFAR-10 Batch 3:  loss: 1.45723 accuracy: 0.5366\n",
      "Epoch 337, CIFAR-10 Batch 4:  loss: 1.51894 accuracy: 0.537\n",
      "Epoch 337, CIFAR-10 Batch 5:  loss: 1.49138 accuracy: 0.5364\n",
      "Epoch 338, CIFAR-10 Batch 1:  loss: 1.52704 accuracy: 0.5326\n",
      "Epoch 338, CIFAR-10 Batch 2:  loss: 1.52786 accuracy: 0.5368\n",
      "Epoch 338, CIFAR-10 Batch 3:  loss: 1.4624 accuracy: 0.5364\n",
      "Epoch 338, CIFAR-10 Batch 4:  loss: 1.51109 accuracy: 0.54\n",
      "Epoch 338, CIFAR-10 Batch 5:  loss: 1.48222 accuracy: 0.5372\n",
      "Epoch 339, CIFAR-10 Batch 1:  loss: 1.52138 accuracy: 0.5352\n",
      "Epoch 339, CIFAR-10 Batch 2:  loss: 1.531 accuracy: 0.537\n",
      "Epoch 339, CIFAR-10 Batch 3:  loss: 1.44711 accuracy: 0.5362\n",
      "Epoch 339, CIFAR-10 Batch 4:  loss: 1.51688 accuracy: 0.5398\n",
      "Epoch 339, CIFAR-10 Batch 5:  loss: 1.4889 accuracy: 0.5368\n",
      "Epoch 340, CIFAR-10 Batch 1:  loss: 1.53879 accuracy: 0.5342\n",
      "Epoch 340, CIFAR-10 Batch 2:  loss: 1.53136 accuracy: 0.5368\n",
      "Epoch 340, CIFAR-10 Batch 3:  loss: 1.45949 accuracy: 0.5388\n",
      "Epoch 340, CIFAR-10 Batch 4:  loss: 1.51634 accuracy: 0.541\n",
      "Epoch 340, CIFAR-10 Batch 5:  loss: 1.49238 accuracy: 0.5382\n",
      "Epoch 341, CIFAR-10 Batch 1:  loss: 1.53455 accuracy: 0.5344\n",
      "Epoch 341, CIFAR-10 Batch 2:  loss: 1.53637 accuracy: 0.5376\n",
      "Epoch 341, CIFAR-10 Batch 3:  loss: 1.44596 accuracy: 0.5368\n",
      "Epoch 341, CIFAR-10 Batch 4:  loss: 1.5078 accuracy: 0.5406\n",
      "Epoch 341, CIFAR-10 Batch 5:  loss: 1.48183 accuracy: 0.5394\n",
      "Epoch 342, CIFAR-10 Batch 1:  loss: 1.53254 accuracy: 0.5332\n",
      "Epoch 342, CIFAR-10 Batch 2:  loss: 1.52509 accuracy: 0.5384\n",
      "Epoch 342, CIFAR-10 Batch 3:  loss: 1.45893 accuracy: 0.5374\n",
      "Epoch 342, CIFAR-10 Batch 4:  loss: 1.49837 accuracy: 0.5416\n",
      "Epoch 342, CIFAR-10 Batch 5:  loss: 1.48402 accuracy: 0.5406\n",
      "Epoch 343, CIFAR-10 Batch 1:  loss: 1.53388 accuracy: 0.5362\n",
      "Epoch 343, CIFAR-10 Batch 2:  loss: 1.53719 accuracy: 0.5388\n",
      "Epoch 343, CIFAR-10 Batch 3:  loss: 1.4545 accuracy: 0.5362\n",
      "Epoch 343, CIFAR-10 Batch 4:  loss: 1.50207 accuracy: 0.54\n",
      "Epoch 343, CIFAR-10 Batch 5:  loss: 1.50165 accuracy: 0.5352\n",
      "Epoch 344, CIFAR-10 Batch 1:  loss: 1.5331 accuracy: 0.5338\n",
      "Epoch 344, CIFAR-10 Batch 2:  loss: 1.53724 accuracy: 0.5388\n",
      "Epoch 344, CIFAR-10 Batch 3:  loss: 1.45434 accuracy: 0.538\n",
      "Epoch 344, CIFAR-10 Batch 4:  loss: 1.51929 accuracy: 0.541\n",
      "Epoch 344, CIFAR-10 Batch 5:  loss: 1.50002 accuracy: 0.5356\n",
      "Epoch 345, CIFAR-10 Batch 1:  loss: 1.54172 accuracy: 0.5344\n",
      "Epoch 345, CIFAR-10 Batch 2:  loss: 1.5463 accuracy: 0.5384\n",
      "Epoch 345, CIFAR-10 Batch 3:  loss: 1.45311 accuracy: 0.5378\n",
      "Epoch 345, CIFAR-10 Batch 4:  loss: 1.51816 accuracy: 0.5406\n",
      "Epoch 345, CIFAR-10 Batch 5:  loss: 1.49498 accuracy: 0.537\n",
      "Epoch 346, CIFAR-10 Batch 1:  loss: 1.54197 accuracy: 0.535\n",
      "Epoch 346, CIFAR-10 Batch 2:  loss: 1.53348 accuracy: 0.5388\n",
      "Epoch 346, CIFAR-10 Batch 3:  loss: 1.44863 accuracy: 0.537\n",
      "Epoch 346, CIFAR-10 Batch 4:  loss: 1.5204 accuracy: 0.5418\n",
      "Epoch 346, CIFAR-10 Batch 5:  loss: 1.48531 accuracy: 0.5368\n",
      "Epoch 347, CIFAR-10 Batch 1:  loss: 1.55345 accuracy: 0.5376\n",
      "Epoch 347, CIFAR-10 Batch 2:  loss: 1.54506 accuracy: 0.54\n",
      "Epoch 347, CIFAR-10 Batch 3:  loss: 1.45413 accuracy: 0.5362\n",
      "Epoch 347, CIFAR-10 Batch 4:  loss: 1.52589 accuracy: 0.5404\n",
      "Epoch 347, CIFAR-10 Batch 5:  loss: 1.4938 accuracy: 0.5364\n",
      "Epoch 348, CIFAR-10 Batch 1:  loss: 1.54905 accuracy: 0.5344\n",
      "Epoch 348, CIFAR-10 Batch 2:  loss: 1.53393 accuracy: 0.5386\n",
      "Epoch 348, CIFAR-10 Batch 3:  loss: 1.44018 accuracy: 0.5378\n",
      "Epoch 348, CIFAR-10 Batch 4:  loss: 1.51164 accuracy: 0.5388\n",
      "Epoch 348, CIFAR-10 Batch 5:  loss: 1.50501 accuracy: 0.5354\n",
      "Epoch 349, CIFAR-10 Batch 1:  loss: 1.54723 accuracy: 0.5358\n",
      "Epoch 349, CIFAR-10 Batch 2:  loss: 1.54728 accuracy: 0.5386\n",
      "Epoch 349, CIFAR-10 Batch 3:  loss: 1.44679 accuracy: 0.5364\n",
      "Epoch 349, CIFAR-10 Batch 4:  loss: 1.51388 accuracy: 0.5396\n",
      "Epoch 349, CIFAR-10 Batch 5:  loss: 1.50069 accuracy: 0.5378\n",
      "Epoch 350, CIFAR-10 Batch 1:  loss: 1.5473 accuracy: 0.5342\n",
      "Epoch 350, CIFAR-10 Batch 2:  loss: 1.55183 accuracy: 0.5412\n",
      "Epoch 350, CIFAR-10 Batch 3:  loss: 1.45047 accuracy: 0.537\n",
      "Epoch 350, CIFAR-10 Batch 4:  loss: 1.50076 accuracy: 0.5404\n",
      "Epoch 350, CIFAR-10 Batch 5:  loss: 1.50384 accuracy: 0.5348\n",
      "Epoch 351, CIFAR-10 Batch 1:  loss: 1.55243 accuracy: 0.5346\n",
      "Epoch 351, CIFAR-10 Batch 2:  loss: 1.53538 accuracy: 0.539\n",
      "Epoch 351, CIFAR-10 Batch 3:  loss: 1.45077 accuracy: 0.5372\n",
      "Epoch 351, CIFAR-10 Batch 4:  loss: 1.51941 accuracy: 0.5398\n",
      "Epoch 351, CIFAR-10 Batch 5:  loss: 1.50982 accuracy: 0.5358\n",
      "Epoch 352, CIFAR-10 Batch 1:  loss: 1.55331 accuracy: 0.535\n",
      "Epoch 352, CIFAR-10 Batch 2:  loss: 1.54812 accuracy: 0.5406\n",
      "Epoch 352, CIFAR-10 Batch 3:  loss: 1.4572 accuracy: 0.5356\n",
      "Epoch 352, CIFAR-10 Batch 4:  loss: 1.50886 accuracy: 0.5402\n",
      "Epoch 352, CIFAR-10 Batch 5:  loss: 1.50577 accuracy: 0.5356\n",
      "Epoch 353, CIFAR-10 Batch 1:  loss: 1.54275 accuracy: 0.5358\n",
      "Epoch 353, CIFAR-10 Batch 2:  loss: 1.55919 accuracy: 0.5402\n",
      "Epoch 353, CIFAR-10 Batch 3:  loss: 1.46444 accuracy: 0.5366\n",
      "Epoch 353, CIFAR-10 Batch 4:  loss: 1.52064 accuracy: 0.5414\n",
      "Epoch 353, CIFAR-10 Batch 5:  loss: 1.50575 accuracy: 0.535\n",
      "Epoch 354, CIFAR-10 Batch 1:  loss: 1.54086 accuracy: 0.5356\n",
      "Epoch 354, CIFAR-10 Batch 2:  loss: 1.55191 accuracy: 0.5412\n",
      "Epoch 354, CIFAR-10 Batch 3:  loss: 1.4643 accuracy: 0.5394\n",
      "Epoch 354, CIFAR-10 Batch 4:  loss: 1.48832 accuracy: 0.5422\n",
      "Epoch 354, CIFAR-10 Batch 5:  loss: 1.52122 accuracy: 0.5374\n",
      "Epoch 355, CIFAR-10 Batch 1:  loss: 1.53833 accuracy: 0.5344\n",
      "Epoch 355, CIFAR-10 Batch 2:  loss: 1.54792 accuracy: 0.54\n",
      "Epoch 355, CIFAR-10 Batch 3:  loss: 1.46343 accuracy: 0.5392\n",
      "Epoch 355, CIFAR-10 Batch 4:  loss: 1.4937 accuracy: 0.543\n",
      "Epoch 355, CIFAR-10 Batch 5:  loss: 1.51597 accuracy: 0.5354\n",
      "Epoch 356, CIFAR-10 Batch 1:  loss: 1.54661 accuracy: 0.5368\n",
      "Epoch 356, CIFAR-10 Batch 2:  loss: 1.54113 accuracy: 0.5412\n",
      "Epoch 356, CIFAR-10 Batch 3:  loss: 1.47122 accuracy: 0.5376\n",
      "Epoch 356, CIFAR-10 Batch 4:  loss: 1.50607 accuracy: 0.5408\n",
      "Epoch 356, CIFAR-10 Batch 5:  loss: 1.5134 accuracy: 0.536\n",
      "Epoch 357, CIFAR-10 Batch 1:  loss: 1.53736 accuracy: 0.5344\n",
      "Epoch 357, CIFAR-10 Batch 2:  loss: 1.54732 accuracy: 0.5416\n",
      "Epoch 357, CIFAR-10 Batch 3:  loss: 1.47433 accuracy: 0.5372\n",
      "Epoch 357, CIFAR-10 Batch 4:  loss: 1.51301 accuracy: 0.5404\n",
      "Epoch 357, CIFAR-10 Batch 5:  loss: 1.52288 accuracy: 0.5392\n",
      "Epoch 358, CIFAR-10 Batch 1:  loss: 1.53784 accuracy: 0.5334\n",
      "Epoch 358, CIFAR-10 Batch 2:  loss: 1.54576 accuracy: 0.5422\n",
      "Epoch 358, CIFAR-10 Batch 3:  loss: 1.47273 accuracy: 0.5388\n",
      "Epoch 358, CIFAR-10 Batch 4:  loss: 1.49936 accuracy: 0.5412\n",
      "Epoch 358, CIFAR-10 Batch 5:  loss: 1.52634 accuracy: 0.5372\n",
      "Epoch 359, CIFAR-10 Batch 1:  loss: 1.53776 accuracy: 0.5342\n",
      "Epoch 359, CIFAR-10 Batch 2:  loss: 1.55858 accuracy: 0.5424\n",
      "Epoch 359, CIFAR-10 Batch 3:  loss: 1.47942 accuracy: 0.5374\n",
      "Epoch 359, CIFAR-10 Batch 4:  loss: 1.50339 accuracy: 0.5406\n",
      "Epoch 359, CIFAR-10 Batch 5:  loss: 1.52533 accuracy: 0.5364\n",
      "Epoch 360, CIFAR-10 Batch 1:  loss: 1.53451 accuracy: 0.5346\n",
      "Epoch 360, CIFAR-10 Batch 2:  loss: 1.56387 accuracy: 0.5412\n",
      "Epoch 360, CIFAR-10 Batch 3:  loss: 1.47084 accuracy: 0.54\n",
      "Epoch 360, CIFAR-10 Batch 4:  loss: 1.50191 accuracy: 0.5406\n",
      "Epoch 360, CIFAR-10 Batch 5:  loss: 1.53016 accuracy: 0.5368\n",
      "Epoch 361, CIFAR-10 Batch 1:  loss: 1.55191 accuracy: 0.5346\n",
      "Epoch 361, CIFAR-10 Batch 2:  loss: 1.55639 accuracy: 0.5412\n",
      "Epoch 361, CIFAR-10 Batch 3:  loss: 1.48418 accuracy: 0.5386\n",
      "Epoch 361, CIFAR-10 Batch 4:  loss: 1.49329 accuracy: 0.54\n",
      "Epoch 361, CIFAR-10 Batch 5:  loss: 1.53776 accuracy: 0.5356\n",
      "Epoch 362, CIFAR-10 Batch 1:  loss: 1.55739 accuracy: 0.5348\n",
      "Epoch 362, CIFAR-10 Batch 2:  loss: 1.58621 accuracy: 0.5426\n",
      "Epoch 362, CIFAR-10 Batch 3:  loss: 1.4708 accuracy: 0.5398\n",
      "Epoch 362, CIFAR-10 Batch 4:  loss: 1.48754 accuracy: 0.54\n",
      "Epoch 362, CIFAR-10 Batch 5:  loss: 1.52479 accuracy: 0.5336\n",
      "Epoch 363, CIFAR-10 Batch 1:  loss: 1.54508 accuracy: 0.5348\n",
      "Epoch 363, CIFAR-10 Batch 2:  loss: 1.5596 accuracy: 0.5416\n",
      "Epoch 363, CIFAR-10 Batch 3:  loss: 1.48404 accuracy: 0.5368\n",
      "Epoch 363, CIFAR-10 Batch 4:  loss: 1.48769 accuracy: 0.5416\n",
      "Epoch 363, CIFAR-10 Batch 5:  loss: 1.528 accuracy: 0.5358\n",
      "Epoch 364, CIFAR-10 Batch 1:  loss: 1.55226 accuracy: 0.5364\n",
      "Epoch 364, CIFAR-10 Batch 2:  loss: 1.56332 accuracy: 0.542\n",
      "Epoch 364, CIFAR-10 Batch 3:  loss: 1.48168 accuracy: 0.5362\n",
      "Epoch 364, CIFAR-10 Batch 4:  loss: 1.48747 accuracy: 0.5408\n",
      "Epoch 364, CIFAR-10 Batch 5:  loss: 1.52233 accuracy: 0.535\n",
      "Epoch 365, CIFAR-10 Batch 1:  loss: 1.56148 accuracy: 0.5356\n",
      "Epoch 365, CIFAR-10 Batch 2:  loss: 1.55454 accuracy: 0.5414\n",
      "Epoch 365, CIFAR-10 Batch 3:  loss: 1.47208 accuracy: 0.5386\n",
      "Epoch 365, CIFAR-10 Batch 4:  loss: 1.49287 accuracy: 0.5408\n",
      "Epoch 365, CIFAR-10 Batch 5:  loss: 1.52578 accuracy: 0.536\n",
      "Epoch 366, CIFAR-10 Batch 1:  loss: 1.56154 accuracy: 0.5354\n",
      "Epoch 366, CIFAR-10 Batch 2:  loss: 1.55771 accuracy: 0.5426\n",
      "Epoch 366, CIFAR-10 Batch 3:  loss: 1.48634 accuracy: 0.5388\n",
      "Epoch 366, CIFAR-10 Batch 4:  loss: 1.48889 accuracy: 0.542\n",
      "Epoch 366, CIFAR-10 Batch 5:  loss: 1.52594 accuracy: 0.537\n",
      "Epoch 367, CIFAR-10 Batch 1:  loss: 1.56042 accuracy: 0.5368\n",
      "Epoch 367, CIFAR-10 Batch 2:  loss: 1.54975 accuracy: 0.5422\n",
      "Epoch 367, CIFAR-10 Batch 3:  loss: 1.47997 accuracy: 0.5416\n",
      "Epoch 367, CIFAR-10 Batch 4:  loss: 1.48858 accuracy: 0.541\n",
      "Epoch 367, CIFAR-10 Batch 5:  loss: 1.5193 accuracy: 0.5344\n",
      "Epoch 368, CIFAR-10 Batch 1:  loss: 1.5599 accuracy: 0.5364\n",
      "Epoch 368, CIFAR-10 Batch 2:  loss: 1.55018 accuracy: 0.5412\n",
      "Epoch 368, CIFAR-10 Batch 3:  loss: 1.47708 accuracy: 0.5414\n",
      "Epoch 368, CIFAR-10 Batch 4:  loss: 1.49982 accuracy: 0.539\n",
      "Epoch 368, CIFAR-10 Batch 5:  loss: 1.51915 accuracy: 0.5366\n",
      "Epoch 369, CIFAR-10 Batch 1:  loss: 1.58047 accuracy: 0.5366\n",
      "Epoch 369, CIFAR-10 Batch 2:  loss: 1.55402 accuracy: 0.543\n",
      "Epoch 369, CIFAR-10 Batch 3:  loss: 1.47639 accuracy: 0.5404\n",
      "Epoch 369, CIFAR-10 Batch 4:  loss: 1.49271 accuracy: 0.5406\n",
      "Epoch 369, CIFAR-10 Batch 5:  loss: 1.53219 accuracy: 0.5382\n",
      "Epoch 370, CIFAR-10 Batch 1:  loss: 1.56581 accuracy: 0.5364\n",
      "Epoch 370, CIFAR-10 Batch 2:  loss: 1.56617 accuracy: 0.5458\n",
      "Epoch 370, CIFAR-10 Batch 3:  loss: 1.48693 accuracy: 0.5398\n",
      "Epoch 370, CIFAR-10 Batch 4:  loss: 1.49061 accuracy: 0.54\n",
      "Epoch 370, CIFAR-10 Batch 5:  loss: 1.52631 accuracy: 0.5374\n",
      "Epoch 371, CIFAR-10 Batch 1:  loss: 1.57133 accuracy: 0.5356\n",
      "Epoch 371, CIFAR-10 Batch 2:  loss: 1.56595 accuracy: 0.5434\n",
      "Epoch 371, CIFAR-10 Batch 3:  loss: 1.48574 accuracy: 0.539\n",
      "Epoch 371, CIFAR-10 Batch 4:  loss: 1.49445 accuracy: 0.5412\n",
      "Epoch 371, CIFAR-10 Batch 5:  loss: 1.52072 accuracy: 0.5374\n",
      "Epoch 372, CIFAR-10 Batch 1:  loss: 1.56972 accuracy: 0.5348\n",
      "Epoch 372, CIFAR-10 Batch 2:  loss: 1.55297 accuracy: 0.545\n",
      "Epoch 372, CIFAR-10 Batch 3:  loss: 1.4926 accuracy: 0.5412\n",
      "Epoch 372, CIFAR-10 Batch 4:  loss: 1.49622 accuracy: 0.5404\n",
      "Epoch 372, CIFAR-10 Batch 5:  loss: 1.52058 accuracy: 0.5372\n",
      "Epoch 373, CIFAR-10 Batch 1:  loss: 1.56582 accuracy: 0.535\n",
      "Epoch 373, CIFAR-10 Batch 2:  loss: 1.56603 accuracy: 0.5436\n",
      "Epoch 373, CIFAR-10 Batch 3:  loss: 1.48641 accuracy: 0.538\n",
      "Epoch 373, CIFAR-10 Batch 4:  loss: 1.49778 accuracy: 0.5406\n",
      "Epoch 373, CIFAR-10 Batch 5:  loss: 1.52704 accuracy: 0.5388\n",
      "Epoch 374, CIFAR-10 Batch 1:  loss: 1.57542 accuracy: 0.535\n",
      "Epoch 374, CIFAR-10 Batch 2:  loss: 1.5666 accuracy: 0.5456\n",
      "Epoch 374, CIFAR-10 Batch 3:  loss: 1.49272 accuracy: 0.5418\n",
      "Epoch 374, CIFAR-10 Batch 4:  loss: 1.50183 accuracy: 0.5402\n",
      "Epoch 374, CIFAR-10 Batch 5:  loss: 1.52435 accuracy: 0.5382\n",
      "Epoch 375, CIFAR-10 Batch 1:  loss: 1.58251 accuracy: 0.5338\n",
      "Epoch 375, CIFAR-10 Batch 2:  loss: 1.57915 accuracy: 0.5442\n",
      "Epoch 375, CIFAR-10 Batch 3:  loss: 1.50022 accuracy: 0.5398\n",
      "Epoch 375, CIFAR-10 Batch 4:  loss: 1.5026 accuracy: 0.5402\n",
      "Epoch 375, CIFAR-10 Batch 5:  loss: 1.52894 accuracy: 0.5366\n",
      "Epoch 376, CIFAR-10 Batch 1:  loss: 1.57666 accuracy: 0.5334\n",
      "Epoch 376, CIFAR-10 Batch 2:  loss: 1.55962 accuracy: 0.5446\n",
      "Epoch 376, CIFAR-10 Batch 3:  loss: 1.4947 accuracy: 0.5416\n",
      "Epoch 376, CIFAR-10 Batch 4:  loss: 1.50723 accuracy: 0.5404\n",
      "Epoch 376, CIFAR-10 Batch 5:  loss: 1.52731 accuracy: 0.5372\n",
      "Epoch 377, CIFAR-10 Batch 1:  loss: 1.58157 accuracy: 0.5348\n",
      "Epoch 377, CIFAR-10 Batch 2:  loss: 1.58546 accuracy: 0.5422\n",
      "Epoch 377, CIFAR-10 Batch 3:  loss: 1.4908 accuracy: 0.5456\n",
      "Epoch 377, CIFAR-10 Batch 4:  loss: 1.49973 accuracy: 0.5416\n",
      "Epoch 377, CIFAR-10 Batch 5:  loss: 1.52888 accuracy: 0.5388\n",
      "Epoch 378, CIFAR-10 Batch 1:  loss: 1.58607 accuracy: 0.5336\n",
      "Epoch 378, CIFAR-10 Batch 2:  loss: 1.55804 accuracy: 0.543\n",
      "Epoch 378, CIFAR-10 Batch 3:  loss: 1.48426 accuracy: 0.5436\n",
      "Epoch 378, CIFAR-10 Batch 4:  loss: 1.50166 accuracy: 0.5426\n",
      "Epoch 378, CIFAR-10 Batch 5:  loss: 1.52918 accuracy: 0.5392\n",
      "Epoch 379, CIFAR-10 Batch 1:  loss: 1.57864 accuracy: 0.5348\n",
      "Epoch 379, CIFAR-10 Batch 2:  loss: 1.56684 accuracy: 0.5428\n",
      "Epoch 379, CIFAR-10 Batch 3:  loss: 1.4889 accuracy: 0.5414\n",
      "Epoch 379, CIFAR-10 Batch 4:  loss: 1.51378 accuracy: 0.5394\n",
      "Epoch 379, CIFAR-10 Batch 5:  loss: 1.53257 accuracy: 0.5374\n",
      "Epoch 380, CIFAR-10 Batch 1:  loss: 1.58805 accuracy: 0.535\n",
      "Epoch 380, CIFAR-10 Batch 2:  loss: 1.57668 accuracy: 0.5432\n",
      "Epoch 380, CIFAR-10 Batch 3:  loss: 1.50973 accuracy: 0.5424\n",
      "Epoch 380, CIFAR-10 Batch 4:  loss: 1.49605 accuracy: 0.5426\n",
      "Epoch 380, CIFAR-10 Batch 5:  loss: 1.53349 accuracy: 0.539\n",
      "Epoch 381, CIFAR-10 Batch 1:  loss: 1.57096 accuracy: 0.5356\n",
      "Epoch 381, CIFAR-10 Batch 2:  loss: 1.56747 accuracy: 0.543\n",
      "Epoch 381, CIFAR-10 Batch 3:  loss: 1.5123 accuracy: 0.5432\n",
      "Epoch 381, CIFAR-10 Batch 4:  loss: 1.50171 accuracy: 0.5414\n",
      "Epoch 381, CIFAR-10 Batch 5:  loss: 1.52943 accuracy: 0.5386\n",
      "Epoch 382, CIFAR-10 Batch 1:  loss: 1.59064 accuracy: 0.5366\n",
      "Epoch 382, CIFAR-10 Batch 2:  loss: 1.58018 accuracy: 0.5432\n",
      "Epoch 382, CIFAR-10 Batch 3:  loss: 1.50474 accuracy: 0.5424\n",
      "Epoch 382, CIFAR-10 Batch 4:  loss: 1.49819 accuracy: 0.5418\n",
      "Epoch 382, CIFAR-10 Batch 5:  loss: 1.52937 accuracy: 0.5404\n",
      "Epoch 383, CIFAR-10 Batch 1:  loss: 1.58014 accuracy: 0.5342\n",
      "Epoch 383, CIFAR-10 Batch 2:  loss: 1.56909 accuracy: 0.5452\n",
      "Epoch 383, CIFAR-10 Batch 3:  loss: 1.51194 accuracy: 0.5434\n",
      "Epoch 383, CIFAR-10 Batch 4:  loss: 1.51072 accuracy: 0.5436\n",
      "Epoch 383, CIFAR-10 Batch 5:  loss: 1.53325 accuracy: 0.5398\n",
      "Epoch 384, CIFAR-10 Batch 1:  loss: 1.58165 accuracy: 0.5354\n",
      "Epoch 384, CIFAR-10 Batch 2:  loss: 1.58052 accuracy: 0.544\n",
      "Epoch 384, CIFAR-10 Batch 3:  loss: 1.51973 accuracy: 0.5406\n",
      "Epoch 384, CIFAR-10 Batch 4:  loss: 1.50717 accuracy: 0.5416\n",
      "Epoch 384, CIFAR-10 Batch 5:  loss: 1.54221 accuracy: 0.5388\n",
      "Epoch 385, CIFAR-10 Batch 1:  loss: 1.58357 accuracy: 0.535\n",
      "Epoch 385, CIFAR-10 Batch 2:  loss: 1.58565 accuracy: 0.544\n",
      "Epoch 385, CIFAR-10 Batch 3:  loss: 1.51408 accuracy: 0.5424\n",
      "Epoch 385, CIFAR-10 Batch 4:  loss: 1.50436 accuracy: 0.5426\n",
      "Epoch 385, CIFAR-10 Batch 5:  loss: 1.5532 accuracy: 0.5368\n",
      "Epoch 386, CIFAR-10 Batch 1:  loss: 1.57964 accuracy: 0.5342\n",
      "Epoch 386, CIFAR-10 Batch 2:  loss: 1.58487 accuracy: 0.5436\n",
      "Epoch 386, CIFAR-10 Batch 3:  loss: 1.51041 accuracy: 0.543\n",
      "Epoch 386, CIFAR-10 Batch 4:  loss: 1.51057 accuracy: 0.5428\n",
      "Epoch 386, CIFAR-10 Batch 5:  loss: 1.54524 accuracy: 0.5378\n",
      "Epoch 387, CIFAR-10 Batch 1:  loss: 1.55903 accuracy: 0.5366\n",
      "Epoch 387, CIFAR-10 Batch 2:  loss: 1.59511 accuracy: 0.5426\n",
      "Epoch 387, CIFAR-10 Batch 3:  loss: 1.50827 accuracy: 0.5462\n",
      "Epoch 387, CIFAR-10 Batch 4:  loss: 1.50471 accuracy: 0.5416\n",
      "Epoch 387, CIFAR-10 Batch 5:  loss: 1.54275 accuracy: 0.5362\n",
      "Epoch 388, CIFAR-10 Batch 1:  loss: 1.56372 accuracy: 0.5366\n",
      "Epoch 388, CIFAR-10 Batch 2:  loss: 1.59405 accuracy: 0.543\n",
      "Epoch 388, CIFAR-10 Batch 3:  loss: 1.51058 accuracy: 0.547\n",
      "Epoch 388, CIFAR-10 Batch 4:  loss: 1.50953 accuracy: 0.5422\n",
      "Epoch 388, CIFAR-10 Batch 5:  loss: 1.54855 accuracy: 0.5376\n",
      "Epoch 389, CIFAR-10 Batch 1:  loss: 1.5621 accuracy: 0.5358\n",
      "Epoch 389, CIFAR-10 Batch 2:  loss: 1.61152 accuracy: 0.5444\n",
      "Epoch 389, CIFAR-10 Batch 3:  loss: 1.52701 accuracy: 0.5454\n",
      "Epoch 389, CIFAR-10 Batch 4:  loss: 1.50176 accuracy: 0.544\n",
      "Epoch 389, CIFAR-10 Batch 5:  loss: 1.54043 accuracy: 0.5378\n",
      "Epoch 390, CIFAR-10 Batch 1:  loss: 1.567 accuracy: 0.5344\n",
      "Epoch 390, CIFAR-10 Batch 2:  loss: 1.62844 accuracy: 0.5452\n",
      "Epoch 390, CIFAR-10 Batch 3:  loss: 1.53044 accuracy: 0.5462\n",
      "Epoch 390, CIFAR-10 Batch 4:  loss: 1.50341 accuracy: 0.5432\n",
      "Epoch 390, CIFAR-10 Batch 5:  loss: 1.54722 accuracy: 0.5378\n",
      "Epoch 391, CIFAR-10 Batch 1:  loss: 1.57445 accuracy: 0.5368\n",
      "Epoch 391, CIFAR-10 Batch 2:  loss: 1.61516 accuracy: 0.546\n",
      "Epoch 391, CIFAR-10 Batch 3:  loss: 1.51433 accuracy: 0.5452\n",
      "Epoch 391, CIFAR-10 Batch 4:  loss: 1.51318 accuracy: 0.5444\n",
      "Epoch 391, CIFAR-10 Batch 5:  loss: 1.56258 accuracy: 0.534\n",
      "Epoch 392, CIFAR-10 Batch 1:  loss: 1.55303 accuracy: 0.536\n",
      "Epoch 392, CIFAR-10 Batch 2:  loss: 1.5985 accuracy: 0.5432\n",
      "Epoch 392, CIFAR-10 Batch 3:  loss: 1.52839 accuracy: 0.545\n",
      "Epoch 392, CIFAR-10 Batch 4:  loss: 1.50783 accuracy: 0.5442\n",
      "Epoch 392, CIFAR-10 Batch 5:  loss: 1.56031 accuracy: 0.5354\n",
      "Epoch 393, CIFAR-10 Batch 1:  loss: 1.56304 accuracy: 0.5356\n",
      "Epoch 393, CIFAR-10 Batch 2:  loss: 1.60826 accuracy: 0.5434\n",
      "Epoch 393, CIFAR-10 Batch 3:  loss: 1.53289 accuracy: 0.5446\n",
      "Epoch 393, CIFAR-10 Batch 4:  loss: 1.50432 accuracy: 0.5442\n",
      "Epoch 393, CIFAR-10 Batch 5:  loss: 1.5424 accuracy: 0.5372\n",
      "Epoch 394, CIFAR-10 Batch 1:  loss: 1.54485 accuracy: 0.5348\n",
      "Epoch 394, CIFAR-10 Batch 2:  loss: 1.59808 accuracy: 0.544\n",
      "Epoch 394, CIFAR-10 Batch 3:  loss: 1.54711 accuracy: 0.5448\n",
      "Epoch 394, CIFAR-10 Batch 4:  loss: 1.49426 accuracy: 0.5454\n",
      "Epoch 394, CIFAR-10 Batch 5:  loss: 1.54172 accuracy: 0.537\n",
      "Epoch 395, CIFAR-10 Batch 1:  loss: 1.53779 accuracy: 0.5384\n",
      "Epoch 395, CIFAR-10 Batch 2:  loss: 1.5795 accuracy: 0.5452\n",
      "Epoch 395, CIFAR-10 Batch 3:  loss: 1.55051 accuracy: 0.544\n",
      "Epoch 395, CIFAR-10 Batch 4:  loss: 1.50621 accuracy: 0.5444\n",
      "Epoch 395, CIFAR-10 Batch 5:  loss: 1.56494 accuracy: 0.535\n",
      "Epoch 396, CIFAR-10 Batch 1:  loss: 1.58387 accuracy: 0.536\n",
      "Epoch 396, CIFAR-10 Batch 2:  loss: 1.61367 accuracy: 0.544\n",
      "Epoch 396, CIFAR-10 Batch 3:  loss: 1.54758 accuracy: 0.546\n",
      "Epoch 396, CIFAR-10 Batch 4:  loss: 1.52689 accuracy: 0.5446\n",
      "Epoch 396, CIFAR-10 Batch 5:  loss: 1.56516 accuracy: 0.532\n",
      "Epoch 397, CIFAR-10 Batch 1:  loss: 1.5698 accuracy: 0.5382\n",
      "Epoch 397, CIFAR-10 Batch 2:  loss: 1.60765 accuracy: 0.543\n",
      "Epoch 397, CIFAR-10 Batch 3:  loss: 1.55612 accuracy: 0.5438\n",
      "Epoch 397, CIFAR-10 Batch 4:  loss: 1.52148 accuracy: 0.5434\n",
      "Epoch 397, CIFAR-10 Batch 5:  loss: 1.56682 accuracy: 0.5328\n",
      "Epoch 398, CIFAR-10 Batch 1:  loss: 1.5747 accuracy: 0.539\n",
      "Epoch 398, CIFAR-10 Batch 2:  loss: 1.62242 accuracy: 0.5432\n",
      "Epoch 398, CIFAR-10 Batch 3:  loss: 1.56098 accuracy: 0.5436\n",
      "Epoch 398, CIFAR-10 Batch 4:  loss: 1.51878 accuracy: 0.5456\n",
      "Epoch 398, CIFAR-10 Batch 5:  loss: 1.57064 accuracy: 0.5344\n",
      "Epoch 399, CIFAR-10 Batch 1:  loss: 1.57723 accuracy: 0.5396\n",
      "Epoch 399, CIFAR-10 Batch 2:  loss: 1.6412 accuracy: 0.5416\n",
      "Epoch 399, CIFAR-10 Batch 3:  loss: 1.58143 accuracy: 0.5426\n",
      "Epoch 399, CIFAR-10 Batch 4:  loss: 1.51854 accuracy: 0.5442\n",
      "Epoch 399, CIFAR-10 Batch 5:  loss: 1.56823 accuracy: 0.5344\n",
      "Epoch 400, CIFAR-10 Batch 1:  loss: 1.57167 accuracy: 0.5404\n",
      "Epoch 400, CIFAR-10 Batch 2:  loss: 1.64345 accuracy: 0.5442\n",
      "Epoch 400, CIFAR-10 Batch 3:  loss: 1.59248 accuracy: 0.5414\n",
      "Epoch 400, CIFAR-10 Batch 4:  loss: 1.51236 accuracy: 0.5462\n",
      "Epoch 400, CIFAR-10 Batch 5:  loss: 1.56219 accuracy: 0.5342\n",
      "Epoch 401, CIFAR-10 Batch 1:  loss: 1.58931 accuracy: 0.5404\n",
      "Epoch 401, CIFAR-10 Batch 2:  loss: 1.62256 accuracy: 0.5396\n",
      "Epoch 401, CIFAR-10 Batch 3:  loss: 1.58384 accuracy: 0.5398\n",
      "Epoch 401, CIFAR-10 Batch 4:  loss: 1.51604 accuracy: 0.546\n",
      "Epoch 401, CIFAR-10 Batch 5:  loss: 1.57503 accuracy: 0.5306\n",
      "Epoch 402, CIFAR-10 Batch 1:  loss: 1.58176 accuracy: 0.5428\n",
      "Epoch 402, CIFAR-10 Batch 2:  loss: 1.65006 accuracy: 0.5434\n",
      "Epoch 402, CIFAR-10 Batch 3:  loss: 1.58654 accuracy: 0.5392\n",
      "Epoch 402, CIFAR-10 Batch 4:  loss: 1.51281 accuracy: 0.547\n",
      "Epoch 402, CIFAR-10 Batch 5:  loss: 1.5705 accuracy: 0.5338\n",
      "Epoch 403, CIFAR-10 Batch 1:  loss: 1.57539 accuracy: 0.5436\n",
      "Epoch 403, CIFAR-10 Batch 2:  loss: 1.64482 accuracy: 0.541\n",
      "Epoch 403, CIFAR-10 Batch 3:  loss: 1.57046 accuracy: 0.5384\n",
      "Epoch 403, CIFAR-10 Batch 4:  loss: 1.52848 accuracy: 0.5468\n",
      "Epoch 403, CIFAR-10 Batch 5:  loss: 1.56678 accuracy: 0.5324\n",
      "Epoch 404, CIFAR-10 Batch 1:  loss: 1.58429 accuracy: 0.5416\n",
      "Epoch 404, CIFAR-10 Batch 2:  loss: 1.65596 accuracy: 0.5424\n",
      "Epoch 404, CIFAR-10 Batch 3:  loss: 1.58036 accuracy: 0.5396\n",
      "Epoch 404, CIFAR-10 Batch 4:  loss: 1.51773 accuracy: 0.5456\n",
      "Epoch 404, CIFAR-10 Batch 5:  loss: 1.56572 accuracy: 0.5338\n",
      "Epoch 405, CIFAR-10 Batch 1:  loss: 1.57878 accuracy: 0.543\n",
      "Epoch 405, CIFAR-10 Batch 2:  loss: 1.65758 accuracy: 0.543\n",
      "Epoch 405, CIFAR-10 Batch 3:  loss: 1.58582 accuracy: 0.5392\n",
      "Epoch 405, CIFAR-10 Batch 4:  loss: 1.52836 accuracy: 0.5446\n",
      "Epoch 405, CIFAR-10 Batch 5:  loss: 1.57357 accuracy: 0.5344\n",
      "Epoch 406, CIFAR-10 Batch 1:  loss: 1.56285 accuracy: 0.5462\n",
      "Epoch 406, CIFAR-10 Batch 2:  loss: 1.63136 accuracy: 0.5432\n",
      "Epoch 406, CIFAR-10 Batch 3:  loss: 1.59827 accuracy: 0.5374\n",
      "Epoch 406, CIFAR-10 Batch 4:  loss: 1.52508 accuracy: 0.5464\n",
      "Epoch 406, CIFAR-10 Batch 5:  loss: 1.57494 accuracy: 0.5306\n",
      "Epoch 407, CIFAR-10 Batch 1:  loss: 1.57769 accuracy: 0.5412\n",
      "Epoch 407, CIFAR-10 Batch 2:  loss: 1.64643 accuracy: 0.5434\n",
      "Epoch 407, CIFAR-10 Batch 3:  loss: 1.58487 accuracy: 0.5388\n",
      "Epoch 407, CIFAR-10 Batch 4:  loss: 1.53494 accuracy: 0.5436\n",
      "Epoch 407, CIFAR-10 Batch 5:  loss: 1.56357 accuracy: 0.5332\n",
      "Epoch 408, CIFAR-10 Batch 1:  loss: 1.58529 accuracy: 0.542\n",
      "Epoch 408, CIFAR-10 Batch 2:  loss: 1.63645 accuracy: 0.544\n",
      "Epoch 408, CIFAR-10 Batch 3:  loss: 1.5774 accuracy: 0.5388\n",
      "Epoch 408, CIFAR-10 Batch 4:  loss: 1.53152 accuracy: 0.5452\n",
      "Epoch 408, CIFAR-10 Batch 5:  loss: 1.56663 accuracy: 0.5348\n",
      "Epoch 409, CIFAR-10 Batch 1:  loss: 1.58052 accuracy: 0.5414\n",
      "Epoch 409, CIFAR-10 Batch 2:  loss: 1.65324 accuracy: 0.5442\n",
      "Epoch 409, CIFAR-10 Batch 3:  loss: 1.57615 accuracy: 0.5398\n",
      "Epoch 409, CIFAR-10 Batch 4:  loss: 1.54932 accuracy: 0.5434\n",
      "Epoch 409, CIFAR-10 Batch 5:  loss: 1.56278 accuracy: 0.534\n",
      "Epoch 410, CIFAR-10 Batch 1:  loss: 1.58289 accuracy: 0.5396\n",
      "Epoch 410, CIFAR-10 Batch 2:  loss: 1.61772 accuracy: 0.5426\n",
      "Epoch 410, CIFAR-10 Batch 3:  loss: 1.57824 accuracy: 0.5414\n",
      "Epoch 410, CIFAR-10 Batch 4:  loss: 1.52974 accuracy: 0.5464\n",
      "Epoch 410, CIFAR-10 Batch 5:  loss: 1.57408 accuracy: 0.535\n",
      "Epoch 411, CIFAR-10 Batch 1:  loss: 1.57728 accuracy: 0.5432\n",
      "Epoch 411, CIFAR-10 Batch 2:  loss: 1.65705 accuracy: 0.5424\n",
      "Epoch 411, CIFAR-10 Batch 3:  loss: 1.57799 accuracy: 0.5394\n",
      "Epoch 411, CIFAR-10 Batch 4:  loss: 1.55201 accuracy: 0.5464\n",
      "Epoch 411, CIFAR-10 Batch 5:  loss: 1.57811 accuracy: 0.5344\n",
      "Epoch 412, CIFAR-10 Batch 1:  loss: 1.57027 accuracy: 0.5428\n",
      "Epoch 412, CIFAR-10 Batch 2:  loss: 1.64989 accuracy: 0.542\n",
      "Epoch 412, CIFAR-10 Batch 3:  loss: 1.57283 accuracy: 0.5406\n",
      "Epoch 412, CIFAR-10 Batch 4:  loss: 1.5605 accuracy: 0.547\n",
      "Epoch 412, CIFAR-10 Batch 5:  loss: 1.56945 accuracy: 0.5344\n",
      "Epoch 413, CIFAR-10 Batch 1:  loss: 1.59312 accuracy: 0.5406\n",
      "Epoch 413, CIFAR-10 Batch 2:  loss: 1.66807 accuracy: 0.5414\n",
      "Epoch 413, CIFAR-10 Batch 3:  loss: 1.57427 accuracy: 0.5434\n",
      "Epoch 413, CIFAR-10 Batch 4:  loss: 1.55836 accuracy: 0.5482\n",
      "Epoch 413, CIFAR-10 Batch 5:  loss: 1.54973 accuracy: 0.5354\n",
      "Epoch 414, CIFAR-10 Batch 1:  loss: 1.59072 accuracy: 0.5406\n",
      "Epoch 414, CIFAR-10 Batch 2:  loss: 1.66921 accuracy: 0.5412\n",
      "Epoch 414, CIFAR-10 Batch 3:  loss: 1.57021 accuracy: 0.5424\n",
      "Epoch 414, CIFAR-10 Batch 4:  loss: 1.56701 accuracy: 0.5472\n",
      "Epoch 414, CIFAR-10 Batch 5:  loss: 1.56962 accuracy: 0.5378\n",
      "Epoch 415, CIFAR-10 Batch 1:  loss: 1.59885 accuracy: 0.5396\n",
      "Epoch 415, CIFAR-10 Batch 2:  loss: 1.65299 accuracy: 0.544\n",
      "Epoch 415, CIFAR-10 Batch 3:  loss: 1.53259 accuracy: 0.5418\n",
      "Epoch 415, CIFAR-10 Batch 4:  loss: 1.55642 accuracy: 0.546\n",
      "Epoch 415, CIFAR-10 Batch 5:  loss: 1.56532 accuracy: 0.5366\n",
      "Epoch 416, CIFAR-10 Batch 1:  loss: 1.57788 accuracy: 0.5412\n",
      "Epoch 416, CIFAR-10 Batch 2:  loss: 1.64317 accuracy: 0.5434\n",
      "Epoch 416, CIFAR-10 Batch 3:  loss: 1.52841 accuracy: 0.5418\n",
      "Epoch 416, CIFAR-10 Batch 4:  loss: 1.54802 accuracy: 0.546\n",
      "Epoch 416, CIFAR-10 Batch 5:  loss: 1.59469 accuracy: 0.535\n",
      "Epoch 417, CIFAR-10 Batch 1:  loss: 1.59464 accuracy: 0.543\n",
      "Epoch 417, CIFAR-10 Batch 2:  loss: 1.67406 accuracy: 0.5442\n",
      "Epoch 417, CIFAR-10 Batch 3:  loss: 1.57819 accuracy: 0.5426\n",
      "Epoch 417, CIFAR-10 Batch 4:  loss: 1.5707 accuracy: 0.546\n",
      "Epoch 417, CIFAR-10 Batch 5:  loss: 1.57535 accuracy: 0.5376\n",
      "Epoch 418, CIFAR-10 Batch 1:  loss: 1.58837 accuracy: 0.5398\n",
      "Epoch 418, CIFAR-10 Batch 2:  loss: 1.69244 accuracy: 0.5406\n",
      "Epoch 418, CIFAR-10 Batch 3:  loss: 1.58095 accuracy: 0.5402\n",
      "Epoch 418, CIFAR-10 Batch 4:  loss: 1.55957 accuracy: 0.5466\n",
      "Epoch 418, CIFAR-10 Batch 5:  loss: 1.58312 accuracy: 0.5376\n",
      "Epoch 419, CIFAR-10 Batch 1:  loss: 1.57851 accuracy: 0.5424\n",
      "Epoch 419, CIFAR-10 Batch 2:  loss: 1.67725 accuracy: 0.544\n",
      "Epoch 419, CIFAR-10 Batch 3:  loss: 1.58408 accuracy: 0.5394\n",
      "Epoch 419, CIFAR-10 Batch 4:  loss: 1.55744 accuracy: 0.547\n",
      "Epoch 419, CIFAR-10 Batch 5:  loss: 1.57079 accuracy: 0.5346\n",
      "Epoch 420, CIFAR-10 Batch 1:  loss: 1.58604 accuracy: 0.5422\n",
      "Epoch 420, CIFAR-10 Batch 2:  loss: 1.68054 accuracy: 0.543\n",
      "Epoch 420, CIFAR-10 Batch 3:  loss: 1.59382 accuracy: 0.543\n",
      "Epoch 420, CIFAR-10 Batch 4:  loss: 1.56826 accuracy: 0.545\n",
      "Epoch 420, CIFAR-10 Batch 5:  loss: 1.58686 accuracy: 0.5352\n",
      "Epoch 421, CIFAR-10 Batch 1:  loss: 1.59493 accuracy: 0.5444\n",
      "Epoch 421, CIFAR-10 Batch 2:  loss: 1.67662 accuracy: 0.5422\n",
      "Epoch 421, CIFAR-10 Batch 3:  loss: 1.59351 accuracy: 0.542\n",
      "Epoch 421, CIFAR-10 Batch 4:  loss: 1.57372 accuracy: 0.5446\n",
      "Epoch 421, CIFAR-10 Batch 5:  loss: 1.58465 accuracy: 0.5346\n",
      "Epoch 422, CIFAR-10 Batch 1:  loss: 1.6069 accuracy: 0.544\n",
      "Epoch 422, CIFAR-10 Batch 2:  loss: 1.69458 accuracy: 0.5418\n",
      "Epoch 422, CIFAR-10 Batch 3:  loss: 1.5785 accuracy: 0.5458\n",
      "Epoch 422, CIFAR-10 Batch 4:  loss: 1.5719 accuracy: 0.5462\n",
      "Epoch 422, CIFAR-10 Batch 5:  loss: 1.58155 accuracy: 0.536\n",
      "Epoch 423, CIFAR-10 Batch 1:  loss: 1.59564 accuracy: 0.5442\n",
      "Epoch 423, CIFAR-10 Batch 2:  loss: 1.68048 accuracy: 0.5436\n",
      "Epoch 423, CIFAR-10 Batch 3:  loss: 1.59444 accuracy: 0.5444\n",
      "Epoch 423, CIFAR-10 Batch 4:  loss: 1.55574 accuracy: 0.5458\n",
      "Epoch 423, CIFAR-10 Batch 5:  loss: 1.58418 accuracy: 0.5358\n",
      "Epoch 424, CIFAR-10 Batch 1:  loss: 1.61131 accuracy: 0.5414\n",
      "Epoch 424, CIFAR-10 Batch 2:  loss: 1.66274 accuracy: 0.5446\n",
      "Epoch 424, CIFAR-10 Batch 3:  loss: 1.60214 accuracy: 0.5438\n",
      "Epoch 424, CIFAR-10 Batch 4:  loss: 1.56601 accuracy: 0.5446\n",
      "Epoch 424, CIFAR-10 Batch 5:  loss: 1.58812 accuracy: 0.5342\n",
      "Epoch 425, CIFAR-10 Batch 1:  loss: 1.61551 accuracy: 0.5424\n",
      "Epoch 425, CIFAR-10 Batch 2:  loss: 1.6664 accuracy: 0.5422\n",
      "Epoch 425, CIFAR-10 Batch 3:  loss: 1.5909 accuracy: 0.5438\n",
      "Epoch 425, CIFAR-10 Batch 4:  loss: 1.56831 accuracy: 0.5448\n",
      "Epoch 425, CIFAR-10 Batch 5:  loss: 1.59927 accuracy: 0.535\n",
      "Epoch 426, CIFAR-10 Batch 1:  loss: 1.616 accuracy: 0.5424\n",
      "Epoch 426, CIFAR-10 Batch 2:  loss: 1.6794 accuracy: 0.543\n",
      "Epoch 426, CIFAR-10 Batch 3:  loss: 1.58346 accuracy: 0.5434\n",
      "Epoch 426, CIFAR-10 Batch 4:  loss: 1.58006 accuracy: 0.546\n",
      "Epoch 426, CIFAR-10 Batch 5:  loss: 1.5953 accuracy: 0.5348\n",
      "Epoch 427, CIFAR-10 Batch 1:  loss: 1.61159 accuracy: 0.5446\n",
      "Epoch 427, CIFAR-10 Batch 2:  loss: 1.68789 accuracy: 0.544\n",
      "Epoch 427, CIFAR-10 Batch 3:  loss: 1.58311 accuracy: 0.5456\n",
      "Epoch 427, CIFAR-10 Batch 4:  loss: 1.58034 accuracy: 0.5462\n",
      "Epoch 427, CIFAR-10 Batch 5:  loss: 1.58625 accuracy: 0.5356\n",
      "Epoch 428, CIFAR-10 Batch 1:  loss: 1.6111 accuracy: 0.5416\n",
      "Epoch 428, CIFAR-10 Batch 2:  loss: 1.69187 accuracy: 0.5444\n",
      "Epoch 428, CIFAR-10 Batch 3:  loss: 1.59194 accuracy: 0.5454\n",
      "Epoch 428, CIFAR-10 Batch 4:  loss: 1.58519 accuracy: 0.5452\n",
      "Epoch 428, CIFAR-10 Batch 5:  loss: 1.60631 accuracy: 0.538\n",
      "Epoch 429, CIFAR-10 Batch 1:  loss: 1.60305 accuracy: 0.542\n",
      "Epoch 429, CIFAR-10 Batch 2:  loss: 1.70044 accuracy: 0.5424\n",
      "Epoch 429, CIFAR-10 Batch 3:  loss: 1.57652 accuracy: 0.5468\n",
      "Epoch 429, CIFAR-10 Batch 4:  loss: 1.5813 accuracy: 0.548\n",
      "Epoch 429, CIFAR-10 Batch 5:  loss: 1.60459 accuracy: 0.5358\n",
      "Epoch 430, CIFAR-10 Batch 1:  loss: 1.61858 accuracy: 0.543\n",
      "Epoch 430, CIFAR-10 Batch 2:  loss: 1.70529 accuracy: 0.5438\n",
      "Epoch 430, CIFAR-10 Batch 3:  loss: 1.58205 accuracy: 0.5444\n",
      "Epoch 430, CIFAR-10 Batch 4:  loss: 1.58285 accuracy: 0.5466\n",
      "Epoch 430, CIFAR-10 Batch 5:  loss: 1.60267 accuracy: 0.535\n",
      "Epoch 431, CIFAR-10 Batch 1:  loss: 1.61296 accuracy: 0.5442\n",
      "Epoch 431, CIFAR-10 Batch 2:  loss: 1.70624 accuracy: 0.5422\n",
      "Epoch 431, CIFAR-10 Batch 3:  loss: 1.59712 accuracy: 0.5464\n",
      "Epoch 431, CIFAR-10 Batch 4:  loss: 1.58028 accuracy: 0.5466\n",
      "Epoch 431, CIFAR-10 Batch 5:  loss: 1.62369 accuracy: 0.5378\n",
      "Epoch 432, CIFAR-10 Batch 1:  loss: 1.61222 accuracy: 0.5442\n",
      "Epoch 432, CIFAR-10 Batch 2:  loss: 1.7108 accuracy: 0.5424\n",
      "Epoch 432, CIFAR-10 Batch 3:  loss: 1.6036 accuracy: 0.545\n",
      "Epoch 432, CIFAR-10 Batch 4:  loss: 1.58039 accuracy: 0.5468\n",
      "Epoch 432, CIFAR-10 Batch 5:  loss: 1.6167 accuracy: 0.5366\n",
      "Epoch 433, CIFAR-10 Batch 1:  loss: 1.61322 accuracy: 0.5436\n",
      "Epoch 433, CIFAR-10 Batch 2:  loss: 1.71006 accuracy: 0.542\n",
      "Epoch 433, CIFAR-10 Batch 3:  loss: 1.60434 accuracy: 0.5454\n",
      "Epoch 433, CIFAR-10 Batch 4:  loss: 1.57723 accuracy: 0.5466\n",
      "Epoch 433, CIFAR-10 Batch 5:  loss: 1.61167 accuracy: 0.5398\n",
      "Epoch 434, CIFAR-10 Batch 1:  loss: 1.60679 accuracy: 0.5442\n",
      "Epoch 434, CIFAR-10 Batch 2:  loss: 1.7072 accuracy: 0.5426\n",
      "Epoch 434, CIFAR-10 Batch 3:  loss: 1.60539 accuracy: 0.5468\n",
      "Epoch 434, CIFAR-10 Batch 4:  loss: 1.56531 accuracy: 0.5466\n",
      "Epoch 434, CIFAR-10 Batch 5:  loss: 1.61197 accuracy: 0.5374\n",
      "Epoch 435, CIFAR-10 Batch 1:  loss: 1.61252 accuracy: 0.5442\n",
      "Epoch 435, CIFAR-10 Batch 2:  loss: 1.70453 accuracy: 0.5442\n",
      "Epoch 435, CIFAR-10 Batch 3:  loss: 1.55303 accuracy: 0.5468\n",
      "Epoch 435, CIFAR-10 Batch 4:  loss: 1.56573 accuracy: 0.5476\n",
      "Epoch 435, CIFAR-10 Batch 5:  loss: 1.60488 accuracy: 0.536\n",
      "Epoch 436, CIFAR-10 Batch 1:  loss: 1.6099 accuracy: 0.544\n",
      "Epoch 436, CIFAR-10 Batch 2:  loss: 1.70457 accuracy: 0.5438\n",
      "Epoch 436, CIFAR-10 Batch 3:  loss: 1.61458 accuracy: 0.545\n",
      "Epoch 436, CIFAR-10 Batch 4:  loss: 1.56698 accuracy: 0.5476\n",
      "Epoch 436, CIFAR-10 Batch 5:  loss: 1.62485 accuracy: 0.5394\n",
      "Epoch 437, CIFAR-10 Batch 1:  loss: 1.61699 accuracy: 0.5412\n",
      "Epoch 437, CIFAR-10 Batch 2:  loss: 1.70246 accuracy: 0.5424\n",
      "Epoch 437, CIFAR-10 Batch 3:  loss: 1.59396 accuracy: 0.5468\n",
      "Epoch 437, CIFAR-10 Batch 4:  loss: 1.56994 accuracy: 0.5464\n",
      "Epoch 437, CIFAR-10 Batch 5:  loss: 1.62397 accuracy: 0.5374\n",
      "Epoch 438, CIFAR-10 Batch 1:  loss: 1.62648 accuracy: 0.5452\n",
      "Epoch 438, CIFAR-10 Batch 2:  loss: 1.71595 accuracy: 0.5438\n",
      "Epoch 438, CIFAR-10 Batch 3:  loss: 1.60441 accuracy: 0.5438\n",
      "Epoch 438, CIFAR-10 Batch 4:  loss: 1.56774 accuracy: 0.548\n",
      "Epoch 438, CIFAR-10 Batch 5:  loss: 1.62096 accuracy: 0.5386\n",
      "Epoch 439, CIFAR-10 Batch 1:  loss: 1.60764 accuracy: 0.5442\n",
      "Epoch 439, CIFAR-10 Batch 2:  loss: 1.70777 accuracy: 0.5412\n",
      "Epoch 439, CIFAR-10 Batch 3:  loss: 1.5821 accuracy: 0.5482\n",
      "Epoch 439, CIFAR-10 Batch 4:  loss: 1.56029 accuracy: 0.5482\n",
      "Epoch 439, CIFAR-10 Batch 5:  loss: 1.62983 accuracy: 0.5384\n",
      "Epoch 440, CIFAR-10 Batch 1:  loss: 1.62586 accuracy: 0.5448\n",
      "Epoch 440, CIFAR-10 Batch 2:  loss: 1.71499 accuracy: 0.544\n",
      "Epoch 440, CIFAR-10 Batch 3:  loss: 1.5855 accuracy: 0.546\n",
      "Epoch 440, CIFAR-10 Batch 4:  loss: 1.56431 accuracy: 0.5476\n",
      "Epoch 440, CIFAR-10 Batch 5:  loss: 1.63477 accuracy: 0.5368\n",
      "Epoch 441, CIFAR-10 Batch 1:  loss: 1.60572 accuracy: 0.5432\n",
      "Epoch 441, CIFAR-10 Batch 2:  loss: 1.71929 accuracy: 0.5424\n",
      "Epoch 441, CIFAR-10 Batch 3:  loss: 1.58422 accuracy: 0.5478\n",
      "Epoch 441, CIFAR-10 Batch 4:  loss: 1.57313 accuracy: 0.5462\n",
      "Epoch 441, CIFAR-10 Batch 5:  loss: 1.62969 accuracy: 0.5356\n",
      "Epoch 442, CIFAR-10 Batch 1:  loss: 1.61383 accuracy: 0.5432\n",
      "Epoch 442, CIFAR-10 Batch 2:  loss: 1.69494 accuracy: 0.548\n",
      "Epoch 442, CIFAR-10 Batch 3:  loss: 1.54663 accuracy: 0.5494\n",
      "Epoch 442, CIFAR-10 Batch 4:  loss: 1.56438 accuracy: 0.5488\n",
      "Epoch 442, CIFAR-10 Batch 5:  loss: 1.62723 accuracy: 0.5388\n",
      "Epoch 443, CIFAR-10 Batch 1:  loss: 1.60627 accuracy: 0.5458\n",
      "Epoch 443, CIFAR-10 Batch 2:  loss: 1.71596 accuracy: 0.5442\n",
      "Epoch 443, CIFAR-10 Batch 3:  loss: 1.56169 accuracy: 0.5492\n",
      "Epoch 443, CIFAR-10 Batch 4:  loss: 1.56516 accuracy: 0.547\n",
      "Epoch 443, CIFAR-10 Batch 5:  loss: 1.64214 accuracy: 0.537\n",
      "Epoch 444, CIFAR-10 Batch 1:  loss: 1.61862 accuracy: 0.5464\n",
      "Epoch 444, CIFAR-10 Batch 2:  loss: 1.6818 accuracy: 0.5414\n",
      "Epoch 444, CIFAR-10 Batch 3:  loss: 1.5587 accuracy: 0.5494\n",
      "Epoch 444, CIFAR-10 Batch 4:  loss: 1.58014 accuracy: 0.547\n",
      "Epoch 444, CIFAR-10 Batch 5:  loss: 1.66154 accuracy: 0.536\n",
      "Epoch 445, CIFAR-10 Batch 1:  loss: 1.61579 accuracy: 0.545\n",
      "Epoch 445, CIFAR-10 Batch 2:  loss: 1.69583 accuracy: 0.5436\n",
      "Epoch 445, CIFAR-10 Batch 3:  loss: 1.59673 accuracy: 0.5482\n",
      "Epoch 445, CIFAR-10 Batch 4:  loss: 1.55265 accuracy: 0.5486\n",
      "Epoch 445, CIFAR-10 Batch 5:  loss: 1.61515 accuracy: 0.5362\n",
      "Epoch 446, CIFAR-10 Batch 1:  loss: 1.61047 accuracy: 0.545\n",
      "Epoch 446, CIFAR-10 Batch 2:  loss: 1.71577 accuracy: 0.5448\n",
      "Epoch 446, CIFAR-10 Batch 3:  loss: 1.60486 accuracy: 0.5498\n",
      "Epoch 446, CIFAR-10 Batch 4:  loss: 1.57173 accuracy: 0.5492\n",
      "Epoch 446, CIFAR-10 Batch 5:  loss: 1.65131 accuracy: 0.535\n",
      "Epoch 447, CIFAR-10 Batch 1:  loss: 1.6181 accuracy: 0.5444\n",
      "Epoch 447, CIFAR-10 Batch 2:  loss: 1.71573 accuracy: 0.5452\n",
      "Epoch 447, CIFAR-10 Batch 3:  loss: 1.58189 accuracy: 0.5512\n",
      "Epoch 447, CIFAR-10 Batch 4:  loss: 1.55836 accuracy: 0.5484\n",
      "Epoch 447, CIFAR-10 Batch 5:  loss: 1.67477 accuracy: 0.536\n",
      "Epoch 448, CIFAR-10 Batch 1:  loss: 1.62113 accuracy: 0.5464\n",
      "Epoch 448, CIFAR-10 Batch 2:  loss: 1.69344 accuracy: 0.544\n",
      "Epoch 448, CIFAR-10 Batch 3:  loss: 1.5877 accuracy: 0.5482\n",
      "Epoch 448, CIFAR-10 Batch 4:  loss: 1.54942 accuracy: 0.5502\n",
      "Epoch 448, CIFAR-10 Batch 5:  loss: 1.65985 accuracy: 0.5352\n",
      "Epoch 449, CIFAR-10 Batch 1:  loss: 1.61302 accuracy: 0.5466\n",
      "Epoch 449, CIFAR-10 Batch 2:  loss: 1.70545 accuracy: 0.5468\n",
      "Epoch 449, CIFAR-10 Batch 3:  loss: 1.60432 accuracy: 0.5486\n",
      "Epoch 449, CIFAR-10 Batch 4:  loss: 1.56495 accuracy: 0.5498\n",
      "Epoch 449, CIFAR-10 Batch 5:  loss: 1.66932 accuracy: 0.5358\n",
      "Epoch 450, CIFAR-10 Batch 1:  loss: 1.61901 accuracy: 0.5466\n",
      "Epoch 450, CIFAR-10 Batch 2:  loss: 1.71065 accuracy: 0.5444\n",
      "Epoch 450, CIFAR-10 Batch 3:  loss: 1.60115 accuracy: 0.5502\n",
      "Epoch 450, CIFAR-10 Batch 4:  loss: 1.56532 accuracy: 0.547\n",
      "Epoch 450, CIFAR-10 Batch 5:  loss: 1.66518 accuracy: 0.5344\n",
      "Epoch 451, CIFAR-10 Batch 1:  loss: 1.60158 accuracy: 0.5464\n",
      "Epoch 451, CIFAR-10 Batch 2:  loss: 1.69949 accuracy: 0.5442\n",
      "Epoch 451, CIFAR-10 Batch 3:  loss: 1.57062 accuracy: 0.55\n",
      "Epoch 451, CIFAR-10 Batch 4:  loss: 1.57386 accuracy: 0.5468\n",
      "Epoch 451, CIFAR-10 Batch 5:  loss: 1.63283 accuracy: 0.537\n",
      "Epoch 452, CIFAR-10 Batch 1:  loss: 1.61628 accuracy: 0.5434\n",
      "Epoch 452, CIFAR-10 Batch 2:  loss: 1.71058 accuracy: 0.544\n",
      "Epoch 452, CIFAR-10 Batch 3:  loss: 1.5914 accuracy: 0.5484\n",
      "Epoch 452, CIFAR-10 Batch 4:  loss: 1.55336 accuracy: 0.5478\n",
      "Epoch 452, CIFAR-10 Batch 5:  loss: 1.608 accuracy: 0.5366\n",
      "Epoch 453, CIFAR-10 Batch 1:  loss: 1.62708 accuracy: 0.5476\n",
      "Epoch 453, CIFAR-10 Batch 2:  loss: 1.70712 accuracy: 0.5448\n",
      "Epoch 453, CIFAR-10 Batch 3:  loss: 1.60367 accuracy: 0.5474\n",
      "Epoch 453, CIFAR-10 Batch 4:  loss: 1.55629 accuracy: 0.5472\n",
      "Epoch 453, CIFAR-10 Batch 5:  loss: 1.6643 accuracy: 0.5352\n",
      "Epoch 454, CIFAR-10 Batch 1:  loss: 1.62462 accuracy: 0.5452\n",
      "Epoch 454, CIFAR-10 Batch 2:  loss: 1.70475 accuracy: 0.5436\n",
      "Epoch 454, CIFAR-10 Batch 3:  loss: 1.60262 accuracy: 0.5488\n",
      "Epoch 454, CIFAR-10 Batch 4:  loss: 1.56952 accuracy: 0.5444\n",
      "Epoch 454, CIFAR-10 Batch 5:  loss: 1.64575 accuracy: 0.5358\n",
      "Epoch 455, CIFAR-10 Batch 1:  loss: 1.62378 accuracy: 0.5464\n",
      "Epoch 455, CIFAR-10 Batch 2:  loss: 1.71112 accuracy: 0.5466\n",
      "Epoch 455, CIFAR-10 Batch 3:  loss: 1.60971 accuracy: 0.5498\n",
      "Epoch 455, CIFAR-10 Batch 4:  loss: 1.57286 accuracy: 0.5452\n",
      "Epoch 455, CIFAR-10 Batch 5:  loss: 1.65682 accuracy: 0.5338\n",
      "Epoch 456, CIFAR-10 Batch 1:  loss: 1.61554 accuracy: 0.5452\n",
      "Epoch 456, CIFAR-10 Batch 2:  loss: 1.70069 accuracy: 0.548\n",
      "Epoch 456, CIFAR-10 Batch 3:  loss: 1.60338 accuracy: 0.5512\n",
      "Epoch 456, CIFAR-10 Batch 4:  loss: 1.5788 accuracy: 0.547\n",
      "Epoch 456, CIFAR-10 Batch 5:  loss: 1.65415 accuracy: 0.535\n",
      "Epoch 457, CIFAR-10 Batch 1:  loss: 1.60193 accuracy: 0.545\n",
      "Epoch 457, CIFAR-10 Batch 2:  loss: 1.71331 accuracy: 0.5458\n",
      "Epoch 457, CIFAR-10 Batch 3:  loss: 1.61569 accuracy: 0.5486\n",
      "Epoch 457, CIFAR-10 Batch 4:  loss: 1.57192 accuracy: 0.5462\n",
      "Epoch 457, CIFAR-10 Batch 5:  loss: 1.65892 accuracy: 0.535\n",
      "Epoch 458, CIFAR-10 Batch 1:  loss: 1.61243 accuracy: 0.549\n",
      "Epoch 458, CIFAR-10 Batch 2:  loss: 1.7426 accuracy: 0.5408\n",
      "Epoch 458, CIFAR-10 Batch 3:  loss: 1.61305 accuracy: 0.5498\n",
      "Epoch 458, CIFAR-10 Batch 4:  loss: 1.58326 accuracy: 0.5464\n",
      "Epoch 458, CIFAR-10 Batch 5:  loss: 1.64838 accuracy: 0.5388\n",
      "Epoch 459, CIFAR-10 Batch 1:  loss: 1.60705 accuracy: 0.5488\n",
      "Epoch 459, CIFAR-10 Batch 2:  loss: 1.72761 accuracy: 0.5468\n",
      "Epoch 459, CIFAR-10 Batch 3:  loss: 1.6078 accuracy: 0.549\n",
      "Epoch 459, CIFAR-10 Batch 4:  loss: 1.57638 accuracy: 0.5472\n",
      "Epoch 459, CIFAR-10 Batch 5:  loss: 1.63947 accuracy: 0.5384\n",
      "Epoch 460, CIFAR-10 Batch 1:  loss: 1.62809 accuracy: 0.5462\n",
      "Epoch 460, CIFAR-10 Batch 2:  loss: 1.70731 accuracy: 0.5458\n",
      "Epoch 460, CIFAR-10 Batch 3:  loss: 1.62303 accuracy: 0.5524\n",
      "Epoch 460, CIFAR-10 Batch 4:  loss: 1.57352 accuracy: 0.5488\n",
      "Epoch 460, CIFAR-10 Batch 5:  loss: 1.65055 accuracy: 0.538\n",
      "Epoch 461, CIFAR-10 Batch 1:  loss: 1.6005 accuracy: 0.547\n",
      "Epoch 461, CIFAR-10 Batch 2:  loss: 1.71205 accuracy: 0.5462\n",
      "Epoch 461, CIFAR-10 Batch 3:  loss: 1.63968 accuracy: 0.5492\n",
      "Epoch 461, CIFAR-10 Batch 4:  loss: 1.58143 accuracy: 0.5476\n",
      "Epoch 461, CIFAR-10 Batch 5:  loss: 1.64853 accuracy: 0.5366\n",
      "Epoch 462, CIFAR-10 Batch 1:  loss: 1.61592 accuracy: 0.5472\n",
      "Epoch 462, CIFAR-10 Batch 2:  loss: 1.7035 accuracy: 0.5478\n",
      "Epoch 462, CIFAR-10 Batch 3:  loss: 1.61949 accuracy: 0.5496\n",
      "Epoch 462, CIFAR-10 Batch 4:  loss: 1.57514 accuracy: 0.5466\n",
      "Epoch 462, CIFAR-10 Batch 5:  loss: 1.66228 accuracy: 0.5372\n",
      "Epoch 463, CIFAR-10 Batch 1:  loss: 1.6149 accuracy: 0.5476\n",
      "Epoch 463, CIFAR-10 Batch 2:  loss: 1.71697 accuracy: 0.5464\n",
      "Epoch 463, CIFAR-10 Batch 3:  loss: 1.62763 accuracy: 0.5492\n",
      "Epoch 463, CIFAR-10 Batch 4:  loss: 1.56759 accuracy: 0.546\n",
      "Epoch 463, CIFAR-10 Batch 5:  loss: 1.66639 accuracy: 0.5392\n",
      "Epoch 464, CIFAR-10 Batch 1:  loss: 1.58562 accuracy: 0.5468\n",
      "Epoch 464, CIFAR-10 Batch 2:  loss: 1.6972 accuracy: 0.5482\n",
      "Epoch 464, CIFAR-10 Batch 3:  loss: 1.5985 accuracy: 0.551\n",
      "Epoch 464, CIFAR-10 Batch 4:  loss: 1.58728 accuracy: 0.5496\n",
      "Epoch 464, CIFAR-10 Batch 5:  loss: 1.66528 accuracy: 0.5374\n",
      "Epoch 465, CIFAR-10 Batch 1:  loss: 1.59137 accuracy: 0.5462\n",
      "Epoch 465, CIFAR-10 Batch 2:  loss: 1.66232 accuracy: 0.548\n",
      "Epoch 465, CIFAR-10 Batch 3:  loss: 1.61591 accuracy: 0.55\n",
      "Epoch 465, CIFAR-10 Batch 4:  loss: 1.58242 accuracy: 0.5444\n",
      "Epoch 465, CIFAR-10 Batch 5:  loss: 1.66237 accuracy: 0.5374\n",
      "Epoch 466, CIFAR-10 Batch 1:  loss: 1.60885 accuracy: 0.5454\n",
      "Epoch 466, CIFAR-10 Batch 2:  loss: 1.68487 accuracy: 0.5488\n",
      "Epoch 466, CIFAR-10 Batch 3:  loss: 1.59497 accuracy: 0.5496\n",
      "Epoch 466, CIFAR-10 Batch 4:  loss: 1.57686 accuracy: 0.5462\n",
      "Epoch 466, CIFAR-10 Batch 5:  loss: 1.66117 accuracy: 0.536\n",
      "Epoch 467, CIFAR-10 Batch 1:  loss: 1.6093 accuracy: 0.5466\n",
      "Epoch 467, CIFAR-10 Batch 2:  loss: 1.6985 accuracy: 0.5488\n",
      "Epoch 467, CIFAR-10 Batch 3:  loss: 1.59971 accuracy: 0.5492\n",
      "Epoch 467, CIFAR-10 Batch 4:  loss: 1.57574 accuracy: 0.546\n",
      "Epoch 467, CIFAR-10 Batch 5:  loss: 1.65252 accuracy: 0.5368\n",
      "Epoch 468, CIFAR-10 Batch 1:  loss: 1.63026 accuracy: 0.5444\n",
      "Epoch 468, CIFAR-10 Batch 2:  loss: 1.69568 accuracy: 0.5458\n",
      "Epoch 468, CIFAR-10 Batch 3:  loss: 1.61461 accuracy: 0.5514\n",
      "Epoch 468, CIFAR-10 Batch 4:  loss: 1.56425 accuracy: 0.5498\n",
      "Epoch 468, CIFAR-10 Batch 5:  loss: 1.65603 accuracy: 0.5392\n",
      "Epoch 469, CIFAR-10 Batch 1:  loss: 1.62235 accuracy: 0.5476\n",
      "Epoch 469, CIFAR-10 Batch 2:  loss: 1.6758 accuracy: 0.5466\n",
      "Epoch 469, CIFAR-10 Batch 3:  loss: 1.62084 accuracy: 0.5504\n",
      "Epoch 469, CIFAR-10 Batch 4:  loss: 1.57422 accuracy: 0.5502\n",
      "Epoch 469, CIFAR-10 Batch 5:  loss: 1.65208 accuracy: 0.5384\n",
      "Epoch 470, CIFAR-10 Batch 1:  loss: 1.63711 accuracy: 0.5468\n",
      "Epoch 470, CIFAR-10 Batch 2:  loss: 1.67281 accuracy: 0.5472\n",
      "Epoch 470, CIFAR-10 Batch 3:  loss: 1.61888 accuracy: 0.5492\n",
      "Epoch 470, CIFAR-10 Batch 4:  loss: 1.56685 accuracy: 0.5484\n",
      "Epoch 470, CIFAR-10 Batch 5:  loss: 1.64819 accuracy: 0.5402\n",
      "Epoch 471, CIFAR-10 Batch 1:  loss: 1.61246 accuracy: 0.5476\n",
      "Epoch 471, CIFAR-10 Batch 2:  loss: 1.70884 accuracy: 0.5456\n",
      "Epoch 471, CIFAR-10 Batch 3:  loss: 1.63255 accuracy: 0.5512\n",
      "Epoch 471, CIFAR-10 Batch 4:  loss: 1.56788 accuracy: 0.5492\n",
      "Epoch 471, CIFAR-10 Batch 5:  loss: 1.67548 accuracy: 0.5416\n",
      "Epoch 472, CIFAR-10 Batch 1:  loss: 1.62122 accuracy: 0.5478\n",
      "Epoch 472, CIFAR-10 Batch 2:  loss: 1.73186 accuracy: 0.5466\n",
      "Epoch 472, CIFAR-10 Batch 3:  loss: 1.62769 accuracy: 0.5498\n",
      "Epoch 472, CIFAR-10 Batch 4:  loss: 1.56857 accuracy: 0.551\n",
      "Epoch 472, CIFAR-10 Batch 5:  loss: 1.67229 accuracy: 0.5386\n",
      "Epoch 473, CIFAR-10 Batch 1:  loss: 1.62191 accuracy: 0.548\n",
      "Epoch 473, CIFAR-10 Batch 2:  loss: 1.73854 accuracy: 0.5464\n",
      "Epoch 473, CIFAR-10 Batch 3:  loss: 1.62872 accuracy: 0.5498\n",
      "Epoch 473, CIFAR-10 Batch 4:  loss: 1.5569 accuracy: 0.548\n",
      "Epoch 473, CIFAR-10 Batch 5:  loss: 1.65611 accuracy: 0.5414\n",
      "Epoch 474, CIFAR-10 Batch 1:  loss: 1.59116 accuracy: 0.5466\n",
      "Epoch 474, CIFAR-10 Batch 2:  loss: 1.69204 accuracy: 0.5476\n",
      "Epoch 474, CIFAR-10 Batch 3:  loss: 1.63493 accuracy: 0.5508\n",
      "Epoch 474, CIFAR-10 Batch 4:  loss: 1.55926 accuracy: 0.5492\n",
      "Epoch 474, CIFAR-10 Batch 5:  loss: 1.64362 accuracy: 0.5402\n",
      "Epoch 475, CIFAR-10 Batch 1:  loss: 1.63589 accuracy: 0.5498\n",
      "Epoch 475, CIFAR-10 Batch 2:  loss: 1.71187 accuracy: 0.546\n",
      "Epoch 475, CIFAR-10 Batch 3:  loss: 1.64687 accuracy: 0.5508\n",
      "Epoch 475, CIFAR-10 Batch 4:  loss: 1.56181 accuracy: 0.547\n",
      "Epoch 475, CIFAR-10 Batch 5:  loss: 1.63124 accuracy: 0.5392\n",
      "Epoch 476, CIFAR-10 Batch 1:  loss: 1.63614 accuracy: 0.5474\n",
      "Epoch 476, CIFAR-10 Batch 2:  loss: 1.70563 accuracy: 0.5462\n",
      "Epoch 476, CIFAR-10 Batch 3:  loss: 1.64546 accuracy: 0.549\n",
      "Epoch 476, CIFAR-10 Batch 4:  loss: 1.57364 accuracy: 0.5492\n",
      "Epoch 476, CIFAR-10 Batch 5:  loss: 1.62391 accuracy: 0.5388\n",
      "Epoch 477, CIFAR-10 Batch 1:  loss: 1.64543 accuracy: 0.5488\n",
      "Epoch 477, CIFAR-10 Batch 2:  loss: 1.71556 accuracy: 0.545\n",
      "Epoch 477, CIFAR-10 Batch 3:  loss: 1.67138 accuracy: 0.5502\n",
      "Epoch 477, CIFAR-10 Batch 4:  loss: 1.58362 accuracy: 0.5476\n",
      "Epoch 477, CIFAR-10 Batch 5:  loss: 1.63526 accuracy: 0.5414\n",
      "Epoch 478, CIFAR-10 Batch 1:  loss: 1.63082 accuracy: 0.549\n",
      "Epoch 478, CIFAR-10 Batch 2:  loss: 1.69141 accuracy: 0.5488\n",
      "Epoch 478, CIFAR-10 Batch 3:  loss: 1.63665 accuracy: 0.5494\n",
      "Epoch 478, CIFAR-10 Batch 4:  loss: 1.5791 accuracy: 0.5484\n",
      "Epoch 478, CIFAR-10 Batch 5:  loss: 1.64403 accuracy: 0.5406\n",
      "Epoch 479, CIFAR-10 Batch 1:  loss: 1.6342 accuracy: 0.5498\n",
      "Epoch 479, CIFAR-10 Batch 2:  loss: 1.68649 accuracy: 0.548\n",
      "Epoch 479, CIFAR-10 Batch 3:  loss: 1.66459 accuracy: 0.5496\n",
      "Epoch 479, CIFAR-10 Batch 4:  loss: 1.57992 accuracy: 0.5474\n",
      "Epoch 479, CIFAR-10 Batch 5:  loss: 1.63431 accuracy: 0.5396\n",
      "Epoch 480, CIFAR-10 Batch 1:  loss: 1.65124 accuracy: 0.547\n",
      "Epoch 480, CIFAR-10 Batch 2:  loss: 1.71062 accuracy: 0.5464\n",
      "Epoch 480, CIFAR-10 Batch 3:  loss: 1.65077 accuracy: 0.5508\n",
      "Epoch 480, CIFAR-10 Batch 4:  loss: 1.59678 accuracy: 0.5484\n",
      "Epoch 480, CIFAR-10 Batch 5:  loss: 1.62585 accuracy: 0.5406\n",
      "Epoch 481, CIFAR-10 Batch 1:  loss: 1.66249 accuracy: 0.5448\n",
      "Epoch 481, CIFAR-10 Batch 2:  loss: 1.71825 accuracy: 0.546\n",
      "Epoch 481, CIFAR-10 Batch 3:  loss: 1.66561 accuracy: 0.548\n",
      "Epoch 481, CIFAR-10 Batch 4:  loss: 1.58776 accuracy: 0.5494\n",
      "Epoch 481, CIFAR-10 Batch 5:  loss: 1.63862 accuracy: 0.5388\n",
      "Epoch 482, CIFAR-10 Batch 1:  loss: 1.66092 accuracy: 0.5468\n",
      "Epoch 482, CIFAR-10 Batch 2:  loss: 1.72019 accuracy: 0.5456\n",
      "Epoch 482, CIFAR-10 Batch 3:  loss: 1.67051 accuracy: 0.5492\n",
      "Epoch 482, CIFAR-10 Batch 4:  loss: 1.60769 accuracy: 0.5486\n",
      "Epoch 482, CIFAR-10 Batch 5:  loss: 1.62453 accuracy: 0.5376\n",
      "Epoch 483, CIFAR-10 Batch 1:  loss: 1.65694 accuracy: 0.5494\n",
      "Epoch 483, CIFAR-10 Batch 2:  loss: 1.71587 accuracy: 0.5462\n",
      "Epoch 483, CIFAR-10 Batch 3:  loss: 1.64158 accuracy: 0.548\n",
      "Epoch 483, CIFAR-10 Batch 4:  loss: 1.57001 accuracy: 0.549\n",
      "Epoch 483, CIFAR-10 Batch 5:  loss: 1.6475 accuracy: 0.5408\n",
      "Epoch 484, CIFAR-10 Batch 1:  loss: 1.64808 accuracy: 0.5486\n",
      "Epoch 484, CIFAR-10 Batch 2:  loss: 1.73259 accuracy: 0.5444\n",
      "Epoch 484, CIFAR-10 Batch 3:  loss: 1.65047 accuracy: 0.5518\n",
      "Epoch 484, CIFAR-10 Batch 4:  loss: 1.58466 accuracy: 0.55\n",
      "Epoch 484, CIFAR-10 Batch 5:  loss: 1.64496 accuracy: 0.5386\n",
      "Epoch 485, CIFAR-10 Batch 1:  loss: 1.6557 accuracy: 0.5478\n",
      "Epoch 485, CIFAR-10 Batch 2:  loss: 1.72991 accuracy: 0.5456\n",
      "Epoch 485, CIFAR-10 Batch 3:  loss: 1.66284 accuracy: 0.551\n",
      "Epoch 485, CIFAR-10 Batch 4:  loss: 1.5827 accuracy: 0.5484\n",
      "Epoch 485, CIFAR-10 Batch 5:  loss: 1.65431 accuracy: 0.5386\n",
      "Epoch 486, CIFAR-10 Batch 1:  loss: 1.64942 accuracy: 0.5494\n",
      "Epoch 486, CIFAR-10 Batch 2:  loss: 1.72415 accuracy: 0.5476\n",
      "Epoch 486, CIFAR-10 Batch 3:  loss: 1.65153 accuracy: 0.5502\n",
      "Epoch 486, CIFAR-10 Batch 4:  loss: 1.60089 accuracy: 0.5478\n",
      "Epoch 486, CIFAR-10 Batch 5:  loss: 1.64615 accuracy: 0.5392\n",
      "Epoch 487, CIFAR-10 Batch 1:  loss: 1.65736 accuracy: 0.5502\n",
      "Epoch 487, CIFAR-10 Batch 2:  loss: 1.68529 accuracy: 0.5466\n",
      "Epoch 487, CIFAR-10 Batch 3:  loss: 1.65249 accuracy: 0.5482\n",
      "Epoch 487, CIFAR-10 Batch 4:  loss: 1.59973 accuracy: 0.55\n",
      "Epoch 487, CIFAR-10 Batch 5:  loss: 1.64582 accuracy: 0.5388\n",
      "Epoch 488, CIFAR-10 Batch 1:  loss: 1.66018 accuracy: 0.5486\n",
      "Epoch 488, CIFAR-10 Batch 2:  loss: 1.71521 accuracy: 0.5466\n",
      "Epoch 488, CIFAR-10 Batch 3:  loss: 1.66681 accuracy: 0.549\n",
      "Epoch 488, CIFAR-10 Batch 4:  loss: 1.60337 accuracy: 0.5492\n",
      "Epoch 488, CIFAR-10 Batch 5:  loss: 1.64198 accuracy: 0.5416\n",
      "Epoch 489, CIFAR-10 Batch 1:  loss: 1.65182 accuracy: 0.5504\n",
      "Epoch 489, CIFAR-10 Batch 2:  loss: 1.72334 accuracy: 0.5468\n",
      "Epoch 489, CIFAR-10 Batch 3:  loss: 1.67263 accuracy: 0.5496\n",
      "Epoch 489, CIFAR-10 Batch 4:  loss: 1.59778 accuracy: 0.5512\n",
      "Epoch 489, CIFAR-10 Batch 5:  loss: 1.65015 accuracy: 0.538\n",
      "Epoch 490, CIFAR-10 Batch 1:  loss: 1.65954 accuracy: 0.55\n",
      "Epoch 490, CIFAR-10 Batch 2:  loss: 1.71177 accuracy: 0.547\n",
      "Epoch 490, CIFAR-10 Batch 3:  loss: 1.68265 accuracy: 0.5498\n",
      "Epoch 490, CIFAR-10 Batch 4:  loss: 1.56817 accuracy: 0.551\n",
      "Epoch 490, CIFAR-10 Batch 5:  loss: 1.63376 accuracy: 0.5384\n",
      "Epoch 491, CIFAR-10 Batch 1:  loss: 1.66657 accuracy: 0.5498\n",
      "Epoch 491, CIFAR-10 Batch 2:  loss: 1.71432 accuracy: 0.5472\n",
      "Epoch 491, CIFAR-10 Batch 3:  loss: 1.67725 accuracy: 0.5508\n",
      "Epoch 491, CIFAR-10 Batch 4:  loss: 1.59303 accuracy: 0.549\n",
      "Epoch 491, CIFAR-10 Batch 5:  loss: 1.64278 accuracy: 0.539\n",
      "Epoch 492, CIFAR-10 Batch 1:  loss: 1.6552 accuracy: 0.549\n",
      "Epoch 492, CIFAR-10 Batch 2:  loss: 1.7209 accuracy: 0.545\n",
      "Epoch 492, CIFAR-10 Batch 3:  loss: 1.6475 accuracy: 0.55\n",
      "Epoch 492, CIFAR-10 Batch 4:  loss: 1.60354 accuracy: 0.5496\n",
      "Epoch 492, CIFAR-10 Batch 5:  loss: 1.66019 accuracy: 0.5394\n",
      "Epoch 493, CIFAR-10 Batch 1:  loss: 1.64128 accuracy: 0.5506\n",
      "Epoch 493, CIFAR-10 Batch 2:  loss: 1.73794 accuracy: 0.5452\n",
      "Epoch 493, CIFAR-10 Batch 3:  loss: 1.66027 accuracy: 0.551\n",
      "Epoch 493, CIFAR-10 Batch 4:  loss: 1.58017 accuracy: 0.549\n",
      "Epoch 493, CIFAR-10 Batch 5:  loss: 1.64831 accuracy: 0.542\n",
      "Epoch 494, CIFAR-10 Batch 1:  loss: 1.64989 accuracy: 0.5508\n",
      "Epoch 494, CIFAR-10 Batch 2:  loss: 1.715 accuracy: 0.5446\n",
      "Epoch 494, CIFAR-10 Batch 3:  loss: 1.67178 accuracy: 0.5504\n",
      "Epoch 494, CIFAR-10 Batch 4:  loss: 1.60711 accuracy: 0.5466\n",
      "Epoch 494, CIFAR-10 Batch 5:  loss: 1.6481 accuracy: 0.5388\n",
      "Epoch 495, CIFAR-10 Batch 1:  loss: 1.64216 accuracy: 0.5492\n",
      "Epoch 495, CIFAR-10 Batch 2:  loss: 1.68105 accuracy: 0.5454\n",
      "Epoch 495, CIFAR-10 Batch 3:  loss: 1.67428 accuracy: 0.5502\n",
      "Epoch 495, CIFAR-10 Batch 4:  loss: 1.60522 accuracy: 0.5456\n",
      "Epoch 495, CIFAR-10 Batch 5:  loss: 1.6442 accuracy: 0.54\n",
      "Epoch 496, CIFAR-10 Batch 1:  loss: 1.64983 accuracy: 0.5502\n",
      "Epoch 496, CIFAR-10 Batch 2:  loss: 1.71954 accuracy: 0.5466\n",
      "Epoch 496, CIFAR-10 Batch 3:  loss: 1.65802 accuracy: 0.5526\n",
      "Epoch 496, CIFAR-10 Batch 4:  loss: 1.59855 accuracy: 0.5476\n",
      "Epoch 496, CIFAR-10 Batch 5:  loss: 1.66737 accuracy: 0.5404\n",
      "Epoch 497, CIFAR-10 Batch 1:  loss: 1.63572 accuracy: 0.5502\n",
      "Epoch 497, CIFAR-10 Batch 2:  loss: 1.69017 accuracy: 0.5444\n",
      "Epoch 497, CIFAR-10 Batch 3:  loss: 1.67405 accuracy: 0.5504\n",
      "Epoch 497, CIFAR-10 Batch 4:  loss: 1.59695 accuracy: 0.546\n",
      "Epoch 497, CIFAR-10 Batch 5:  loss: 1.63957 accuracy: 0.541\n",
      "Epoch 498, CIFAR-10 Batch 1:  loss: 1.65928 accuracy: 0.5496\n",
      "Epoch 498, CIFAR-10 Batch 2:  loss: 1.70775 accuracy: 0.5458\n",
      "Epoch 498, CIFAR-10 Batch 3:  loss: 1.68359 accuracy: 0.5496\n",
      "Epoch 498, CIFAR-10 Batch 4:  loss: 1.60271 accuracy: 0.5474\n",
      "Epoch 498, CIFAR-10 Batch 5:  loss: 1.6529 accuracy: 0.54\n",
      "Epoch 499, CIFAR-10 Batch 1:  loss: 1.67516 accuracy: 0.5516\n",
      "Epoch 499, CIFAR-10 Batch 2:  loss: 1.7132 accuracy: 0.544\n",
      "Epoch 499, CIFAR-10 Batch 3:  loss: 1.6962 accuracy: 0.5526\n",
      "Epoch 499, CIFAR-10 Batch 4:  loss: 1.59527 accuracy: 0.547\n",
      "Epoch 499, CIFAR-10 Batch 5:  loss: 1.65964 accuracy: 0.5402\n",
      "Epoch 500, CIFAR-10 Batch 1:  loss: 1.66394 accuracy: 0.5512\n",
      "Epoch 500, CIFAR-10 Batch 2:  loss: 1.74741 accuracy: 0.5494\n",
      "Epoch 500, CIFAR-10 Batch 3:  loss: 1.66355 accuracy: 0.5528\n",
      "Epoch 500, CIFAR-10 Batch 4:  loss: 1.59849 accuracy: 0.5484\n",
      "Epoch 500, CIFAR-10 Batch 5:  loss: 1.64407 accuracy: 0.5388\n",
      "Epoch 501, CIFAR-10 Batch 1:  loss: 1.6425 accuracy: 0.5498\n",
      "Epoch 501, CIFAR-10 Batch 2:  loss: 1.69113 accuracy: 0.545\n",
      "Epoch 501, CIFAR-10 Batch 3:  loss: 1.65277 accuracy: 0.5518\n",
      "Epoch 501, CIFAR-10 Batch 4:  loss: 1.61919 accuracy: 0.5484\n",
      "Epoch 501, CIFAR-10 Batch 5:  loss: 1.66931 accuracy: 0.5406\n",
      "Epoch 502, CIFAR-10 Batch 1:  loss: 1.67242 accuracy: 0.5484\n",
      "Epoch 502, CIFAR-10 Batch 2:  loss: 1.72169 accuracy: 0.5502\n",
      "Epoch 502, CIFAR-10 Batch 3:  loss: 1.68663 accuracy: 0.5506\n",
      "Epoch 502, CIFAR-10 Batch 4:  loss: 1.60521 accuracy: 0.548\n",
      "Epoch 502, CIFAR-10 Batch 5:  loss: 1.6669 accuracy: 0.5412\n",
      "Epoch 503, CIFAR-10 Batch 1:  loss: 1.64852 accuracy: 0.5492\n",
      "Epoch 503, CIFAR-10 Batch 2:  loss: 1.74855 accuracy: 0.5468\n",
      "Epoch 503, CIFAR-10 Batch 3:  loss: 1.69436 accuracy: 0.5526\n",
      "Epoch 503, CIFAR-10 Batch 4:  loss: 1.6133 accuracy: 0.5466\n",
      "Epoch 503, CIFAR-10 Batch 5:  loss: 1.64298 accuracy: 0.5408\n",
      "Epoch 504, CIFAR-10 Batch 1:  loss: 1.66193 accuracy: 0.5488\n",
      "Epoch 504, CIFAR-10 Batch 2:  loss: 1.74265 accuracy: 0.5466\n",
      "Epoch 504, CIFAR-10 Batch 3:  loss: 1.70923 accuracy: 0.5496\n",
      "Epoch 504, CIFAR-10 Batch 4:  loss: 1.60564 accuracy: 0.5476\n",
      "Epoch 504, CIFAR-10 Batch 5:  loss: 1.66503 accuracy: 0.54\n",
      "Epoch 505, CIFAR-10 Batch 1:  loss: 1.64727 accuracy: 0.55\n",
      "Epoch 505, CIFAR-10 Batch 2:  loss: 1.69683 accuracy: 0.5474\n",
      "Epoch 505, CIFAR-10 Batch 3:  loss: 1.6915 accuracy: 0.5492\n",
      "Epoch 505, CIFAR-10 Batch 4:  loss: 1.61004 accuracy: 0.5464\n",
      "Epoch 505, CIFAR-10 Batch 5:  loss: 1.66645 accuracy: 0.5394\n",
      "Epoch 506, CIFAR-10 Batch 1:  loss: 1.65416 accuracy: 0.55\n",
      "Epoch 506, CIFAR-10 Batch 2:  loss: 1.75385 accuracy: 0.5462\n",
      "Epoch 506, CIFAR-10 Batch 3:  loss: 1.71079 accuracy: 0.5498\n",
      "Epoch 506, CIFAR-10 Batch 4:  loss: 1.62676 accuracy: 0.545\n",
      "Epoch 506, CIFAR-10 Batch 5:  loss: 1.65011 accuracy: 0.542\n",
      "Epoch 507, CIFAR-10 Batch 1:  loss: 1.66414 accuracy: 0.547\n",
      "Epoch 507, CIFAR-10 Batch 2:  loss: 1.70888 accuracy: 0.546\n",
      "Epoch 507, CIFAR-10 Batch 3:  loss: 1.6705 accuracy: 0.5502\n",
      "Epoch 507, CIFAR-10 Batch 4:  loss: 1.62241 accuracy: 0.5468\n",
      "Epoch 507, CIFAR-10 Batch 5:  loss: 1.64981 accuracy: 0.5426\n",
      "Epoch 508, CIFAR-10 Batch 1:  loss: 1.65295 accuracy: 0.5498\n",
      "Epoch 508, CIFAR-10 Batch 2:  loss: 1.74881 accuracy: 0.5448\n",
      "Epoch 508, CIFAR-10 Batch 3:  loss: 1.69611 accuracy: 0.5502\n",
      "Epoch 508, CIFAR-10 Batch 4:  loss: 1.62679 accuracy: 0.5484\n",
      "Epoch 508, CIFAR-10 Batch 5:  loss: 1.66094 accuracy: 0.543\n",
      "Epoch 509, CIFAR-10 Batch 1:  loss: 1.68712 accuracy: 0.5512\n",
      "Epoch 509, CIFAR-10 Batch 2:  loss: 1.72709 accuracy: 0.5474\n",
      "Epoch 509, CIFAR-10 Batch 3:  loss: 1.71552 accuracy: 0.5514\n",
      "Epoch 509, CIFAR-10 Batch 4:  loss: 1.62669 accuracy: 0.5474\n",
      "Epoch 509, CIFAR-10 Batch 5:  loss: 1.67245 accuracy: 0.539\n",
      "Epoch 510, CIFAR-10 Batch 1:  loss: 1.66942 accuracy: 0.5504\n",
      "Epoch 510, CIFAR-10 Batch 2:  loss: 1.75157 accuracy: 0.5462\n",
      "Epoch 510, CIFAR-10 Batch 3:  loss: 1.71963 accuracy: 0.5492\n",
      "Epoch 510, CIFAR-10 Batch 4:  loss: 1.63959 accuracy: 0.5462\n",
      "Epoch 510, CIFAR-10 Batch 5:  loss: 1.65453 accuracy: 0.541\n",
      "Epoch 511, CIFAR-10 Batch 1:  loss: 1.67609 accuracy: 0.5492\n",
      "Epoch 511, CIFAR-10 Batch 2:  loss: 1.75469 accuracy: 0.5446\n",
      "Epoch 511, CIFAR-10 Batch 3:  loss: 1.70943 accuracy: 0.5518\n",
      "Epoch 511, CIFAR-10 Batch 4:  loss: 1.63397 accuracy: 0.5466\n",
      "Epoch 511, CIFAR-10 Batch 5:  loss: 1.64389 accuracy: 0.5406\n",
      "Epoch 512, CIFAR-10 Batch 1:  loss: 1.67676 accuracy: 0.5504\n",
      "Epoch 512, CIFAR-10 Batch 2:  loss: 1.74797 accuracy: 0.5452\n",
      "Epoch 512, CIFAR-10 Batch 3:  loss: 1.72453 accuracy: 0.5506\n",
      "Epoch 512, CIFAR-10 Batch 4:  loss: 1.64175 accuracy: 0.5474\n",
      "Epoch 512, CIFAR-10 Batch 5:  loss: 1.67628 accuracy: 0.5394\n",
      "Epoch 513, CIFAR-10 Batch 1:  loss: 1.67633 accuracy: 0.5482\n",
      "Epoch 513, CIFAR-10 Batch 2:  loss: 1.73871 accuracy: 0.543\n",
      "Epoch 513, CIFAR-10 Batch 3:  loss: 1.73702 accuracy: 0.5506\n",
      "Epoch 513, CIFAR-10 Batch 4:  loss: 1.63562 accuracy: 0.546\n",
      "Epoch 513, CIFAR-10 Batch 5:  loss: 1.66421 accuracy: 0.54\n",
      "Epoch 514, CIFAR-10 Batch 1:  loss: 1.66845 accuracy: 0.5508\n",
      "Epoch 514, CIFAR-10 Batch 2:  loss: 1.70046 accuracy: 0.5468\n",
      "Epoch 514, CIFAR-10 Batch 3:  loss: 1.71568 accuracy: 0.5508\n",
      "Epoch 514, CIFAR-10 Batch 4:  loss: 1.62079 accuracy: 0.5472\n",
      "Epoch 514, CIFAR-10 Batch 5:  loss: 1.6784 accuracy: 0.5392\n",
      "Epoch 515, CIFAR-10 Batch 1:  loss: 1.68293 accuracy: 0.5504\n",
      "Epoch 515, CIFAR-10 Batch 2:  loss: 1.75955 accuracy: 0.5438\n",
      "Epoch 515, CIFAR-10 Batch 3:  loss: 1.73606 accuracy: 0.5508\n",
      "Epoch 515, CIFAR-10 Batch 4:  loss: 1.63392 accuracy: 0.5482\n",
      "Epoch 515, CIFAR-10 Batch 5:  loss: 1.69032 accuracy: 0.54\n",
      "Epoch 516, CIFAR-10 Batch 1:  loss: 1.66086 accuracy: 0.5518\n",
      "Epoch 516, CIFAR-10 Batch 2:  loss: 1.75591 accuracy: 0.5456\n",
      "Epoch 516, CIFAR-10 Batch 3:  loss: 1.75248 accuracy: 0.5498\n",
      "Epoch 516, CIFAR-10 Batch 4:  loss: 1.63871 accuracy: 0.5466\n",
      "Epoch 516, CIFAR-10 Batch 5:  loss: 1.67189 accuracy: 0.5396\n",
      "Epoch 517, CIFAR-10 Batch 1:  loss: 1.67437 accuracy: 0.5506\n",
      "Epoch 517, CIFAR-10 Batch 2:  loss: 1.74607 accuracy: 0.5446\n",
      "Epoch 517, CIFAR-10 Batch 3:  loss: 1.77768 accuracy: 0.5494\n",
      "Epoch 517, CIFAR-10 Batch 4:  loss: 1.62026 accuracy: 0.548\n",
      "Epoch 517, CIFAR-10 Batch 5:  loss: 1.68146 accuracy: 0.5388\n",
      "Epoch 518, CIFAR-10 Batch 1:  loss: 1.67881 accuracy: 0.5512\n",
      "Epoch 518, CIFAR-10 Batch 2:  loss: 1.74696 accuracy: 0.5412\n",
      "Epoch 518, CIFAR-10 Batch 3:  loss: 1.74763 accuracy: 0.5512\n",
      "Epoch 518, CIFAR-10 Batch 4:  loss: 1.62849 accuracy: 0.5454\n",
      "Epoch 518, CIFAR-10 Batch 5:  loss: 1.67357 accuracy: 0.5396\n",
      "Epoch 519, CIFAR-10 Batch 1:  loss: 1.68598 accuracy: 0.5478\n",
      "Epoch 519, CIFAR-10 Batch 2:  loss: 1.70284 accuracy: 0.5466\n",
      "Epoch 519, CIFAR-10 Batch 3:  loss: 1.70783 accuracy: 0.5508\n",
      "Epoch 519, CIFAR-10 Batch 4:  loss: 1.63479 accuracy: 0.5484\n",
      "Epoch 519, CIFAR-10 Batch 5:  loss: 1.66332 accuracy: 0.5396\n",
      "Epoch 520, CIFAR-10 Batch 1:  loss: 1.70454 accuracy: 0.5506\n",
      "Epoch 520, CIFAR-10 Batch 2:  loss: 1.72857 accuracy: 0.545\n",
      "Epoch 520, CIFAR-10 Batch 3:  loss: 1.74994 accuracy: 0.5514\n",
      "Epoch 520, CIFAR-10 Batch 4:  loss: 1.62028 accuracy: 0.5488\n",
      "Epoch 520, CIFAR-10 Batch 5:  loss: 1.67234 accuracy: 0.5388\n",
      "Epoch 521, CIFAR-10 Batch 1:  loss: 1.6965 accuracy: 0.5478\n",
      "Epoch 521, CIFAR-10 Batch 2:  loss: 1.74904 accuracy: 0.5446\n",
      "Epoch 521, CIFAR-10 Batch 3:  loss: 1.75683 accuracy: 0.5508\n",
      "Epoch 521, CIFAR-10 Batch 4:  loss: 1.62829 accuracy: 0.5474\n",
      "Epoch 521, CIFAR-10 Batch 5:  loss: 1.67764 accuracy: 0.5384\n",
      "Epoch 522, CIFAR-10 Batch 1:  loss: 1.68953 accuracy: 0.5478\n",
      "Epoch 522, CIFAR-10 Batch 2:  loss: 1.75498 accuracy: 0.5476\n",
      "Epoch 522, CIFAR-10 Batch 3:  loss: 1.73169 accuracy: 0.5502\n",
      "Epoch 522, CIFAR-10 Batch 4:  loss: 1.61884 accuracy: 0.5466\n",
      "Epoch 522, CIFAR-10 Batch 5:  loss: 1.66 accuracy: 0.5408\n",
      "Epoch 523, CIFAR-10 Batch 1:  loss: 1.70299 accuracy: 0.548\n",
      "Epoch 523, CIFAR-10 Batch 2:  loss: 1.74776 accuracy: 0.5432\n",
      "Epoch 523, CIFAR-10 Batch 3:  loss: 1.75882 accuracy: 0.5504\n",
      "Epoch 523, CIFAR-10 Batch 4:  loss: 1.60674 accuracy: 0.5472\n",
      "Epoch 523, CIFAR-10 Batch 5:  loss: 1.68124 accuracy: 0.5414\n",
      "Epoch 524, CIFAR-10 Batch 1:  loss: 1.69089 accuracy: 0.5498\n",
      "Epoch 524, CIFAR-10 Batch 2:  loss: 1.72486 accuracy: 0.5426\n",
      "Epoch 524, CIFAR-10 Batch 3:  loss: 1.75592 accuracy: 0.5496\n",
      "Epoch 524, CIFAR-10 Batch 4:  loss: 1.6041 accuracy: 0.547\n",
      "Epoch 524, CIFAR-10 Batch 5:  loss: 1.68722 accuracy: 0.539\n",
      "Epoch 525, CIFAR-10 Batch 1:  loss: 1.65928 accuracy: 0.5508\n",
      "Epoch 525, CIFAR-10 Batch 2:  loss: 1.716 accuracy: 0.5448\n",
      "Epoch 525, CIFAR-10 Batch 3:  loss: 1.7489 accuracy: 0.5498\n",
      "Epoch 525, CIFAR-10 Batch 4:  loss: 1.62643 accuracy: 0.5476\n",
      "Epoch 525, CIFAR-10 Batch 5:  loss: 1.6957 accuracy: 0.5392\n",
      "Epoch 526, CIFAR-10 Batch 1:  loss: 1.68047 accuracy: 0.5498\n",
      "Epoch 526, CIFAR-10 Batch 2:  loss: 1.75148 accuracy: 0.5438\n",
      "Epoch 526, CIFAR-10 Batch 3:  loss: 1.74993 accuracy: 0.5522\n",
      "Epoch 526, CIFAR-10 Batch 4:  loss: 1.63893 accuracy: 0.5472\n",
      "Epoch 526, CIFAR-10 Batch 5:  loss: 1.66564 accuracy: 0.5404\n",
      "Epoch 527, CIFAR-10 Batch 1:  loss: 1.69067 accuracy: 0.5472\n",
      "Epoch 527, CIFAR-10 Batch 2:  loss: 1.74136 accuracy: 0.5462\n",
      "Epoch 527, CIFAR-10 Batch 3:  loss: 1.74857 accuracy: 0.5494\n",
      "Epoch 527, CIFAR-10 Batch 4:  loss: 1.64228 accuracy: 0.5474\n",
      "Epoch 527, CIFAR-10 Batch 5:  loss: 1.68292 accuracy: 0.5386\n",
      "Epoch 528, CIFAR-10 Batch 1:  loss: 1.70427 accuracy: 0.5478\n",
      "Epoch 528, CIFAR-10 Batch 2:  loss: 1.75727 accuracy: 0.5452\n",
      "Epoch 528, CIFAR-10 Batch 3:  loss: 1.73831 accuracy: 0.55\n",
      "Epoch 528, CIFAR-10 Batch 4:  loss: 1.6497 accuracy: 0.5484\n",
      "Epoch 528, CIFAR-10 Batch 5:  loss: 1.69061 accuracy: 0.5408\n",
      "Epoch 529, CIFAR-10 Batch 1:  loss: 1.69188 accuracy: 0.5486\n",
      "Epoch 529, CIFAR-10 Batch 2:  loss: 1.78232 accuracy: 0.5436\n",
      "Epoch 529, CIFAR-10 Batch 3:  loss: 1.72803 accuracy: 0.5504\n",
      "Epoch 529, CIFAR-10 Batch 4:  loss: 1.63617 accuracy: 0.5468\n",
      "Epoch 529, CIFAR-10 Batch 5:  loss: 1.70046 accuracy: 0.5394\n",
      "Epoch 530, CIFAR-10 Batch 1:  loss: 1.69708 accuracy: 0.551\n",
      "Epoch 530, CIFAR-10 Batch 2:  loss: 1.76496 accuracy: 0.547\n",
      "Epoch 530, CIFAR-10 Batch 3:  loss: 1.70698 accuracy: 0.551\n",
      "Epoch 530, CIFAR-10 Batch 4:  loss: 1.65765 accuracy: 0.5482\n",
      "Epoch 530, CIFAR-10 Batch 5:  loss: 1.69987 accuracy: 0.5406\n",
      "Epoch 531, CIFAR-10 Batch 1:  loss: 1.70825 accuracy: 0.5476\n",
      "Epoch 531, CIFAR-10 Batch 2:  loss: 1.7676 accuracy: 0.548\n",
      "Epoch 531, CIFAR-10 Batch 3:  loss: 1.71013 accuracy: 0.5516\n",
      "Epoch 531, CIFAR-10 Batch 4:  loss: 1.67025 accuracy: 0.5478\n",
      "Epoch 531, CIFAR-10 Batch 5:  loss: 1.67628 accuracy: 0.5414\n",
      "Epoch 532, CIFAR-10 Batch 1:  loss: 1.69886 accuracy: 0.5476\n",
      "Epoch 532, CIFAR-10 Batch 2:  loss: 1.79785 accuracy: 0.5456\n",
      "Epoch 532, CIFAR-10 Batch 3:  loss: 1.71391 accuracy: 0.552\n",
      "Epoch 532, CIFAR-10 Batch 4:  loss: 1.66911 accuracy: 0.5458\n",
      "Epoch 532, CIFAR-10 Batch 5:  loss: 1.67988 accuracy: 0.5408\n",
      "Epoch 533, CIFAR-10 Batch 1:  loss: 1.69553 accuracy: 0.5486\n",
      "Epoch 533, CIFAR-10 Batch 2:  loss: 1.78148 accuracy: 0.5454\n",
      "Epoch 533, CIFAR-10 Batch 3:  loss: 1.70393 accuracy: 0.5472\n",
      "Epoch 533, CIFAR-10 Batch 4:  loss: 1.66348 accuracy: 0.548\n",
      "Epoch 533, CIFAR-10 Batch 5:  loss: 1.67178 accuracy: 0.5422\n",
      "Epoch 534, CIFAR-10 Batch 1:  loss: 1.68534 accuracy: 0.5478\n",
      "Epoch 534, CIFAR-10 Batch 2:  loss: 1.75622 accuracy: 0.5476\n",
      "Epoch 534, CIFAR-10 Batch 3:  loss: 1.72759 accuracy: 0.5494\n",
      "Epoch 534, CIFAR-10 Batch 4:  loss: 1.64084 accuracy: 0.547\n",
      "Epoch 534, CIFAR-10 Batch 5:  loss: 1.67877 accuracy: 0.5448\n",
      "Epoch 535, CIFAR-10 Batch 1:  loss: 1.68878 accuracy: 0.549\n",
      "Epoch 535, CIFAR-10 Batch 2:  loss: 1.73164 accuracy: 0.5484\n",
      "Epoch 535, CIFAR-10 Batch 3:  loss: 1.70992 accuracy: 0.5498\n",
      "Epoch 535, CIFAR-10 Batch 4:  loss: 1.67742 accuracy: 0.5472\n",
      "Epoch 535, CIFAR-10 Batch 5:  loss: 1.70182 accuracy: 0.5418\n",
      "Epoch 536, CIFAR-10 Batch 1:  loss: 1.70564 accuracy: 0.55\n",
      "Epoch 536, CIFAR-10 Batch 2:  loss: 1.77981 accuracy: 0.5448\n",
      "Epoch 536, CIFAR-10 Batch 3:  loss: 1.73146 accuracy: 0.5486\n",
      "Epoch 536, CIFAR-10 Batch 4:  loss: 1.66083 accuracy: 0.5466\n",
      "Epoch 536, CIFAR-10 Batch 5:  loss: 1.67547 accuracy: 0.5422\n",
      "Epoch 537, CIFAR-10 Batch 1:  loss: 1.69655 accuracy: 0.5518\n",
      "Epoch 537, CIFAR-10 Batch 2:  loss: 1.80268 accuracy: 0.5442\n",
      "Epoch 537, CIFAR-10 Batch 3:  loss: 1.67237 accuracy: 0.551\n",
      "Epoch 537, CIFAR-10 Batch 4:  loss: 1.66918 accuracy: 0.5456\n",
      "Epoch 537, CIFAR-10 Batch 5:  loss: 1.67678 accuracy: 0.54\n",
      "Epoch 538, CIFAR-10 Batch 1:  loss: 1.69662 accuracy: 0.5482\n",
      "Epoch 538, CIFAR-10 Batch 2:  loss: 1.74029 accuracy: 0.5446\n",
      "Epoch 538, CIFAR-10 Batch 3:  loss: 1.719 accuracy: 0.5498\n",
      "Epoch 538, CIFAR-10 Batch 4:  loss: 1.67423 accuracy: 0.5448\n",
      "Epoch 538, CIFAR-10 Batch 5:  loss: 1.6839 accuracy: 0.5432\n",
      "Epoch 539, CIFAR-10 Batch 1:  loss: 1.67527 accuracy: 0.5504\n",
      "Epoch 539, CIFAR-10 Batch 2:  loss: 1.75389 accuracy: 0.5488\n",
      "Epoch 539, CIFAR-10 Batch 3:  loss: 1.72233 accuracy: 0.5512\n",
      "Epoch 539, CIFAR-10 Batch 4:  loss: 1.6371 accuracy: 0.5468\n",
      "Epoch 539, CIFAR-10 Batch 5:  loss: 1.6765 accuracy: 0.5426\n",
      "Epoch 540, CIFAR-10 Batch 1:  loss: 1.6983 accuracy: 0.5518\n",
      "Epoch 540, CIFAR-10 Batch 2:  loss: 1.79338 accuracy: 0.546\n",
      "Epoch 540, CIFAR-10 Batch 3:  loss: 1.71365 accuracy: 0.5504\n",
      "Epoch 540, CIFAR-10 Batch 4:  loss: 1.6537 accuracy: 0.547\n",
      "Epoch 540, CIFAR-10 Batch 5:  loss: 1.68549 accuracy: 0.5414\n",
      "Epoch 541, CIFAR-10 Batch 1:  loss: 1.71093 accuracy: 0.5486\n",
      "Epoch 541, CIFAR-10 Batch 2:  loss: 1.75651 accuracy: 0.5464\n",
      "Epoch 541, CIFAR-10 Batch 3:  loss: 1.72454 accuracy: 0.5484\n",
      "Epoch 541, CIFAR-10 Batch 4:  loss: 1.65209 accuracy: 0.5472\n",
      "Epoch 541, CIFAR-10 Batch 5:  loss: 1.69014 accuracy: 0.54\n",
      "Epoch 542, CIFAR-10 Batch 1:  loss: 1.72828 accuracy: 0.5486\n",
      "Epoch 542, CIFAR-10 Batch 2:  loss: 1.76597 accuracy: 0.5478\n",
      "Epoch 542, CIFAR-10 Batch 3:  loss: 1.72133 accuracy: 0.5502\n",
      "Epoch 542, CIFAR-10 Batch 4:  loss: 1.68014 accuracy: 0.5474\n",
      "Epoch 542, CIFAR-10 Batch 5:  loss: 1.68686 accuracy: 0.541\n",
      "Epoch 543, CIFAR-10 Batch 1:  loss: 1.72612 accuracy: 0.5482\n",
      "Epoch 543, CIFAR-10 Batch 2:  loss: 1.7947 accuracy: 0.5468\n",
      "Epoch 543, CIFAR-10 Batch 3:  loss: 1.74911 accuracy: 0.5484\n",
      "Epoch 543, CIFAR-10 Batch 4:  loss: 1.65822 accuracy: 0.5446\n",
      "Epoch 543, CIFAR-10 Batch 5:  loss: 1.69829 accuracy: 0.543\n",
      "Epoch 544, CIFAR-10 Batch 1:  loss: 1.72211 accuracy: 0.5488\n",
      "Epoch 544, CIFAR-10 Batch 2:  loss: 1.79248 accuracy: 0.5444\n",
      "Epoch 544, CIFAR-10 Batch 3:  loss: 1.72112 accuracy: 0.55\n",
      "Epoch 544, CIFAR-10 Batch 4:  loss: 1.67371 accuracy: 0.5456\n",
      "Epoch 544, CIFAR-10 Batch 5:  loss: 1.69239 accuracy: 0.5436\n",
      "Epoch 545, CIFAR-10 Batch 1:  loss: 1.73888 accuracy: 0.5464\n",
      "Epoch 545, CIFAR-10 Batch 2:  loss: 1.80852 accuracy: 0.5444\n",
      "Epoch 545, CIFAR-10 Batch 3:  loss: 1.71692 accuracy: 0.5512\n",
      "Epoch 545, CIFAR-10 Batch 4:  loss: 1.6822 accuracy: 0.5454\n",
      "Epoch 545, CIFAR-10 Batch 5:  loss: 1.69439 accuracy: 0.5424\n",
      "Epoch 546, CIFAR-10 Batch 1:  loss: 1.71134 accuracy: 0.5472\n",
      "Epoch 546, CIFAR-10 Batch 2:  loss: 1.80339 accuracy: 0.5458\n",
      "Epoch 546, CIFAR-10 Batch 3:  loss: 1.70545 accuracy: 0.5482\n",
      "Epoch 546, CIFAR-10 Batch 4:  loss: 1.68587 accuracy: 0.547\n",
      "Epoch 546, CIFAR-10 Batch 5:  loss: 1.69285 accuracy: 0.5422\n",
      "Epoch 547, CIFAR-10 Batch 1:  loss: 1.71382 accuracy: 0.5458\n",
      "Epoch 547, CIFAR-10 Batch 2:  loss: 1.80697 accuracy: 0.5462\n",
      "Epoch 547, CIFAR-10 Batch 3:  loss: 1.71431 accuracy: 0.5496\n",
      "Epoch 547, CIFAR-10 Batch 4:  loss: 1.69339 accuracy: 0.5458\n",
      "Epoch 547, CIFAR-10 Batch 5:  loss: 1.70846 accuracy: 0.5436\n",
      "Epoch 548, CIFAR-10 Batch 1:  loss: 1.7205 accuracy: 0.5478\n",
      "Epoch 548, CIFAR-10 Batch 2:  loss: 1.80177 accuracy: 0.5476\n",
      "Epoch 548, CIFAR-10 Batch 3:  loss: 1.70946 accuracy: 0.5498\n",
      "Epoch 548, CIFAR-10 Batch 4:  loss: 1.66524 accuracy: 0.5458\n",
      "Epoch 548, CIFAR-10 Batch 5:  loss: 1.71788 accuracy: 0.5394\n",
      "Epoch 549, CIFAR-10 Batch 1:  loss: 1.68728 accuracy: 0.5474\n",
      "Epoch 549, CIFAR-10 Batch 2:  loss: 1.74501 accuracy: 0.546\n",
      "Epoch 549, CIFAR-10 Batch 3:  loss: 1.72032 accuracy: 0.5492\n",
      "Epoch 549, CIFAR-10 Batch 4:  loss: 1.68723 accuracy: 0.5434\n",
      "Epoch 549, CIFAR-10 Batch 5:  loss: 1.72555 accuracy: 0.541\n",
      "Epoch 550, CIFAR-10 Batch 1:  loss: 1.71712 accuracy: 0.5488\n",
      "Epoch 550, CIFAR-10 Batch 2:  loss: 1.79672 accuracy: 0.5468\n",
      "Epoch 550, CIFAR-10 Batch 3:  loss: 1.69544 accuracy: 0.5506\n",
      "Epoch 550, CIFAR-10 Batch 4:  loss: 1.6962 accuracy: 0.5444\n",
      "Epoch 550, CIFAR-10 Batch 5:  loss: 1.7094 accuracy: 0.5426\n",
      "Epoch 551, CIFAR-10 Batch 1:  loss: 1.71727 accuracy: 0.547\n",
      "Epoch 551, CIFAR-10 Batch 2:  loss: 1.80314 accuracy: 0.5452\n",
      "Epoch 551, CIFAR-10 Batch 3:  loss: 1.70057 accuracy: 0.5488\n",
      "Epoch 551, CIFAR-10 Batch 4:  loss: 1.69944 accuracy: 0.5452\n",
      "Epoch 551, CIFAR-10 Batch 5:  loss: 1.73294 accuracy: 0.539\n",
      "Epoch 552, CIFAR-10 Batch 1:  loss: 1.74473 accuracy: 0.5462\n",
      "Epoch 552, CIFAR-10 Batch 2:  loss: 1.80032 accuracy: 0.5462\n",
      "Epoch 552, CIFAR-10 Batch 3:  loss: 1.72177 accuracy: 0.552\n",
      "Epoch 552, CIFAR-10 Batch 4:  loss: 1.70114 accuracy: 0.5462\n",
      "Epoch 552, CIFAR-10 Batch 5:  loss: 1.7138 accuracy: 0.543\n",
      "Epoch 553, CIFAR-10 Batch 1:  loss: 1.73365 accuracy: 0.5478\n",
      "Epoch 553, CIFAR-10 Batch 2:  loss: 1.79695 accuracy: 0.5444\n",
      "Epoch 553, CIFAR-10 Batch 3:  loss: 1.71778 accuracy: 0.5512\n",
      "Epoch 553, CIFAR-10 Batch 4:  loss: 1.68845 accuracy: 0.5458\n",
      "Epoch 553, CIFAR-10 Batch 5:  loss: 1.72723 accuracy: 0.5408\n",
      "Epoch 554, CIFAR-10 Batch 1:  loss: 1.71777 accuracy: 0.5492\n",
      "Epoch 554, CIFAR-10 Batch 2:  loss: 1.79627 accuracy: 0.5476\n",
      "Epoch 554, CIFAR-10 Batch 3:  loss: 1.72001 accuracy: 0.5472\n",
      "Epoch 554, CIFAR-10 Batch 4:  loss: 1.69945 accuracy: 0.5478\n",
      "Epoch 554, CIFAR-10 Batch 5:  loss: 1.7059 accuracy: 0.5424\n",
      "Epoch 555, CIFAR-10 Batch 1:  loss: 1.72041 accuracy: 0.5496\n",
      "Epoch 555, CIFAR-10 Batch 2:  loss: 1.79203 accuracy: 0.5462\n",
      "Epoch 555, CIFAR-10 Batch 3:  loss: 1.74732 accuracy: 0.547\n",
      "Epoch 555, CIFAR-10 Batch 4:  loss: 1.68929 accuracy: 0.5488\n",
      "Epoch 555, CIFAR-10 Batch 5:  loss: 1.72274 accuracy: 0.5414\n",
      "Epoch 556, CIFAR-10 Batch 1:  loss: 1.73778 accuracy: 0.5488\n",
      "Epoch 556, CIFAR-10 Batch 2:  loss: 1.78485 accuracy: 0.5458\n",
      "Epoch 556, CIFAR-10 Batch 3:  loss: 1.73791 accuracy: 0.548\n",
      "Epoch 556, CIFAR-10 Batch 4:  loss: 1.69605 accuracy: 0.547\n",
      "Epoch 556, CIFAR-10 Batch 5:  loss: 1.71143 accuracy: 0.5418\n",
      "Epoch 557, CIFAR-10 Batch 1:  loss: 1.73328 accuracy: 0.5502\n",
      "Epoch 557, CIFAR-10 Batch 2:  loss: 1.76313 accuracy: 0.5452\n",
      "Epoch 557, CIFAR-10 Batch 3:  loss: 1.74232 accuracy: 0.549\n",
      "Epoch 557, CIFAR-10 Batch 4:  loss: 1.71673 accuracy: 0.5464\n",
      "Epoch 557, CIFAR-10 Batch 5:  loss: 1.702 accuracy: 0.5432\n",
      "Epoch 558, CIFAR-10 Batch 1:  loss: 1.75132 accuracy: 0.5478\n",
      "Epoch 558, CIFAR-10 Batch 2:  loss: 1.79255 accuracy: 0.5442\n",
      "Epoch 558, CIFAR-10 Batch 3:  loss: 1.72186 accuracy: 0.5504\n",
      "Epoch 558, CIFAR-10 Batch 4:  loss: 1.69219 accuracy: 0.5476\n",
      "Epoch 558, CIFAR-10 Batch 5:  loss: 1.7104 accuracy: 0.5412\n",
      "Epoch 559, CIFAR-10 Batch 1:  loss: 1.73014 accuracy: 0.5474\n",
      "Epoch 559, CIFAR-10 Batch 2:  loss: 1.75927 accuracy: 0.5466\n",
      "Epoch 559, CIFAR-10 Batch 3:  loss: 1.71385 accuracy: 0.5496\n",
      "Epoch 559, CIFAR-10 Batch 4:  loss: 1.69656 accuracy: 0.549\n",
      "Epoch 559, CIFAR-10 Batch 5:  loss: 1.73042 accuracy: 0.5402\n",
      "Epoch 560, CIFAR-10 Batch 1:  loss: 1.75445 accuracy: 0.5468\n",
      "Epoch 560, CIFAR-10 Batch 2:  loss: 1.78749 accuracy: 0.5452\n",
      "Epoch 560, CIFAR-10 Batch 3:  loss: 1.71999 accuracy: 0.5506\n",
      "Epoch 560, CIFAR-10 Batch 4:  loss: 1.70117 accuracy: 0.5482\n",
      "Epoch 560, CIFAR-10 Batch 5:  loss: 1.7391 accuracy: 0.542\n",
      "Epoch 561, CIFAR-10 Batch 1:  loss: 1.72726 accuracy: 0.5498\n",
      "Epoch 561, CIFAR-10 Batch 2:  loss: 1.76332 accuracy: 0.5454\n",
      "Epoch 561, CIFAR-10 Batch 3:  loss: 1.72646 accuracy: 0.55\n",
      "Epoch 561, CIFAR-10 Batch 4:  loss: 1.70168 accuracy: 0.547\n",
      "Epoch 561, CIFAR-10 Batch 5:  loss: 1.72005 accuracy: 0.5414\n",
      "Epoch 562, CIFAR-10 Batch 1:  loss: 1.73326 accuracy: 0.5496\n",
      "Epoch 562, CIFAR-10 Batch 2:  loss: 1.79066 accuracy: 0.546\n",
      "Epoch 562, CIFAR-10 Batch 3:  loss: 1.75541 accuracy: 0.5488\n",
      "Epoch 562, CIFAR-10 Batch 4:  loss: 1.71805 accuracy: 0.5492\n",
      "Epoch 562, CIFAR-10 Batch 5:  loss: 1.73789 accuracy: 0.5398\n",
      "Epoch 563, CIFAR-10 Batch 1:  loss: 1.74196 accuracy: 0.55\n",
      "Epoch 563, CIFAR-10 Batch 2:  loss: 1.80127 accuracy: 0.5426\n",
      "Epoch 563, CIFAR-10 Batch 3:  loss: 1.77614 accuracy: 0.551\n",
      "Epoch 563, CIFAR-10 Batch 4:  loss: 1.7024 accuracy: 0.5484\n",
      "Epoch 563, CIFAR-10 Batch 5:  loss: 1.75836 accuracy: 0.54\n",
      "Epoch 564, CIFAR-10 Batch 1:  loss: 1.73356 accuracy: 0.5504\n",
      "Epoch 564, CIFAR-10 Batch 2:  loss: 1.77312 accuracy: 0.5452\n",
      "Epoch 564, CIFAR-10 Batch 3:  loss: 1.75204 accuracy: 0.5478\n",
      "Epoch 564, CIFAR-10 Batch 4:  loss: 1.71916 accuracy: 0.5486\n",
      "Epoch 564, CIFAR-10 Batch 5:  loss: 1.75229 accuracy: 0.5418\n",
      "Epoch 565, CIFAR-10 Batch 1:  loss: 1.74159 accuracy: 0.5498\n",
      "Epoch 565, CIFAR-10 Batch 2:  loss: 1.80148 accuracy: 0.5444\n",
      "Epoch 565, CIFAR-10 Batch 3:  loss: 1.78075 accuracy: 0.549\n",
      "Epoch 565, CIFAR-10 Batch 4:  loss: 1.70548 accuracy: 0.5472\n",
      "Epoch 565, CIFAR-10 Batch 5:  loss: 1.75311 accuracy: 0.5398\n",
      "Epoch 566, CIFAR-10 Batch 1:  loss: 1.75228 accuracy: 0.551\n",
      "Epoch 566, CIFAR-10 Batch 2:  loss: 1.76784 accuracy: 0.5464\n",
      "Epoch 566, CIFAR-10 Batch 3:  loss: 1.78345 accuracy: 0.549\n",
      "Epoch 566, CIFAR-10 Batch 4:  loss: 1.71836 accuracy: 0.5472\n",
      "Epoch 566, CIFAR-10 Batch 5:  loss: 1.75851 accuracy: 0.5438\n",
      "Epoch 567, CIFAR-10 Batch 1:  loss: 1.75984 accuracy: 0.5502\n",
      "Epoch 567, CIFAR-10 Batch 2:  loss: 1.78541 accuracy: 0.544\n",
      "Epoch 567, CIFAR-10 Batch 3:  loss: 1.76032 accuracy: 0.5482\n",
      "Epoch 567, CIFAR-10 Batch 4:  loss: 1.73382 accuracy: 0.5498\n",
      "Epoch 567, CIFAR-10 Batch 5:  loss: 1.77133 accuracy: 0.541\n",
      "Epoch 568, CIFAR-10 Batch 1:  loss: 1.76333 accuracy: 0.5486\n",
      "Epoch 568, CIFAR-10 Batch 2:  loss: 1.80152 accuracy: 0.5452\n",
      "Epoch 568, CIFAR-10 Batch 3:  loss: 1.75988 accuracy: 0.5508\n",
      "Epoch 568, CIFAR-10 Batch 4:  loss: 1.72435 accuracy: 0.549\n",
      "Epoch 568, CIFAR-10 Batch 5:  loss: 1.76266 accuracy: 0.5406\n",
      "Epoch 569, CIFAR-10 Batch 1:  loss: 1.76406 accuracy: 0.5488\n",
      "Epoch 569, CIFAR-10 Batch 2:  loss: 1.78597 accuracy: 0.5466\n",
      "Epoch 569, CIFAR-10 Batch 3:  loss: 1.78111 accuracy: 0.5476\n",
      "Epoch 569, CIFAR-10 Batch 4:  loss: 1.71478 accuracy: 0.55\n",
      "Epoch 569, CIFAR-10 Batch 5:  loss: 1.75931 accuracy: 0.5422\n",
      "Epoch 570, CIFAR-10 Batch 1:  loss: 1.77713 accuracy: 0.5478\n",
      "Epoch 570, CIFAR-10 Batch 2:  loss: 1.79915 accuracy: 0.5448\n",
      "Epoch 570, CIFAR-10 Batch 3:  loss: 1.75903 accuracy: 0.5512\n",
      "Epoch 570, CIFAR-10 Batch 4:  loss: 1.70948 accuracy: 0.5498\n",
      "Epoch 570, CIFAR-10 Batch 5:  loss: 1.75009 accuracy: 0.5426\n",
      "Epoch 571, CIFAR-10 Batch 1:  loss: 1.76552 accuracy: 0.5476\n",
      "Epoch 571, CIFAR-10 Batch 2:  loss: 1.78445 accuracy: 0.5466\n",
      "Epoch 571, CIFAR-10 Batch 3:  loss: 1.77436 accuracy: 0.5498\n",
      "Epoch 571, CIFAR-10 Batch 4:  loss: 1.71554 accuracy: 0.5522\n",
      "Epoch 571, CIFAR-10 Batch 5:  loss: 1.74511 accuracy: 0.5424\n",
      "Epoch 572, CIFAR-10 Batch 1:  loss: 1.77066 accuracy: 0.5478\n",
      "Epoch 572, CIFAR-10 Batch 2:  loss: 1.78714 accuracy: 0.5474\n",
      "Epoch 572, CIFAR-10 Batch 3:  loss: 1.78409 accuracy: 0.552\n",
      "Epoch 572, CIFAR-10 Batch 4:  loss: 1.70585 accuracy: 0.5496\n",
      "Epoch 572, CIFAR-10 Batch 5:  loss: 1.74427 accuracy: 0.5428\n",
      "Epoch 573, CIFAR-10 Batch 1:  loss: 1.73618 accuracy: 0.5472\n",
      "Epoch 573, CIFAR-10 Batch 2:  loss: 1.79151 accuracy: 0.5478\n",
      "Epoch 573, CIFAR-10 Batch 3:  loss: 1.78536 accuracy: 0.5512\n",
      "Epoch 573, CIFAR-10 Batch 4:  loss: 1.70821 accuracy: 0.5494\n",
      "Epoch 573, CIFAR-10 Batch 5:  loss: 1.75232 accuracy: 0.5408\n",
      "Epoch 574, CIFAR-10 Batch 1:  loss: 1.78531 accuracy: 0.5476\n",
      "Epoch 574, CIFAR-10 Batch 2:  loss: 1.82013 accuracy: 0.5466\n",
      "Epoch 574, CIFAR-10 Batch 3:  loss: 1.78715 accuracy: 0.5486\n",
      "Epoch 574, CIFAR-10 Batch 4:  loss: 1.7393 accuracy: 0.5486\n",
      "Epoch 574, CIFAR-10 Batch 5:  loss: 1.75106 accuracy: 0.5386\n",
      "Epoch 575, CIFAR-10 Batch 1:  loss: 1.80382 accuracy: 0.5462\n",
      "Epoch 575, CIFAR-10 Batch 2:  loss: 1.796 accuracy: 0.5466\n",
      "Epoch 575, CIFAR-10 Batch 3:  loss: 1.80578 accuracy: 0.5492\n",
      "Epoch 575, CIFAR-10 Batch 4:  loss: 1.71108 accuracy: 0.5524\n",
      "Epoch 575, CIFAR-10 Batch 5:  loss: 1.76001 accuracy: 0.5414\n",
      "Epoch 576, CIFAR-10 Batch 1:  loss: 1.76225 accuracy: 0.5484\n",
      "Epoch 576, CIFAR-10 Batch 2:  loss: 1.80852 accuracy: 0.5444\n",
      "Epoch 576, CIFAR-10 Batch 3:  loss: 1.80717 accuracy: 0.5492\n",
      "Epoch 576, CIFAR-10 Batch 4:  loss: 1.71849 accuracy: 0.5488\n",
      "Epoch 576, CIFAR-10 Batch 5:  loss: 1.7674 accuracy: 0.5418\n",
      "Epoch 577, CIFAR-10 Batch 1:  loss: 1.773 accuracy: 0.548\n",
      "Epoch 577, CIFAR-10 Batch 2:  loss: 1.81052 accuracy: 0.5456\n",
      "Epoch 577, CIFAR-10 Batch 3:  loss: 1.82851 accuracy: 0.5494\n",
      "Epoch 577, CIFAR-10 Batch 4:  loss: 1.72045 accuracy: 0.5484\n",
      "Epoch 577, CIFAR-10 Batch 5:  loss: 1.75843 accuracy: 0.54\n",
      "Epoch 578, CIFAR-10 Batch 1:  loss: 1.78073 accuracy: 0.546\n",
      "Epoch 578, CIFAR-10 Batch 2:  loss: 1.80303 accuracy: 0.546\n",
      "Epoch 578, CIFAR-10 Batch 3:  loss: 1.83206 accuracy: 0.5486\n",
      "Epoch 578, CIFAR-10 Batch 4:  loss: 1.73217 accuracy: 0.5486\n",
      "Epoch 578, CIFAR-10 Batch 5:  loss: 1.75563 accuracy: 0.5402\n",
      "Epoch 579, CIFAR-10 Batch 1:  loss: 1.76274 accuracy: 0.5472\n",
      "Epoch 579, CIFAR-10 Batch 2:  loss: 1.81755 accuracy: 0.547\n",
      "Epoch 579, CIFAR-10 Batch 3:  loss: 1.85478 accuracy: 0.5502\n",
      "Epoch 579, CIFAR-10 Batch 4:  loss: 1.72321 accuracy: 0.5494\n",
      "Epoch 579, CIFAR-10 Batch 5:  loss: 1.76315 accuracy: 0.5402\n",
      "Epoch 580, CIFAR-10 Batch 1:  loss: 1.77652 accuracy: 0.5484\n",
      "Epoch 580, CIFAR-10 Batch 2:  loss: 1.81179 accuracy: 0.5468\n",
      "Epoch 580, CIFAR-10 Batch 3:  loss: 1.83389 accuracy: 0.5498\n",
      "Epoch 580, CIFAR-10 Batch 4:  loss: 1.69993 accuracy: 0.549\n",
      "Epoch 580, CIFAR-10 Batch 5:  loss: 1.7802 accuracy: 0.5406\n",
      "Epoch 581, CIFAR-10 Batch 1:  loss: 1.78588 accuracy: 0.5466\n",
      "Epoch 581, CIFAR-10 Batch 2:  loss: 1.81706 accuracy: 0.545\n",
      "Epoch 581, CIFAR-10 Batch 3:  loss: 1.83591 accuracy: 0.5486\n",
      "Epoch 581, CIFAR-10 Batch 4:  loss: 1.7181 accuracy: 0.5472\n",
      "Epoch 581, CIFAR-10 Batch 5:  loss: 1.75858 accuracy: 0.5404\n",
      "Epoch 582, CIFAR-10 Batch 1:  loss: 1.76343 accuracy: 0.5482\n",
      "Epoch 582, CIFAR-10 Batch 2:  loss: 1.80697 accuracy: 0.5456\n",
      "Epoch 582, CIFAR-10 Batch 3:  loss: 1.8353 accuracy: 0.5496\n",
      "Epoch 582, CIFAR-10 Batch 4:  loss: 1.72468 accuracy: 0.5474\n",
      "Epoch 582, CIFAR-10 Batch 5:  loss: 1.75301 accuracy: 0.541\n",
      "Epoch 583, CIFAR-10 Batch 1:  loss: 1.7947 accuracy: 0.5474\n",
      "Epoch 583, CIFAR-10 Batch 2:  loss: 1.80397 accuracy: 0.546\n",
      "Epoch 583, CIFAR-10 Batch 3:  loss: 1.83444 accuracy: 0.551\n",
      "Epoch 583, CIFAR-10 Batch 4:  loss: 1.71841 accuracy: 0.549\n",
      "Epoch 583, CIFAR-10 Batch 5:  loss: 1.77423 accuracy: 0.5432\n",
      "Epoch 584, CIFAR-10 Batch 1:  loss: 1.80221 accuracy: 0.5468\n",
      "Epoch 584, CIFAR-10 Batch 2:  loss: 1.79673 accuracy: 0.545\n",
      "Epoch 584, CIFAR-10 Batch 3:  loss: 1.83566 accuracy: 0.5482\n",
      "Epoch 584, CIFAR-10 Batch 4:  loss: 1.75701 accuracy: 0.5476\n",
      "Epoch 584, CIFAR-10 Batch 5:  loss: 1.74427 accuracy: 0.5422\n",
      "Epoch 585, CIFAR-10 Batch 1:  loss: 1.81339 accuracy: 0.5472\n",
      "Epoch 585, CIFAR-10 Batch 2:  loss: 1.80423 accuracy: 0.546\n",
      "Epoch 585, CIFAR-10 Batch 3:  loss: 1.88502 accuracy: 0.5482\n",
      "Epoch 585, CIFAR-10 Batch 4:  loss: 1.71072 accuracy: 0.5464\n",
      "Epoch 585, CIFAR-10 Batch 5:  loss: 1.75199 accuracy: 0.542\n",
      "Epoch 586, CIFAR-10 Batch 1:  loss: 1.81179 accuracy: 0.5466\n",
      "Epoch 586, CIFAR-10 Batch 2:  loss: 1.80295 accuracy: 0.5424\n",
      "Epoch 586, CIFAR-10 Batch 3:  loss: 1.85477 accuracy: 0.5494\n",
      "Epoch 586, CIFAR-10 Batch 4:  loss: 1.73212 accuracy: 0.549\n",
      "Epoch 586, CIFAR-10 Batch 5:  loss: 1.75774 accuracy: 0.5392\n",
      "Epoch 587, CIFAR-10 Batch 1:  loss: 1.81046 accuracy: 0.5484\n",
      "Epoch 587, CIFAR-10 Batch 2:  loss: 1.82836 accuracy: 0.545\n",
      "Epoch 587, CIFAR-10 Batch 3:  loss: 1.8835 accuracy: 0.5466\n",
      "Epoch 587, CIFAR-10 Batch 4:  loss: 1.72672 accuracy: 0.5498\n",
      "Epoch 587, CIFAR-10 Batch 5:  loss: 1.77386 accuracy: 0.5372\n",
      "Epoch 588, CIFAR-10 Batch 1:  loss: 1.8195 accuracy: 0.5492\n",
      "Epoch 588, CIFAR-10 Batch 2:  loss: 1.79887 accuracy: 0.5438\n",
      "Epoch 588, CIFAR-10 Batch 3:  loss: 1.86661 accuracy: 0.5484\n",
      "Epoch 588, CIFAR-10 Batch 4:  loss: 1.74453 accuracy: 0.547\n",
      "Epoch 588, CIFAR-10 Batch 5:  loss: 1.75803 accuracy: 0.5412\n",
      "Epoch 589, CIFAR-10 Batch 1:  loss: 1.81286 accuracy: 0.5494\n",
      "Epoch 589, CIFAR-10 Batch 2:  loss: 1.81035 accuracy: 0.5464\n",
      "Epoch 589, CIFAR-10 Batch 3:  loss: 1.86994 accuracy: 0.5498\n",
      "Epoch 589, CIFAR-10 Batch 4:  loss: 1.75783 accuracy: 0.547\n",
      "Epoch 589, CIFAR-10 Batch 5:  loss: 1.76898 accuracy: 0.5412\n",
      "Epoch 590, CIFAR-10 Batch 1:  loss: 1.82027 accuracy: 0.5478\n",
      "Epoch 590, CIFAR-10 Batch 2:  loss: 1.81031 accuracy: 0.5436\n",
      "Epoch 590, CIFAR-10 Batch 3:  loss: 1.89298 accuracy: 0.5484\n",
      "Epoch 590, CIFAR-10 Batch 4:  loss: 1.7334 accuracy: 0.5486\n",
      "Epoch 590, CIFAR-10 Batch 5:  loss: 1.78131 accuracy: 0.542\n",
      "Epoch 591, CIFAR-10 Batch 1:  loss: 1.80393 accuracy: 0.5488\n",
      "Epoch 591, CIFAR-10 Batch 2:  loss: 1.81383 accuracy: 0.5448\n",
      "Epoch 591, CIFAR-10 Batch 3:  loss: 1.89431 accuracy: 0.5468\n",
      "Epoch 591, CIFAR-10 Batch 4:  loss: 1.73796 accuracy: 0.5474\n",
      "Epoch 591, CIFAR-10 Batch 5:  loss: 1.7769 accuracy: 0.5426\n",
      "Epoch 592, CIFAR-10 Batch 1:  loss: 1.79549 accuracy: 0.5496\n",
      "Epoch 592, CIFAR-10 Batch 2:  loss: 1.81615 accuracy: 0.5466\n",
      "Epoch 592, CIFAR-10 Batch 3:  loss: 1.88227 accuracy: 0.5472\n",
      "Epoch 592, CIFAR-10 Batch 4:  loss: 1.76024 accuracy: 0.5484\n",
      "Epoch 592, CIFAR-10 Batch 5:  loss: 1.7578 accuracy: 0.5402\n",
      "Epoch 593, CIFAR-10 Batch 1:  loss: 1.80938 accuracy: 0.5454\n",
      "Epoch 593, CIFAR-10 Batch 2:  loss: 1.78858 accuracy: 0.5438\n",
      "Epoch 593, CIFAR-10 Batch 3:  loss: 1.89204 accuracy: 0.5468\n",
      "Epoch 593, CIFAR-10 Batch 4:  loss: 1.73504 accuracy: 0.5482\n",
      "Epoch 593, CIFAR-10 Batch 5:  loss: 1.78129 accuracy: 0.538\n",
      "Epoch 594, CIFAR-10 Batch 1:  loss: 1.81724 accuracy: 0.5466\n",
      "Epoch 594, CIFAR-10 Batch 2:  loss: 1.8184 accuracy: 0.546\n",
      "Epoch 594, CIFAR-10 Batch 3:  loss: 1.87875 accuracy: 0.546\n",
      "Epoch 594, CIFAR-10 Batch 4:  loss: 1.75922 accuracy: 0.5482\n",
      "Epoch 594, CIFAR-10 Batch 5:  loss: 1.77125 accuracy: 0.5412\n",
      "Epoch 595, CIFAR-10 Batch 1:  loss: 1.82796 accuracy: 0.5478\n",
      "Epoch 595, CIFAR-10 Batch 2:  loss: 1.82496 accuracy: 0.5438\n",
      "Epoch 595, CIFAR-10 Batch 3:  loss: 1.86072 accuracy: 0.548\n",
      "Epoch 595, CIFAR-10 Batch 4:  loss: 1.75239 accuracy: 0.548\n",
      "Epoch 595, CIFAR-10 Batch 5:  loss: 1.7607 accuracy: 0.5422\n",
      "Epoch 596, CIFAR-10 Batch 1:  loss: 1.82928 accuracy: 0.548\n",
      "Epoch 596, CIFAR-10 Batch 2:  loss: 1.81253 accuracy: 0.5442\n",
      "Epoch 596, CIFAR-10 Batch 3:  loss: 1.88188 accuracy: 0.5468\n",
      "Epoch 596, CIFAR-10 Batch 4:  loss: 1.74899 accuracy: 0.5484\n",
      "Epoch 596, CIFAR-10 Batch 5:  loss: 1.78842 accuracy: 0.5422\n",
      "Epoch 597, CIFAR-10 Batch 1:  loss: 1.82605 accuracy: 0.548\n",
      "Epoch 597, CIFAR-10 Batch 2:  loss: 1.8442 accuracy: 0.5434\n",
      "Epoch 597, CIFAR-10 Batch 3:  loss: 1.90501 accuracy: 0.5492\n",
      "Epoch 597, CIFAR-10 Batch 4:  loss: 1.77696 accuracy: 0.547\n",
      "Epoch 597, CIFAR-10 Batch 5:  loss: 1.75378 accuracy: 0.543\n",
      "Epoch 598, CIFAR-10 Batch 1:  loss: 1.8356 accuracy: 0.5468\n",
      "Epoch 598, CIFAR-10 Batch 2:  loss: 1.84259 accuracy: 0.5446\n",
      "Epoch 598, CIFAR-10 Batch 3:  loss: 1.90241 accuracy: 0.5502\n",
      "Epoch 598, CIFAR-10 Batch 4:  loss: 1.78569 accuracy: 0.548\n",
      "Epoch 598, CIFAR-10 Batch 5:  loss: 1.77954 accuracy: 0.5404\n",
      "Epoch 599, CIFAR-10 Batch 1:  loss: 1.82885 accuracy: 0.549\n",
      "Epoch 599, CIFAR-10 Batch 2:  loss: 1.82729 accuracy: 0.5452\n",
      "Epoch 599, CIFAR-10 Batch 3:  loss: 1.92379 accuracy: 0.5484\n",
      "Epoch 599, CIFAR-10 Batch 4:  loss: 1.77345 accuracy: 0.5466\n",
      "Epoch 599, CIFAR-10 Batch 5:  loss: 1.78635 accuracy: 0.5374\n",
      "Epoch 600, CIFAR-10 Batch 1:  loss: 1.83112 accuracy: 0.5474\n",
      "Epoch 600, CIFAR-10 Batch 2:  loss: 1.80469 accuracy: 0.5484\n",
      "Epoch 600, CIFAR-10 Batch 3:  loss: 1.94489 accuracy: 0.5472\n",
      "Epoch 600, CIFAR-10 Batch 4:  loss: 1.74787 accuracy: 0.5488\n",
      "Epoch 600, CIFAR-10 Batch 5:  loss: 1.77438 accuracy: 0.5392\n",
      "Epoch 601, CIFAR-10 Batch 1:  loss: 1.83194 accuracy: 0.5464\n",
      "Epoch 601, CIFAR-10 Batch 2:  loss: 1.83025 accuracy: 0.5462\n",
      "Epoch 601, CIFAR-10 Batch 3:  loss: 1.91509 accuracy: 0.5476\n",
      "Epoch 601, CIFAR-10 Batch 4:  loss: 1.77023 accuracy: 0.5484\n",
      "Epoch 601, CIFAR-10 Batch 5:  loss: 1.7958 accuracy: 0.5398\n",
      "Epoch 602, CIFAR-10 Batch 1:  loss: 1.80786 accuracy: 0.55\n",
      "Epoch 602, CIFAR-10 Batch 2:  loss: 1.83154 accuracy: 0.5448\n",
      "Epoch 602, CIFAR-10 Batch 3:  loss: 1.89437 accuracy: 0.5506\n",
      "Epoch 602, CIFAR-10 Batch 4:  loss: 1.76067 accuracy: 0.5496\n",
      "Epoch 602, CIFAR-10 Batch 5:  loss: 1.79154 accuracy: 0.544\n",
      "Epoch 603, CIFAR-10 Batch 1:  loss: 1.81176 accuracy: 0.5486\n",
      "Epoch 603, CIFAR-10 Batch 2:  loss: 1.80399 accuracy: 0.5464\n",
      "Epoch 603, CIFAR-10 Batch 3:  loss: 1.88423 accuracy: 0.55\n",
      "Epoch 603, CIFAR-10 Batch 4:  loss: 1.75621 accuracy: 0.5458\n",
      "Epoch 603, CIFAR-10 Batch 5:  loss: 1.77152 accuracy: 0.541\n",
      "Epoch 604, CIFAR-10 Batch 1:  loss: 1.84206 accuracy: 0.5482\n",
      "Epoch 604, CIFAR-10 Batch 2:  loss: 1.79948 accuracy: 0.547\n",
      "Epoch 604, CIFAR-10 Batch 3:  loss: 1.89511 accuracy: 0.5486\n",
      "Epoch 604, CIFAR-10 Batch 4:  loss: 1.79277 accuracy: 0.5474\n",
      "Epoch 604, CIFAR-10 Batch 5:  loss: 1.76663 accuracy: 0.5412\n",
      "Epoch 605, CIFAR-10 Batch 1:  loss: 1.861 accuracy: 0.5476\n",
      "Epoch 605, CIFAR-10 Batch 2:  loss: 1.81171 accuracy: 0.546\n",
      "Epoch 605, CIFAR-10 Batch 3:  loss: 1.92613 accuracy: 0.548\n",
      "Epoch 605, CIFAR-10 Batch 4:  loss: 1.76725 accuracy: 0.55\n",
      "Epoch 605, CIFAR-10 Batch 5:  loss: 1.79064 accuracy: 0.5434\n",
      "Epoch 606, CIFAR-10 Batch 1:  loss: 1.87016 accuracy: 0.549\n",
      "Epoch 606, CIFAR-10 Batch 2:  loss: 1.80373 accuracy: 0.5464\n",
      "Epoch 606, CIFAR-10 Batch 3:  loss: 1.91277 accuracy: 0.5494\n",
      "Epoch 606, CIFAR-10 Batch 4:  loss: 1.75668 accuracy: 0.5476\n",
      "Epoch 606, CIFAR-10 Batch 5:  loss: 1.78594 accuracy: 0.5442\n",
      "Epoch 607, CIFAR-10 Batch 1:  loss: 1.85992 accuracy: 0.5466\n",
      "Epoch 607, CIFAR-10 Batch 2:  loss: 1.82438 accuracy: 0.543\n",
      "Epoch 607, CIFAR-10 Batch 3:  loss: 1.91172 accuracy: 0.5512\n",
      "Epoch 607, CIFAR-10 Batch 4:  loss: 1.74603 accuracy: 0.5488\n",
      "Epoch 607, CIFAR-10 Batch 5:  loss: 1.8224 accuracy: 0.5416\n",
      "Epoch 608, CIFAR-10 Batch 1:  loss: 1.84809 accuracy: 0.5472\n",
      "Epoch 608, CIFAR-10 Batch 2:  loss: 1.80899 accuracy: 0.5472\n",
      "Epoch 608, CIFAR-10 Batch 3:  loss: 1.90076 accuracy: 0.5474\n",
      "Epoch 608, CIFAR-10 Batch 4:  loss: 1.73676 accuracy: 0.5504\n",
      "Epoch 608, CIFAR-10 Batch 5:  loss: 1.8079 accuracy: 0.5408\n",
      "Epoch 609, CIFAR-10 Batch 1:  loss: 1.84932 accuracy: 0.5472\n",
      "Epoch 609, CIFAR-10 Batch 2:  loss: 1.81554 accuracy: 0.5446\n",
      "Epoch 609, CIFAR-10 Batch 3:  loss: 1.8633 accuracy: 0.5482\n",
      "Epoch 609, CIFAR-10 Batch 4:  loss: 1.75134 accuracy: 0.548\n",
      "Epoch 609, CIFAR-10 Batch 5:  loss: 1.82167 accuracy: 0.5444\n",
      "Epoch 610, CIFAR-10 Batch 1:  loss: 1.84793 accuracy: 0.5472\n",
      "Epoch 610, CIFAR-10 Batch 2:  loss: 1.79863 accuracy: 0.5474\n",
      "Epoch 610, CIFAR-10 Batch 3:  loss: 1.91069 accuracy: 0.5498\n",
      "Epoch 610, CIFAR-10 Batch 4:  loss: 1.77821 accuracy: 0.549\n",
      "Epoch 610, CIFAR-10 Batch 5:  loss: 1.81982 accuracy: 0.5422\n",
      "Epoch 611, CIFAR-10 Batch 1:  loss: 1.83351 accuracy: 0.5492\n",
      "Epoch 611, CIFAR-10 Batch 2:  loss: 1.81144 accuracy: 0.5458\n",
      "Epoch 611, CIFAR-10 Batch 3:  loss: 1.95467 accuracy: 0.5474\n",
      "Epoch 611, CIFAR-10 Batch 4:  loss: 1.77229 accuracy: 0.551\n",
      "Epoch 611, CIFAR-10 Batch 5:  loss: 1.81605 accuracy: 0.5416\n",
      "Epoch 612, CIFAR-10 Batch 1:  loss: 1.84935 accuracy: 0.5476\n",
      "Epoch 612, CIFAR-10 Batch 2:  loss: 1.83884 accuracy: 0.5452\n",
      "Epoch 612, CIFAR-10 Batch 3:  loss: 1.92401 accuracy: 0.5494\n",
      "Epoch 612, CIFAR-10 Batch 4:  loss: 1.76985 accuracy: 0.5472\n",
      "Epoch 612, CIFAR-10 Batch 5:  loss: 1.81338 accuracy: 0.541\n",
      "Epoch 613, CIFAR-10 Batch 1:  loss: 1.85488 accuracy: 0.5478\n",
      "Epoch 613, CIFAR-10 Batch 2:  loss: 1.83319 accuracy: 0.5452\n",
      "Epoch 613, CIFAR-10 Batch 3:  loss: 1.93748 accuracy: 0.5462\n",
      "Epoch 613, CIFAR-10 Batch 4:  loss: 1.75955 accuracy: 0.5476\n",
      "Epoch 613, CIFAR-10 Batch 5:  loss: 1.82459 accuracy: 0.5446\n",
      "Epoch 614, CIFAR-10 Batch 1:  loss: 1.85308 accuracy: 0.547\n",
      "Epoch 614, CIFAR-10 Batch 2:  loss: 1.83319 accuracy: 0.5456\n",
      "Epoch 614, CIFAR-10 Batch 3:  loss: 1.92428 accuracy: 0.5474\n",
      "Epoch 614, CIFAR-10 Batch 4:  loss: 1.76249 accuracy: 0.5496\n",
      "Epoch 614, CIFAR-10 Batch 5:  loss: 1.81363 accuracy: 0.5424\n",
      "Epoch 615, CIFAR-10 Batch 1:  loss: 1.85398 accuracy: 0.5498\n",
      "Epoch 615, CIFAR-10 Batch 2:  loss: 1.82783 accuracy: 0.5438\n",
      "Epoch 615, CIFAR-10 Batch 3:  loss: 1.95078 accuracy: 0.5464\n",
      "Epoch 615, CIFAR-10 Batch 4:  loss: 1.79446 accuracy: 0.5492\n",
      "Epoch 615, CIFAR-10 Batch 5:  loss: 1.77296 accuracy: 0.5422\n",
      "Epoch 616, CIFAR-10 Batch 1:  loss: 1.84999 accuracy: 0.5488\n",
      "Epoch 616, CIFAR-10 Batch 2:  loss: 1.86221 accuracy: 0.5454\n",
      "Epoch 616, CIFAR-10 Batch 3:  loss: 1.97198 accuracy: 0.547\n",
      "Epoch 616, CIFAR-10 Batch 4:  loss: 1.75062 accuracy: 0.5484\n",
      "Epoch 616, CIFAR-10 Batch 5:  loss: 1.80818 accuracy: 0.5402\n",
      "Epoch 617, CIFAR-10 Batch 1:  loss: 1.8644 accuracy: 0.5456\n",
      "Epoch 617, CIFAR-10 Batch 2:  loss: 1.8474 accuracy: 0.544\n",
      "Epoch 617, CIFAR-10 Batch 3:  loss: 1.94806 accuracy: 0.5468\n",
      "Epoch 617, CIFAR-10 Batch 4:  loss: 1.76062 accuracy: 0.5486\n",
      "Epoch 617, CIFAR-10 Batch 5:  loss: 1.83025 accuracy: 0.5416\n",
      "Epoch 618, CIFAR-10 Batch 1:  loss: 1.86164 accuracy: 0.5466\n",
      "Epoch 618, CIFAR-10 Batch 2:  loss: 1.85527 accuracy: 0.5434\n",
      "Epoch 618, CIFAR-10 Batch 3:  loss: 1.94737 accuracy: 0.5462\n",
      "Epoch 618, CIFAR-10 Batch 4:  loss: 1.74836 accuracy: 0.5494\n",
      "Epoch 618, CIFAR-10 Batch 5:  loss: 1.83881 accuracy: 0.5436\n",
      "Epoch 619, CIFAR-10 Batch 1:  loss: 1.85587 accuracy: 0.5476\n",
      "Epoch 619, CIFAR-10 Batch 2:  loss: 1.82329 accuracy: 0.5466\n",
      "Epoch 619, CIFAR-10 Batch 3:  loss: 1.97843 accuracy: 0.5462\n",
      "Epoch 619, CIFAR-10 Batch 4:  loss: 1.77106 accuracy: 0.549\n",
      "Epoch 619, CIFAR-10 Batch 5:  loss: 1.82608 accuracy: 0.5428\n",
      "Epoch 620, CIFAR-10 Batch 1:  loss: 1.84016 accuracy: 0.5486\n",
      "Epoch 620, CIFAR-10 Batch 2:  loss: 1.85543 accuracy: 0.5444\n",
      "Epoch 620, CIFAR-10 Batch 3:  loss: 1.95744 accuracy: 0.5452\n",
      "Epoch 620, CIFAR-10 Batch 4:  loss: 1.77558 accuracy: 0.5486\n",
      "Epoch 620, CIFAR-10 Batch 5:  loss: 1.80854 accuracy: 0.541\n",
      "Epoch 621, CIFAR-10 Batch 1:  loss: 1.84114 accuracy: 0.5454\n",
      "Epoch 621, CIFAR-10 Batch 2:  loss: 1.8431 accuracy: 0.5466\n",
      "Epoch 621, CIFAR-10 Batch 3:  loss: 1.93953 accuracy: 0.5486\n",
      "Epoch 621, CIFAR-10 Batch 4:  loss: 1.78476 accuracy: 0.5484\n",
      "Epoch 621, CIFAR-10 Batch 5:  loss: 1.81817 accuracy: 0.5422\n",
      "Epoch 622, CIFAR-10 Batch 1:  loss: 1.83692 accuracy: 0.5476\n",
      "Epoch 622, CIFAR-10 Batch 2:  loss: 1.87248 accuracy: 0.5472\n",
      "Epoch 622, CIFAR-10 Batch 3:  loss: 1.94724 accuracy: 0.546\n",
      "Epoch 622, CIFAR-10 Batch 4:  loss: 1.77674 accuracy: 0.5484\n",
      "Epoch 622, CIFAR-10 Batch 5:  loss: 1.81863 accuracy: 0.5426\n",
      "Epoch 623, CIFAR-10 Batch 1:  loss: 1.80979 accuracy: 0.5492\n",
      "Epoch 623, CIFAR-10 Batch 2:  loss: 1.86065 accuracy: 0.548\n",
      "Epoch 623, CIFAR-10 Batch 3:  loss: 1.88566 accuracy: 0.5494\n",
      "Epoch 623, CIFAR-10 Batch 4:  loss: 1.74684 accuracy: 0.551\n",
      "Epoch 623, CIFAR-10 Batch 5:  loss: 1.80715 accuracy: 0.5412\n",
      "Epoch 624, CIFAR-10 Batch 1:  loss: 1.83664 accuracy: 0.5482\n",
      "Epoch 624, CIFAR-10 Batch 2:  loss: 1.83993 accuracy: 0.545\n",
      "Epoch 624, CIFAR-10 Batch 3:  loss: 1.86496 accuracy: 0.5472\n",
      "Epoch 624, CIFAR-10 Batch 4:  loss: 1.7984 accuracy: 0.5476\n",
      "Epoch 624, CIFAR-10 Batch 5:  loss: 1.83473 accuracy: 0.5452\n",
      "Epoch 625, CIFAR-10 Batch 1:  loss: 1.8551 accuracy: 0.5482\n",
      "Epoch 625, CIFAR-10 Batch 2:  loss: 1.83038 accuracy: 0.5442\n",
      "Epoch 625, CIFAR-10 Batch 3:  loss: 1.8897 accuracy: 0.5482\n",
      "Epoch 625, CIFAR-10 Batch 4:  loss: 1.8005 accuracy: 0.5486\n",
      "Epoch 625, CIFAR-10 Batch 5:  loss: 1.82101 accuracy: 0.544\n",
      "Epoch 626, CIFAR-10 Batch 1:  loss: 1.86592 accuracy: 0.548\n",
      "Epoch 626, CIFAR-10 Batch 2:  loss: 1.83503 accuracy: 0.5466\n",
      "Epoch 626, CIFAR-10 Batch 3:  loss: 1.88728 accuracy: 0.5456\n",
      "Epoch 626, CIFAR-10 Batch 4:  loss: 1.77052 accuracy: 0.5494\n",
      "Epoch 626, CIFAR-10 Batch 5:  loss: 1.81832 accuracy: 0.5442\n",
      "Epoch 627, CIFAR-10 Batch 1:  loss: 1.83895 accuracy: 0.5472\n",
      "Epoch 627, CIFAR-10 Batch 2:  loss: 1.84167 accuracy: 0.5468\n",
      "Epoch 627, CIFAR-10 Batch 3:  loss: 1.94053 accuracy: 0.548\n",
      "Epoch 627, CIFAR-10 Batch 4:  loss: 1.77102 accuracy: 0.549\n",
      "Epoch 627, CIFAR-10 Batch 5:  loss: 1.82905 accuracy: 0.5412\n",
      "Epoch 628, CIFAR-10 Batch 1:  loss: 1.87229 accuracy: 0.5482\n",
      "Epoch 628, CIFAR-10 Batch 2:  loss: 1.85516 accuracy: 0.5476\n",
      "Epoch 628, CIFAR-10 Batch 3:  loss: 1.90663 accuracy: 0.5504\n",
      "Epoch 628, CIFAR-10 Batch 4:  loss: 1.77274 accuracy: 0.5472\n",
      "Epoch 628, CIFAR-10 Batch 5:  loss: 1.83878 accuracy: 0.5422\n",
      "Epoch 629, CIFAR-10 Batch 1:  loss: 1.87915 accuracy: 0.5466\n",
      "Epoch 629, CIFAR-10 Batch 2:  loss: 1.80562 accuracy: 0.55\n",
      "Epoch 629, CIFAR-10 Batch 3:  loss: 1.92866 accuracy: 0.549\n",
      "Epoch 629, CIFAR-10 Batch 4:  loss: 1.76113 accuracy: 0.548\n",
      "Epoch 629, CIFAR-10 Batch 5:  loss: 1.83246 accuracy: 0.5408\n",
      "Epoch 630, CIFAR-10 Batch 1:  loss: 1.87302 accuracy: 0.55\n",
      "Epoch 630, CIFAR-10 Batch 2:  loss: 1.84951 accuracy: 0.5462\n",
      "Epoch 630, CIFAR-10 Batch 3:  loss: 1.90462 accuracy: 0.5534\n",
      "Epoch 630, CIFAR-10 Batch 4:  loss: 1.77265 accuracy: 0.5492\n",
      "Epoch 630, CIFAR-10 Batch 5:  loss: 1.82408 accuracy: 0.5426\n",
      "Epoch 631, CIFAR-10 Batch 1:  loss: 1.83892 accuracy: 0.5498\n",
      "Epoch 631, CIFAR-10 Batch 2:  loss: 1.84871 accuracy: 0.545\n",
      "Epoch 631, CIFAR-10 Batch 3:  loss: 1.92413 accuracy: 0.5494\n",
      "Epoch 631, CIFAR-10 Batch 4:  loss: 1.76856 accuracy: 0.5482\n",
      "Epoch 631, CIFAR-10 Batch 5:  loss: 1.85854 accuracy: 0.5422\n",
      "Epoch 632, CIFAR-10 Batch 1:  loss: 1.86706 accuracy: 0.5486\n",
      "Epoch 632, CIFAR-10 Batch 2:  loss: 1.84497 accuracy: 0.547\n",
      "Epoch 632, CIFAR-10 Batch 3:  loss: 1.89384 accuracy: 0.5492\n",
      "Epoch 632, CIFAR-10 Batch 4:  loss: 1.80434 accuracy: 0.5498\n",
      "Epoch 632, CIFAR-10 Batch 5:  loss: 1.83959 accuracy: 0.5418\n",
      "Epoch 633, CIFAR-10 Batch 1:  loss: 1.87549 accuracy: 0.5474\n",
      "Epoch 633, CIFAR-10 Batch 2:  loss: 1.89336 accuracy: 0.5482\n",
      "Epoch 633, CIFAR-10 Batch 3:  loss: 1.92719 accuracy: 0.5502\n",
      "Epoch 633, CIFAR-10 Batch 4:  loss: 1.79931 accuracy: 0.5488\n",
      "Epoch 633, CIFAR-10 Batch 5:  loss: 1.84187 accuracy: 0.5428\n",
      "Epoch 634, CIFAR-10 Batch 1:  loss: 1.88577 accuracy: 0.5464\n",
      "Epoch 634, CIFAR-10 Batch 2:  loss: 1.82128 accuracy: 0.5482\n",
      "Epoch 634, CIFAR-10 Batch 3:  loss: 1.93269 accuracy: 0.5484\n",
      "Epoch 634, CIFAR-10 Batch 4:  loss: 1.79999 accuracy: 0.5476\n",
      "Epoch 634, CIFAR-10 Batch 5:  loss: 1.82535 accuracy: 0.5442\n",
      "Epoch 635, CIFAR-10 Batch 1:  loss: 1.88947 accuracy: 0.5492\n",
      "Epoch 635, CIFAR-10 Batch 2:  loss: 1.84765 accuracy: 0.5474\n",
      "Epoch 635, CIFAR-10 Batch 3:  loss: 1.94388 accuracy: 0.55\n",
      "Epoch 635, CIFAR-10 Batch 4:  loss: 1.82302 accuracy: 0.5494\n",
      "Epoch 635, CIFAR-10 Batch 5:  loss: 1.85384 accuracy: 0.5426\n",
      "Epoch 636, CIFAR-10 Batch 1:  loss: 1.87456 accuracy: 0.548\n",
      "Epoch 636, CIFAR-10 Batch 2:  loss: 1.87939 accuracy: 0.5442\n",
      "Epoch 636, CIFAR-10 Batch 3:  loss: 1.98155 accuracy: 0.5452\n",
      "Epoch 636, CIFAR-10 Batch 4:  loss: 1.81001 accuracy: 0.5486\n",
      "Epoch 636, CIFAR-10 Batch 5:  loss: 1.845 accuracy: 0.5438\n",
      "Epoch 637, CIFAR-10 Batch 1:  loss: 1.89748 accuracy: 0.5452\n",
      "Epoch 637, CIFAR-10 Batch 2:  loss: 1.90227 accuracy: 0.5454\n",
      "Epoch 637, CIFAR-10 Batch 3:  loss: 1.98559 accuracy: 0.5482\n",
      "Epoch 637, CIFAR-10 Batch 4:  loss: 1.81057 accuracy: 0.5488\n",
      "Epoch 637, CIFAR-10 Batch 5:  loss: 1.86947 accuracy: 0.5422\n",
      "Epoch 638, CIFAR-10 Batch 1:  loss: 1.90868 accuracy: 0.546\n",
      "Epoch 638, CIFAR-10 Batch 2:  loss: 1.90032 accuracy: 0.5474\n",
      "Epoch 638, CIFAR-10 Batch 3:  loss: 1.98396 accuracy: 0.5484\n",
      "Epoch 638, CIFAR-10 Batch 4:  loss: 1.79278 accuracy: 0.5512\n",
      "Epoch 638, CIFAR-10 Batch 5:  loss: 1.85049 accuracy: 0.5456\n",
      "Epoch 639, CIFAR-10 Batch 1:  loss: 1.91163 accuracy: 0.546\n",
      "Epoch 639, CIFAR-10 Batch 2:  loss: 1.85446 accuracy: 0.5474\n",
      "Epoch 639, CIFAR-10 Batch 3:  loss: 1.94431 accuracy: 0.546\n",
      "Epoch 639, CIFAR-10 Batch 4:  loss: 1.82573 accuracy: 0.5498\n",
      "Epoch 639, CIFAR-10 Batch 5:  loss: 1.86699 accuracy: 0.5446\n",
      "Epoch 640, CIFAR-10 Batch 1:  loss: 1.90186 accuracy: 0.5496\n",
      "Epoch 640, CIFAR-10 Batch 2:  loss: 1.85027 accuracy: 0.5504\n",
      "Epoch 640, CIFAR-10 Batch 3:  loss: 1.93467 accuracy: 0.5488\n",
      "Epoch 640, CIFAR-10 Batch 4:  loss: 1.80275 accuracy: 0.5472\n",
      "Epoch 640, CIFAR-10 Batch 5:  loss: 1.84054 accuracy: 0.5456\n",
      "Epoch 641, CIFAR-10 Batch 1:  loss: 1.83108 accuracy: 0.5492\n",
      "Epoch 641, CIFAR-10 Batch 2:  loss: 1.88792 accuracy: 0.5496\n",
      "Epoch 641, CIFAR-10 Batch 3:  loss: 1.93644 accuracy: 0.5508\n",
      "Epoch 641, CIFAR-10 Batch 4:  loss: 1.82172 accuracy: 0.5488\n",
      "Epoch 641, CIFAR-10 Batch 5:  loss: 1.85881 accuracy: 0.5428\n",
      "Epoch 642, CIFAR-10 Batch 1:  loss: 1.8706 accuracy: 0.5484\n",
      "Epoch 642, CIFAR-10 Batch 2:  loss: 1.89167 accuracy: 0.5494\n",
      "Epoch 642, CIFAR-10 Batch 3:  loss: 1.9761 accuracy: 0.5488\n",
      "Epoch 642, CIFAR-10 Batch 4:  loss: 1.82579 accuracy: 0.5504\n",
      "Epoch 642, CIFAR-10 Batch 5:  loss: 1.84079 accuracy: 0.543\n",
      "Epoch 643, CIFAR-10 Batch 1:  loss: 1.88747 accuracy: 0.5482\n",
      "Epoch 643, CIFAR-10 Batch 2:  loss: 1.88702 accuracy: 0.546\n",
      "Epoch 643, CIFAR-10 Batch 3:  loss: 1.96219 accuracy: 0.5498\n",
      "Epoch 643, CIFAR-10 Batch 4:  loss: 1.8184 accuracy: 0.549\n",
      "Epoch 643, CIFAR-10 Batch 5:  loss: 1.83852 accuracy: 0.5456\n",
      "Epoch 644, CIFAR-10 Batch 1:  loss: 1.87112 accuracy: 0.5478\n",
      "Epoch 644, CIFAR-10 Batch 2:  loss: 1.91062 accuracy: 0.5482\n",
      "Epoch 644, CIFAR-10 Batch 3:  loss: 1.96459 accuracy: 0.55\n",
      "Epoch 644, CIFAR-10 Batch 4:  loss: 1.81479 accuracy: 0.55\n",
      "Epoch 644, CIFAR-10 Batch 5:  loss: 1.83418 accuracy: 0.5432\n",
      "Epoch 645, CIFAR-10 Batch 1:  loss: 1.8999 accuracy: 0.5464\n",
      "Epoch 645, CIFAR-10 Batch 2:  loss: 1.90819 accuracy: 0.5476\n",
      "Epoch 645, CIFAR-10 Batch 3:  loss: 1.9736 accuracy: 0.5482\n",
      "Epoch 645, CIFAR-10 Batch 4:  loss: 1.80927 accuracy: 0.5494\n",
      "Epoch 645, CIFAR-10 Batch 5:  loss: 1.83317 accuracy: 0.5434\n",
      "Epoch 646, CIFAR-10 Batch 1:  loss: 1.9229 accuracy: 0.5462\n",
      "Epoch 646, CIFAR-10 Batch 2:  loss: 1.92491 accuracy: 0.5468\n",
      "Epoch 646, CIFAR-10 Batch 3:  loss: 1.97887 accuracy: 0.5498\n",
      "Epoch 646, CIFAR-10 Batch 4:  loss: 1.8322 accuracy: 0.5522\n",
      "Epoch 646, CIFAR-10 Batch 5:  loss: 1.85171 accuracy: 0.5444\n",
      "Epoch 647, CIFAR-10 Batch 1:  loss: 1.92737 accuracy: 0.5492\n",
      "Epoch 647, CIFAR-10 Batch 2:  loss: 1.90256 accuracy: 0.5464\n",
      "Epoch 647, CIFAR-10 Batch 3:  loss: 1.96791 accuracy: 0.5476\n",
      "Epoch 647, CIFAR-10 Batch 4:  loss: 1.79076 accuracy: 0.552\n",
      "Epoch 647, CIFAR-10 Batch 5:  loss: 1.84698 accuracy: 0.5434\n",
      "Epoch 648, CIFAR-10 Batch 1:  loss: 1.89821 accuracy: 0.5466\n",
      "Epoch 648, CIFAR-10 Batch 2:  loss: 1.90909 accuracy: 0.5494\n",
      "Epoch 648, CIFAR-10 Batch 3:  loss: 1.96439 accuracy: 0.5484\n",
      "Epoch 648, CIFAR-10 Batch 4:  loss: 1.80215 accuracy: 0.5494\n",
      "Epoch 648, CIFAR-10 Batch 5:  loss: 1.83287 accuracy: 0.5422\n",
      "Epoch 649, CIFAR-10 Batch 1:  loss: 1.90668 accuracy: 0.5474\n",
      "Epoch 649, CIFAR-10 Batch 2:  loss: 1.9092 accuracy: 0.5494\n",
      "Epoch 649, CIFAR-10 Batch 3:  loss: 1.91952 accuracy: 0.5496\n",
      "Epoch 649, CIFAR-10 Batch 4:  loss: 1.81023 accuracy: 0.5494\n",
      "Epoch 649, CIFAR-10 Batch 5:  loss: 1.85843 accuracy: 0.5452\n",
      "Epoch 650, CIFAR-10 Batch 1:  loss: 1.89243 accuracy: 0.5476\n",
      "Epoch 650, CIFAR-10 Batch 2:  loss: 1.89181 accuracy: 0.5498\n",
      "Epoch 650, CIFAR-10 Batch 3:  loss: 1.91735 accuracy: 0.5532\n",
      "Epoch 650, CIFAR-10 Batch 4:  loss: 1.78299 accuracy: 0.549\n",
      "Epoch 650, CIFAR-10 Batch 5:  loss: 1.88187 accuracy: 0.5418\n",
      "Epoch 651, CIFAR-10 Batch 1:  loss: 1.88018 accuracy: 0.5468\n",
      "Epoch 651, CIFAR-10 Batch 2:  loss: 1.89211 accuracy: 0.5492\n",
      "Epoch 651, CIFAR-10 Batch 3:  loss: 1.95973 accuracy: 0.5496\n",
      "Epoch 651, CIFAR-10 Batch 4:  loss: 1.80336 accuracy: 0.5526\n",
      "Epoch 651, CIFAR-10 Batch 5:  loss: 1.8564 accuracy: 0.5442\n",
      "Epoch 652, CIFAR-10 Batch 1:  loss: 1.89724 accuracy: 0.5476\n",
      "Epoch 652, CIFAR-10 Batch 2:  loss: 1.89512 accuracy: 0.5498\n",
      "Epoch 652, CIFAR-10 Batch 3:  loss: 1.93851 accuracy: 0.5498\n",
      "Epoch 652, CIFAR-10 Batch 4:  loss: 1.83024 accuracy: 0.5508\n",
      "Epoch 652, CIFAR-10 Batch 5:  loss: 1.86537 accuracy: 0.543\n",
      "Epoch 653, CIFAR-10 Batch 1:  loss: 1.92312 accuracy: 0.5476\n",
      "Epoch 653, CIFAR-10 Batch 2:  loss: 1.91844 accuracy: 0.5498\n",
      "Epoch 653, CIFAR-10 Batch 3:  loss: 1.95244 accuracy: 0.5526\n",
      "Epoch 653, CIFAR-10 Batch 4:  loss: 1.85067 accuracy: 0.5506\n",
      "Epoch 653, CIFAR-10 Batch 5:  loss: 1.86694 accuracy: 0.542\n",
      "Epoch 654, CIFAR-10 Batch 1:  loss: 1.91819 accuracy: 0.547\n",
      "Epoch 654, CIFAR-10 Batch 2:  loss: 1.88857 accuracy: 0.552\n",
      "Epoch 654, CIFAR-10 Batch 3:  loss: 1.95883 accuracy: 0.5502\n",
      "Epoch 654, CIFAR-10 Batch 4:  loss: 1.8475 accuracy: 0.5504\n",
      "Epoch 654, CIFAR-10 Batch 5:  loss: 1.87775 accuracy: 0.5402\n",
      "Epoch 655, CIFAR-10 Batch 1:  loss: 1.90602 accuracy: 0.5482\n",
      "Epoch 655, CIFAR-10 Batch 2:  loss: 1.92515 accuracy: 0.5496\n",
      "Epoch 655, CIFAR-10 Batch 3:  loss: 1.97137 accuracy: 0.5506\n",
      "Epoch 655, CIFAR-10 Batch 4:  loss: 1.84037 accuracy: 0.5512\n",
      "Epoch 655, CIFAR-10 Batch 5:  loss: 1.86555 accuracy: 0.5426\n",
      "Epoch 656, CIFAR-10 Batch 1:  loss: 1.91605 accuracy: 0.5472\n",
      "Epoch 656, CIFAR-10 Batch 2:  loss: 1.92677 accuracy: 0.5476\n",
      "Epoch 656, CIFAR-10 Batch 3:  loss: 1.97019 accuracy: 0.5522\n",
      "Epoch 656, CIFAR-10 Batch 4:  loss: 1.84092 accuracy: 0.5532\n",
      "Epoch 656, CIFAR-10 Batch 5:  loss: 1.87645 accuracy: 0.5402\n",
      "Epoch 657, CIFAR-10 Batch 1:  loss: 1.92318 accuracy: 0.5452\n",
      "Epoch 657, CIFAR-10 Batch 2:  loss: 1.90285 accuracy: 0.548\n",
      "Epoch 657, CIFAR-10 Batch 3:  loss: 1.97045 accuracy: 0.5516\n",
      "Epoch 657, CIFAR-10 Batch 4:  loss: 1.83775 accuracy: 0.5532\n",
      "Epoch 657, CIFAR-10 Batch 5:  loss: 1.8633 accuracy: 0.542\n",
      "Epoch 658, CIFAR-10 Batch 1:  loss: 1.89386 accuracy: 0.551\n",
      "Epoch 658, CIFAR-10 Batch 2:  loss: 1.94222 accuracy: 0.5508\n",
      "Epoch 658, CIFAR-10 Batch 3:  loss: 1.98009 accuracy: 0.55\n",
      "Epoch 658, CIFAR-10 Batch 4:  loss: 1.83668 accuracy: 0.5512\n",
      "Epoch 658, CIFAR-10 Batch 5:  loss: 1.87707 accuracy: 0.542\n",
      "Epoch 659, CIFAR-10 Batch 1:  loss: 1.92158 accuracy: 0.5482\n",
      "Epoch 659, CIFAR-10 Batch 2:  loss: 1.92799 accuracy: 0.5508\n",
      "Epoch 659, CIFAR-10 Batch 3:  loss: 1.96942 accuracy: 0.552\n",
      "Epoch 659, CIFAR-10 Batch 4:  loss: 1.82039 accuracy: 0.5502\n",
      "Epoch 659, CIFAR-10 Batch 5:  loss: 1.85606 accuracy: 0.5414\n",
      "Epoch 660, CIFAR-10 Batch 1:  loss: 1.89412 accuracy: 0.548\n",
      "Epoch 660, CIFAR-10 Batch 2:  loss: 1.92078 accuracy: 0.5512\n",
      "Epoch 660, CIFAR-10 Batch 3:  loss: 1.97915 accuracy: 0.5508\n",
      "Epoch 660, CIFAR-10 Batch 4:  loss: 1.82944 accuracy: 0.5516\n",
      "Epoch 660, CIFAR-10 Batch 5:  loss: 1.89458 accuracy: 0.5402\n",
      "Epoch 661, CIFAR-10 Batch 1:  loss: 1.91876 accuracy: 0.548\n",
      "Epoch 661, CIFAR-10 Batch 2:  loss: 1.92296 accuracy: 0.55\n",
      "Epoch 661, CIFAR-10 Batch 3:  loss: 1.98181 accuracy: 0.5522\n",
      "Epoch 661, CIFAR-10 Batch 4:  loss: 1.84467 accuracy: 0.5504\n",
      "Epoch 661, CIFAR-10 Batch 5:  loss: 1.88793 accuracy: 0.5398\n",
      "Epoch 662, CIFAR-10 Batch 1:  loss: 1.92044 accuracy: 0.5478\n",
      "Epoch 662, CIFAR-10 Batch 2:  loss: 1.92283 accuracy: 0.5488\n",
      "Epoch 662, CIFAR-10 Batch 3:  loss: 1.92965 accuracy: 0.5526\n",
      "Epoch 662, CIFAR-10 Batch 4:  loss: 1.81549 accuracy: 0.554\n",
      "Epoch 662, CIFAR-10 Batch 5:  loss: 1.87205 accuracy: 0.5404\n",
      "Epoch 663, CIFAR-10 Batch 1:  loss: 1.91339 accuracy: 0.5486\n",
      "Epoch 663, CIFAR-10 Batch 2:  loss: 1.93431 accuracy: 0.5498\n",
      "Epoch 663, CIFAR-10 Batch 3:  loss: 1.93189 accuracy: 0.5534\n",
      "Epoch 663, CIFAR-10 Batch 4:  loss: 1.78528 accuracy: 0.5508\n",
      "Epoch 663, CIFAR-10 Batch 5:  loss: 1.89144 accuracy: 0.54\n",
      "Epoch 664, CIFAR-10 Batch 1:  loss: 1.95539 accuracy: 0.5452\n",
      "Epoch 664, CIFAR-10 Batch 2:  loss: 1.91049 accuracy: 0.547\n",
      "Epoch 664, CIFAR-10 Batch 3:  loss: 1.96037 accuracy: 0.5492\n",
      "Epoch 664, CIFAR-10 Batch 4:  loss: 1.82865 accuracy: 0.5502\n",
      "Epoch 664, CIFAR-10 Batch 5:  loss: 1.89215 accuracy: 0.5434\n",
      "Epoch 665, CIFAR-10 Batch 1:  loss: 1.93386 accuracy: 0.5492\n",
      "Epoch 665, CIFAR-10 Batch 2:  loss: 1.92793 accuracy: 0.5466\n",
      "Epoch 665, CIFAR-10 Batch 3:  loss: 1.9713 accuracy: 0.5518\n",
      "Epoch 665, CIFAR-10 Batch 4:  loss: 1.81854 accuracy: 0.5512\n",
      "Epoch 665, CIFAR-10 Batch 5:  loss: 1.8806 accuracy: 0.5432\n",
      "Epoch 666, CIFAR-10 Batch 1:  loss: 1.94342 accuracy: 0.5486\n",
      "Epoch 666, CIFAR-10 Batch 2:  loss: 1.92421 accuracy: 0.5464\n",
      "Epoch 666, CIFAR-10 Batch 3:  loss: 1.97096 accuracy: 0.5518\n",
      "Epoch 666, CIFAR-10 Batch 4:  loss: 1.81216 accuracy: 0.553\n",
      "Epoch 666, CIFAR-10 Batch 5:  loss: 1.86247 accuracy: 0.5428\n",
      "Epoch 667, CIFAR-10 Batch 1:  loss: 1.93359 accuracy: 0.547\n",
      "Epoch 667, CIFAR-10 Batch 2:  loss: 1.92779 accuracy: 0.5472\n",
      "Epoch 667, CIFAR-10 Batch 3:  loss: 1.96189 accuracy: 0.55\n",
      "Epoch 667, CIFAR-10 Batch 4:  loss: 1.81086 accuracy: 0.5502\n",
      "Epoch 667, CIFAR-10 Batch 5:  loss: 1.90995 accuracy: 0.5416\n",
      "Epoch 668, CIFAR-10 Batch 1:  loss: 1.92445 accuracy: 0.5496\n",
      "Epoch 668, CIFAR-10 Batch 2:  loss: 1.96315 accuracy: 0.5458\n",
      "Epoch 668, CIFAR-10 Batch 3:  loss: 1.98013 accuracy: 0.5504\n",
      "Epoch 668, CIFAR-10 Batch 4:  loss: 1.82766 accuracy: 0.552\n",
      "Epoch 668, CIFAR-10 Batch 5:  loss: 1.91485 accuracy: 0.5424\n",
      "Epoch 669, CIFAR-10 Batch 1:  loss: 1.92907 accuracy: 0.5488\n",
      "Epoch 669, CIFAR-10 Batch 2:  loss: 1.94889 accuracy: 0.5472\n",
      "Epoch 669, CIFAR-10 Batch 3:  loss: 1.97743 accuracy: 0.5514\n",
      "Epoch 669, CIFAR-10 Batch 4:  loss: 1.83146 accuracy: 0.5504\n",
      "Epoch 669, CIFAR-10 Batch 5:  loss: 1.905 accuracy: 0.5436\n",
      "Epoch 670, CIFAR-10 Batch 1:  loss: 1.93478 accuracy: 0.5486\n",
      "Epoch 670, CIFAR-10 Batch 2:  loss: 1.94226 accuracy: 0.5462\n",
      "Epoch 670, CIFAR-10 Batch 3:  loss: 1.99387 accuracy: 0.551\n",
      "Epoch 670, CIFAR-10 Batch 4:  loss: 1.83347 accuracy: 0.5502\n",
      "Epoch 670, CIFAR-10 Batch 5:  loss: 1.89851 accuracy: 0.5398\n",
      "Epoch 671, CIFAR-10 Batch 1:  loss: 1.93406 accuracy: 0.5502\n",
      "Epoch 671, CIFAR-10 Batch 2:  loss: 1.95099 accuracy: 0.547\n",
      "Epoch 671, CIFAR-10 Batch 3:  loss: 1.93356 accuracy: 0.5518\n",
      "Epoch 671, CIFAR-10 Batch 4:  loss: 1.87069 accuracy: 0.5494\n",
      "Epoch 671, CIFAR-10 Batch 5:  loss: 1.91663 accuracy: 0.543\n",
      "Epoch 672, CIFAR-10 Batch 1:  loss: 1.93683 accuracy: 0.546\n",
      "Epoch 672, CIFAR-10 Batch 2:  loss: 1.97779 accuracy: 0.5466\n",
      "Epoch 672, CIFAR-10 Batch 3:  loss: 1.9752 accuracy: 0.5508\n",
      "Epoch 672, CIFAR-10 Batch 4:  loss: 1.8408 accuracy: 0.5474\n",
      "Epoch 672, CIFAR-10 Batch 5:  loss: 1.92233 accuracy: 0.5434\n",
      "Epoch 673, CIFAR-10 Batch 1:  loss: 1.94329 accuracy: 0.5492\n",
      "Epoch 673, CIFAR-10 Batch 2:  loss: 1.99161 accuracy: 0.5452\n",
      "Epoch 673, CIFAR-10 Batch 3:  loss: 1.9765 accuracy: 0.5518\n",
      "Epoch 673, CIFAR-10 Batch 4:  loss: 1.84432 accuracy: 0.5482\n",
      "Epoch 673, CIFAR-10 Batch 5:  loss: 1.91117 accuracy: 0.5412\n",
      "Epoch 674, CIFAR-10 Batch 1:  loss: 1.94142 accuracy: 0.5486\n",
      "Epoch 674, CIFAR-10 Batch 2:  loss: 1.99021 accuracy: 0.5458\n",
      "Epoch 674, CIFAR-10 Batch 3:  loss: 1.99565 accuracy: 0.5492\n",
      "Epoch 674, CIFAR-10 Batch 4:  loss: 1.85368 accuracy: 0.5492\n",
      "Epoch 674, CIFAR-10 Batch 5:  loss: 1.90504 accuracy: 0.5406\n",
      "Epoch 675, CIFAR-10 Batch 1:  loss: 1.95343 accuracy: 0.5476\n",
      "Epoch 675, CIFAR-10 Batch 2:  loss: 1.97078 accuracy: 0.5466\n",
      "Epoch 675, CIFAR-10 Batch 3:  loss: 1.97229 accuracy: 0.5482\n",
      "Epoch 675, CIFAR-10 Batch 4:  loss: 1.84283 accuracy: 0.5496\n",
      "Epoch 675, CIFAR-10 Batch 5:  loss: 1.9429 accuracy: 0.539\n",
      "Epoch 676, CIFAR-10 Batch 1:  loss: 1.945 accuracy: 0.548\n",
      "Epoch 676, CIFAR-10 Batch 2:  loss: 1.97801 accuracy: 0.5472\n",
      "Epoch 676, CIFAR-10 Batch 3:  loss: 1.96338 accuracy: 0.5486\n",
      "Epoch 676, CIFAR-10 Batch 4:  loss: 1.86049 accuracy: 0.5504\n",
      "Epoch 676, CIFAR-10 Batch 5:  loss: 1.88281 accuracy: 0.5404\n",
      "Epoch 677, CIFAR-10 Batch 1:  loss: 1.95296 accuracy: 0.5482\n",
      "Epoch 677, CIFAR-10 Batch 2:  loss: 1.98517 accuracy: 0.5488\n",
      "Epoch 677, CIFAR-10 Batch 3:  loss: 1.95997 accuracy: 0.5502\n",
      "Epoch 677, CIFAR-10 Batch 4:  loss: 1.86796 accuracy: 0.5486\n",
      "Epoch 677, CIFAR-10 Batch 5:  loss: 1.92668 accuracy: 0.5436\n",
      "Epoch 678, CIFAR-10 Batch 1:  loss: 1.94047 accuracy: 0.5484\n",
      "Epoch 678, CIFAR-10 Batch 2:  loss: 1.98143 accuracy: 0.5466\n",
      "Epoch 678, CIFAR-10 Batch 3:  loss: 1.97991 accuracy: 0.5482\n",
      "Epoch 678, CIFAR-10 Batch 4:  loss: 1.86641 accuracy: 0.5496\n",
      "Epoch 678, CIFAR-10 Batch 5:  loss: 1.89089 accuracy: 0.5406\n",
      "Epoch 679, CIFAR-10 Batch 1:  loss: 1.97504 accuracy: 0.5482\n",
      "Epoch 679, CIFAR-10 Batch 2:  loss: 2.00055 accuracy: 0.5466\n",
      "Epoch 679, CIFAR-10 Batch 3:  loss: 1.98898 accuracy: 0.5486\n",
      "Epoch 679, CIFAR-10 Batch 4:  loss: 1.87931 accuracy: 0.5492\n",
      "Epoch 679, CIFAR-10 Batch 5:  loss: 1.92379 accuracy: 0.5404\n",
      "Epoch 680, CIFAR-10 Batch 1:  loss: 1.94363 accuracy: 0.5496\n",
      "Epoch 680, CIFAR-10 Batch 2:  loss: 2.01219 accuracy: 0.546\n",
      "Epoch 680, CIFAR-10 Batch 3:  loss: 2.01523 accuracy: 0.5492\n",
      "Epoch 680, CIFAR-10 Batch 4:  loss: 1.86426 accuracy: 0.5506\n",
      "Epoch 680, CIFAR-10 Batch 5:  loss: 1.91122 accuracy: 0.5424\n",
      "Epoch 681, CIFAR-10 Batch 1:  loss: 1.94688 accuracy: 0.547\n",
      "Epoch 681, CIFAR-10 Batch 2:  loss: 1.98535 accuracy: 0.546\n",
      "Epoch 681, CIFAR-10 Batch 3:  loss: 1.96781 accuracy: 0.5482\n",
      "Epoch 681, CIFAR-10 Batch 4:  loss: 1.8466 accuracy: 0.549\n",
      "Epoch 681, CIFAR-10 Batch 5:  loss: 1.92039 accuracy: 0.542\n",
      "Epoch 682, CIFAR-10 Batch 1:  loss: 1.93891 accuracy: 0.5492\n",
      "Epoch 682, CIFAR-10 Batch 2:  loss: 1.98457 accuracy: 0.5474\n",
      "Epoch 682, CIFAR-10 Batch 3:  loss: 1.94079 accuracy: 0.549\n",
      "Epoch 682, CIFAR-10 Batch 4:  loss: 1.85912 accuracy: 0.5482\n",
      "Epoch 682, CIFAR-10 Batch 5:  loss: 1.87778 accuracy: 0.5452\n",
      "Epoch 683, CIFAR-10 Batch 1:  loss: 1.89678 accuracy: 0.55\n",
      "Epoch 683, CIFAR-10 Batch 2:  loss: 2.0069 accuracy: 0.5476\n",
      "Epoch 683, CIFAR-10 Batch 3:  loss: 1.9723 accuracy: 0.5504\n",
      "Epoch 683, CIFAR-10 Batch 4:  loss: 1.86529 accuracy: 0.5498\n",
      "Epoch 683, CIFAR-10 Batch 5:  loss: 1.9214 accuracy: 0.544\n",
      "Epoch 684, CIFAR-10 Batch 1:  loss: 1.91137 accuracy: 0.5518\n",
      "Epoch 684, CIFAR-10 Batch 2:  loss: 1.98492 accuracy: 0.5506\n",
      "Epoch 684, CIFAR-10 Batch 3:  loss: 1.98279 accuracy: 0.5494\n",
      "Epoch 684, CIFAR-10 Batch 4:  loss: 1.86785 accuracy: 0.5502\n",
      "Epoch 684, CIFAR-10 Batch 5:  loss: 1.92176 accuracy: 0.5412\n",
      "Epoch 685, CIFAR-10 Batch 1:  loss: 1.9543 accuracy: 0.5508\n",
      "Epoch 685, CIFAR-10 Batch 2:  loss: 2.00825 accuracy: 0.5468\n",
      "Epoch 685, CIFAR-10 Batch 3:  loss: 1.99497 accuracy: 0.5482\n",
      "Epoch 685, CIFAR-10 Batch 4:  loss: 1.90896 accuracy: 0.5486\n",
      "Epoch 685, CIFAR-10 Batch 5:  loss: 1.93435 accuracy: 0.5402\n",
      "Epoch 686, CIFAR-10 Batch 1:  loss: 1.95594 accuracy: 0.552\n",
      "Epoch 686, CIFAR-10 Batch 2:  loss: 2.03101 accuracy: 0.548\n",
      "Epoch 686, CIFAR-10 Batch 3:  loss: 1.99047 accuracy: 0.5504\n",
      "Epoch 686, CIFAR-10 Batch 4:  loss: 1.87219 accuracy: 0.5502\n",
      "Epoch 686, CIFAR-10 Batch 5:  loss: 1.91491 accuracy: 0.5434\n",
      "Epoch 687, CIFAR-10 Batch 1:  loss: 1.9719 accuracy: 0.5492\n",
      "Epoch 687, CIFAR-10 Batch 2:  loss: 2.03443 accuracy: 0.546\n",
      "Epoch 687, CIFAR-10 Batch 3:  loss: 2.0023 accuracy: 0.5504\n",
      "Epoch 687, CIFAR-10 Batch 4:  loss: 1.8829 accuracy: 0.5522\n",
      "Epoch 687, CIFAR-10 Batch 5:  loss: 1.94017 accuracy: 0.5404\n",
      "Epoch 688, CIFAR-10 Batch 1:  loss: 1.93647 accuracy: 0.5462\n",
      "Epoch 688, CIFAR-10 Batch 2:  loss: 2.0305 accuracy: 0.5456\n",
      "Epoch 688, CIFAR-10 Batch 3:  loss: 1.98737 accuracy: 0.5504\n",
      "Epoch 688, CIFAR-10 Batch 4:  loss: 1.87276 accuracy: 0.5506\n",
      "Epoch 688, CIFAR-10 Batch 5:  loss: 1.95269 accuracy: 0.539\n",
      "Epoch 689, CIFAR-10 Batch 1:  loss: 1.94329 accuracy: 0.5476\n",
      "Epoch 689, CIFAR-10 Batch 2:  loss: 2.01835 accuracy: 0.5456\n",
      "Epoch 689, CIFAR-10 Batch 3:  loss: 1.98403 accuracy: 0.5522\n",
      "Epoch 689, CIFAR-10 Batch 4:  loss: 1.87269 accuracy: 0.5514\n",
      "Epoch 689, CIFAR-10 Batch 5:  loss: 1.9208 accuracy: 0.5418\n",
      "Epoch 690, CIFAR-10 Batch 1:  loss: 1.95262 accuracy: 0.5496\n",
      "Epoch 690, CIFAR-10 Batch 2:  loss: 2.03152 accuracy: 0.5462\n",
      "Epoch 690, CIFAR-10 Batch 3:  loss: 1.98832 accuracy: 0.5514\n",
      "Epoch 690, CIFAR-10 Batch 4:  loss: 1.87073 accuracy: 0.5516\n",
      "Epoch 690, CIFAR-10 Batch 5:  loss: 1.91602 accuracy: 0.5444\n",
      "Epoch 691, CIFAR-10 Batch 1:  loss: 1.9578 accuracy: 0.5486\n",
      "Epoch 691, CIFAR-10 Batch 2:  loss: 2.03223 accuracy: 0.5472\n",
      "Epoch 691, CIFAR-10 Batch 3:  loss: 1.99834 accuracy: 0.549\n",
      "Epoch 691, CIFAR-10 Batch 4:  loss: 1.90088 accuracy: 0.55\n",
      "Epoch 691, CIFAR-10 Batch 5:  loss: 1.9207 accuracy: 0.5426\n",
      "Epoch 692, CIFAR-10 Batch 1:  loss: 1.96148 accuracy: 0.5492\n",
      "Epoch 692, CIFAR-10 Batch 2:  loss: 2.03098 accuracy: 0.5494\n",
      "Epoch 692, CIFAR-10 Batch 3:  loss: 1.97962 accuracy: 0.5498\n",
      "Epoch 692, CIFAR-10 Batch 4:  loss: 1.88462 accuracy: 0.5494\n",
      "Epoch 692, CIFAR-10 Batch 5:  loss: 1.94151 accuracy: 0.5414\n",
      "Epoch 693, CIFAR-10 Batch 1:  loss: 1.9562 accuracy: 0.5466\n",
      "Epoch 693, CIFAR-10 Batch 2:  loss: 2.0323 accuracy: 0.5468\n",
      "Epoch 693, CIFAR-10 Batch 3:  loss: 1.97604 accuracy: 0.5492\n",
      "Epoch 693, CIFAR-10 Batch 4:  loss: 1.85681 accuracy: 0.5514\n",
      "Epoch 693, CIFAR-10 Batch 5:  loss: 1.94542 accuracy: 0.5434\n",
      "Epoch 694, CIFAR-10 Batch 1:  loss: 1.96926 accuracy: 0.5482\n",
      "Epoch 694, CIFAR-10 Batch 2:  loss: 2.05807 accuracy: 0.5496\n",
      "Epoch 694, CIFAR-10 Batch 3:  loss: 1.99568 accuracy: 0.5502\n",
      "Epoch 694, CIFAR-10 Batch 4:  loss: 1.93063 accuracy: 0.5504\n",
      "Epoch 694, CIFAR-10 Batch 5:  loss: 1.91149 accuracy: 0.5412\n",
      "Epoch 695, CIFAR-10 Batch 1:  loss: 1.95314 accuracy: 0.545\n",
      "Epoch 695, CIFAR-10 Batch 2:  loss: 2.05314 accuracy: 0.547\n",
      "Epoch 695, CIFAR-10 Batch 3:  loss: 1.98379 accuracy: 0.5484\n",
      "Epoch 695, CIFAR-10 Batch 4:  loss: 1.93004 accuracy: 0.5518\n",
      "Epoch 695, CIFAR-10 Batch 5:  loss: 1.95552 accuracy: 0.5434\n",
      "Epoch 696, CIFAR-10 Batch 1:  loss: 1.9893 accuracy: 0.5474\n",
      "Epoch 696, CIFAR-10 Batch 2:  loss: 2.07125 accuracy: 0.5434\n",
      "Epoch 696, CIFAR-10 Batch 3:  loss: 2.03038 accuracy: 0.5496\n",
      "Epoch 696, CIFAR-10 Batch 4:  loss: 1.93736 accuracy: 0.55\n",
      "Epoch 696, CIFAR-10 Batch 5:  loss: 1.91608 accuracy: 0.5406\n",
      "Epoch 697, CIFAR-10 Batch 1:  loss: 1.97414 accuracy: 0.5488\n",
      "Epoch 697, CIFAR-10 Batch 2:  loss: 2.04515 accuracy: 0.5458\n",
      "Epoch 697, CIFAR-10 Batch 3:  loss: 2.02084 accuracy: 0.55\n",
      "Epoch 697, CIFAR-10 Batch 4:  loss: 1.9316 accuracy: 0.5488\n",
      "Epoch 697, CIFAR-10 Batch 5:  loss: 1.93978 accuracy: 0.5406\n",
      "Epoch 698, CIFAR-10 Batch 1:  loss: 1.96883 accuracy: 0.5476\n",
      "Epoch 698, CIFAR-10 Batch 2:  loss: 2.05892 accuracy: 0.5474\n",
      "Epoch 698, CIFAR-10 Batch 3:  loss: 2.00047 accuracy: 0.5494\n",
      "Epoch 698, CIFAR-10 Batch 4:  loss: 1.93266 accuracy: 0.5512\n",
      "Epoch 698, CIFAR-10 Batch 5:  loss: 1.96025 accuracy: 0.543\n",
      "Epoch 699, CIFAR-10 Batch 1:  loss: 1.97051 accuracy: 0.5468\n",
      "Epoch 699, CIFAR-10 Batch 2:  loss: 2.05346 accuracy: 0.5482\n",
      "Epoch 699, CIFAR-10 Batch 3:  loss: 1.98351 accuracy: 0.5496\n",
      "Epoch 699, CIFAR-10 Batch 4:  loss: 1.91341 accuracy: 0.5494\n",
      "Epoch 699, CIFAR-10 Batch 5:  loss: 1.93525 accuracy: 0.5426\n",
      "Epoch 700, CIFAR-10 Batch 1:  loss: 1.98597 accuracy: 0.547\n",
      "Epoch 700, CIFAR-10 Batch 2:  loss: 2.03516 accuracy: 0.5468\n",
      "Epoch 700, CIFAR-10 Batch 3:  loss: 2.02309 accuracy: 0.5524\n",
      "Epoch 700, CIFAR-10 Batch 4:  loss: 1.9041 accuracy: 0.5534\n",
      "Epoch 700, CIFAR-10 Batch 5:  loss: 1.91439 accuracy: 0.5422\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5411079406738282\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3Xl8XFd5//HPo122bMm7HTu2kxCyECjEgQApicNWIGUp\nlKWUlkBLSyhQwlLoQpuUUihtISVAKFAa1iaUtT+WNmwJYQmBBAjZVye2492WZNmy1uf3x3Nm7tX1\nSBpZu/R9+zWv8dxz77lnRrOceeY555i7IyIiIiIiUDPdDRARERERmSnUORYRERERSdQ5FhERERFJ\n1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnU\nORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5nmZmtsHMXmhmF5nZX5rZO8zsDWb2YjM7\ny8xapruNwzGzGjN7vpldZWb3mlmnmXnu8tXpbqPITGNmGwuvk0smYt+Zysw2F+7DhdPdJhGRkdRN\ndwPmIzNbClwEvAbYMMrug2Z2O3A98A3gu+5+ZJKbOKp0H74InD/dbZGpZ2ZXAq8cZbd+oB3YC9xM\nPIf/y907Jrd1IiIix06R4ylmZr8N3A78A6N3jCH+RmcQnemvA787ea0bk08zho6xokfzUh2wHDgV\neDlwBbDdzC4xM30xn0UKr90rp7s9IiKTSR9QU8jMXgJ8HqgtFHUCvwZ2Aj3AEmA9cBoz8AuMmT0R\nuCC36UHgUuDnwMHc9sNT2S6ZFRYCfweca2bPdvee6W6QiIhInjrHU8TMTiKirfmO8a3AXwPfdPf+\nCse0AOcBLwZ+B1g8BU2txgsLt5/v7r+alpbITPE2Is0mrw5YBfwm8DriC1/J+UQk+dVT0joREZEq\nqXM8dd4NNOZufwd4nrt3D3eAu3cRecbfMLM3AH9MRJen26bc/7eoYyzAXnffUmH7vcCPzOyDwOeI\nL3klF5rZB939l1PRwNkoPaY23e0YD3e/lll+H0RkfplxP9nPRWbWDDwvt6kPeOVIHeMidz/o7h9w\n9+9MeAPHbmXu/w9PWytk1kjP9d8H7s5tNuC109MiERGRytQ5nhpnAs252z9299ncqcxPL9c3ba2Q\nWSV1kD9Q2Py06WiLiIjIcJRWMTVWF25vn8qTm9li4CnAWmAZMWhuF/BTd3/oWKqcwOZNCDM7kUj3\nWAc0AFuA77v77lGOW0fkxB5P3K8d6bht42jLWuBRwIlAW9q8H3gI+Mk8n8rsu4XbJ5lZrbsPjKUS\nMzsDOB1YQwzy2+Lun6/iuEbgycRMMSuBAeK1cIu73zKWNgxT/8nAE4DjgCPANuBGd5/S13yFdj0S\neCywgnhOHiae67cCt7v74DQ2b1RmdjzwRCKHfRHxenoYuN7d2yf4XCcSAY3jiTEiu4Afufv946jz\nFOLxX00EF/qBLmArcA9wp7v7OJsuIhPF3XWZ5AvwMsBzl29N0XnPAr4F9BbOn7/cQkyzZSPUs3mE\n44e7XJuO3XKsxxbacGV+n9z284DvA4MV6ukFPgK0VKjvdOCbwxw3CHwJWFvl41yT2nEFcN8o922A\nyDc/v8q6P1U4/mNj+Pu/p3Ds10f6O4/xuXVloe4LqzyuucJjsrLCfvnnzbW57a8iOnTFOtpHOe8Z\nwH8Dh0b422wF3gTUH8PjcQ7w02Hq7SfGDmxK+24slF8yQr1V71vh2Dbg74kvZSM9J/cAnwQeP8rf\nuKpLFe8fVT1X0rEvAX45wvn6gG8DTxxDndfmjt+S23428eWt0nuCAzcATxrDeeqBtxB596M9bu3E\ne84zJuL1qYsuuozvMu0NmA8X4KmFN8KDQNskns+A943wJl/pci2wZJj6ih9uVdWXjt1yrMcW2jDk\ngzpte2OV9/Fn5DrIxGwbh6s4bguwvorH+9XHcB8d+FegdpS6FwJ3FI57WRVtekbhsdkGLJvA59iV\nhTZdWOVxTRUehxUV9ss/b64lBrN+YYTHsmLnmPji8s/El5Jq/y6/osovRukcf1Xl87CXyLveWNh+\nyQh1V71v4bjfAQ6M8fn4y1H+xlVdqnj/GPW5QszM850xnvsyoKaKuq/NHbMlbXsDIwcR8n/Dl1Rx\njhXEwjdjffy+OlGvUV100eXYL0qrmBo3ER/OpWncWoBPm9nLPWakmGgfB/6osK2XiHw8TESUziIW\naCg5D/iBmZ3r7gcmoU0TKs0Z/W/pphPRpfuILwaPBU7K7X4WcDnwKjM7H7iaLKXoznTpJeaVfnTu\nuA1E5Ha0xU6KufvdwG3Ez9adRLR0PfAYIuWj5M1E5Osdw1Xs7ofM7KVEVLIpbf6Ymf3c3e+tdIyZ\nrQY+Q5b+MgC83N33jXI/psK6wm0nOnGjuYyY0rB0zC/IOtAnAicUDzCzWuJv/aJC0WHiNbmDeE2e\nBPwG2eP1GODHZvYEd981UqPM7E3ETDR5A8TfayuRAvA4Iv2jnuhwFl+bEyq16f0cnf60k/ilaC+w\ngPhbPJqhs+hMOzNbBFxHvI7zDgA3pus1RJpFvu1/TrynvWKM5/t94IO5TbcS0d4e4rmxieyxrAeu\nNLNfuPs9w9RnwJeJv3veLmI++73El6nWVP8jUIqjyMwy3b3z+XIhftIuRgkeJhZEeDQT93P3Kwvn\nGCQ6Fm2F/eqID+mOwv7/VaHOJiKCVbpsy+1/Q6GsdFmdjl2XbhdTS946zHHlYwttuLJwfCkq9g3g\npAr7v4TopOYfhyelx9yBHwOPrXDcZmBf4VzPGeUxL02x9550jorRK+JLydsZ+tP+IHB2FX/X1xba\n9HOgocJ+NcTPzPl93zkJz+fi3+PCKo/7k8Jx9w6z35bcPgdz//8MsK7C/hsrbHt34Vy7iLSMSo/b\nSRz9Gv3mKPfl0Rwdbfx88fmb/iYvAXanffYXjrlkhHNsrHbftP9vcXSU/Doiz/qo9xiic/lc4if9\nmwply8lek/n6vsjwr91Kf4fNY3muAP9Z2L8T+FMK6S5E5/JfOTpq/6ej1H9tbt8usveJrwCPqLD/\nacSvCflzXD1C/RcU9r2HGHha8T2e+HXo+cBVwH9P9GtVF110Gftl2hswXy5EZOpI4U0zf9lHdPTe\nSfwkvvAYztHC0T+lXjzKMWdzdB7miHlvDJMPOsoxY/qArHD8lRUes88xws+oxJLblTrU3wEaRzju\nt6v9IEz7rx6pvgr7P6nwXBix/txxVxfa9W8V9vnrwj7fG+kxGsfzufj3GPXvSXzJKqaIVMyhpnI6\nznvH0L6zGdpJvIsKX7oKx9RwdI73s0fY//uFfT88Sv2P4uiO8YR1jolo8K7C/h+q9u8PrBqhLF/n\nlWN8rlT92icGx+b3PQycM0r9ry8c08UwKWJp/2sr/A0+xMjjLlYx9L21Z7hzEGMPSvv1ASeM4bFq\nGstjq4suukzORVO5TRGPhTL+gOgUVbIUeA4xgOYa4ICZXW9mf5pmm6jGK8lmRwD4X3cvTp1VbNdP\ngb8tbP7zKs83nR4mIkQjjbL/DyIyXlIapf8HPsKyxe7+daIzVbJ5pIa4+86R6quw/0+AD+c2vSDN\nojCa1xCpIyVvNLPnl26Y2W8Sy3iX7AF+f5THaEqYWRMR9T21UPTvVVbxS6LjX613kKW79AMvcPcR\nF9BJj9OfMnQ2mTdV2tfMTmfo8+Ju4OJR6r8N+IsRWz0+r2HoHOTfB95Q7d/fR0khmSLF955L3f1H\nIx3g7h8iov4lCxlb6sqtRBDBRzjHLqLTW9JApHVUkl8J8pfu/kC1DXH34T4fRGQKqXM8hdz9v4mf\nN39Yxe71RBTlo8D9Zva6lMs2kt8v3P67Kpv2QaIjVfIcM1ta5bHT5WM+Sr62u/cCxQ/Wq9x9RxX1\nfy/3/5Upj3cifS33/waOzq88irt3EukpvbnN/2lm69Pf67/I8tod+MMq7+tEWG5mGwuXR5jZk83s\nL4Dbgd8tHPM5d7+pyvo/4FVO95am0ssvuvN5d7+jmmNT5+RjuU3nm9mCCrsW81rfl55vo/kkkZY0\nGV5TuD1ih2+mMbOFwAtymw4QKWHV+JvC7bHkHX/A3auZr/2bhdu/UcUxK8bQDhGZIdQ5nmLu/gt3\nfwpwLhHZHHEe3mQZEWm8yswaKu2QIo9n5jbd7+43VtmmPmKaq3J1DB8VmSmuqXK/+wq3v13lccXB\nbmP+kLOwyMyOK3YcOXqwVDGiWpG7/5zIWy5ZQnSKP8XQwW7/7O7/O9Y2j8M/Aw8ULvcQX07+iaMH\nzP2IoztzI/n66LuUbWboe9uXxnAswA9y/68HHl9hnyfl/l+a+m9UKYr7xTG2Z1RmtoJI2yj5mc++\nZd0fz9CBaV+p9heZdF9vz216dBrYV41qXyd3Fm4P956Q/9Vpg5n9WZX1i8gMoRGy08Tdrweuh/JP\ntE8mZlV4PBFFrPTF5SXESOdKb7ZnMHTk9k/H2KQbgNflbm/i6EjJTFL8oBpOZ+H2XRX3Gv24UVNb\n0uwITydmVXg80eGt+GWmgiVV7oe7X2Zmm4lBPBDPnbwbGFsKwlTqJmYZ+dsqo3UAD7n7/jGc45zC\n7QPpC0m1agu3TyQGteXlv4je42NbiOJnY9i3WmcXbl8/CeeYbJsKt4/lPez09P8a4n10tMeh06tf\nrbS4eM9w7wlXMTTF5kNm9gJioOG3fBbMBiQy36lzPAO4++1E1OMTAGbWRvy8eDExrVTe68zskxV+\nji5GMSpOMzSCYqdxpv8cWO0qc/0TdFz9SDub2ZOI/NlHj7TfCKrNKy95FZGHu76wvR34PXcvtn86\nDBCP9z5i6rXriRSHsXR0YWjKTzWK08X9oOJe1RuSYpR+pcn/vYq/Toym4hR841RM+6kqjWSGmY73\nsKpXq3T3vkJmW8X3BHe/0cw+wtBgw9PTZdDMfk2k1v2AGNBcza+HIjKFlFYxA7l7u7tfSUQ+/r7C\nLm+osK2tcLsY+RxN8UOi6kjmdBjHILMJH5xmZs8iBj8da8cYxvhaTNGnf6xQ9BZ33zKOdhyrV7m7\nFS517r7M3R/p7i919w8dQ8cYYvaBsZjofPmWwu3ia2O8r7WJsKxwe0KXVJ4i0/EeNlmDVV9P/Hpz\nuLC9hshV/jNi9pkdZvZ9M/vdKsaUiMgUUed4BvPwd8SbaN7Tqzl8jKfTG/MxSAPhPsvQlJYtwLuA\nZwOnEB/6TfmOIxUWrRjjeZcR0/4VvcLM5vvresQo/zEY7bUxE19rs2Yg3ghm4uNalfTe/Y9ESs7b\ngZ9w9K9REJ/Bm4kxH9eZ2Zopa6SIDEtpFbPD5cBLc7fXmlmzu3fnthUjRa1jPEfxZ33lxVXndQyN\n2l0FvLKKmQuqHSx0lBRh+hSwtkLx+cTI/Uq/OMwX+eh0P9A8wWkmxdfGeF9rE6EYkS9GYWeDOfce\nlqaAex/wPjNrAZ4APIV4nZ7D0M/gpwD/m1ZmrHpqSBGZePM9wjRbVBp1XvzJsJiX+YgxnuORo9Qn\nlV2Q+38H8MdVTuk1nqnhLi6c90aGznryt2b2lHHUP9vl5+utY5xR+qLUccn/5H/ScPsOY6yvzWoU\n53A+bRLOMdnm9HuYu3e5+/fc/VJ330wsgf03xCDVkscAr56O9olIRp3j2aFSXlwxH+9Whs5/Wxy9\nPpri1G3Vzj9brbnwM28l+Q/wH7r7oSqPO6ap8szsLOC9uU0HiNkx/pDsMa4FPp9SL+ajGwq3nzYJ\n57g59/+T0yDaalWaGm68bmDoa2w2fjkqvueM5z1skBiwOmO5+153fzdHT2n43Oloj4hk1DmeHU4p\n3O4qLoCRoln5D5eTzKw4NVJFZlZHdLDK1TH2aZRGU/yZsNopzma6/E+/VQ0gSmkRvzfWE6WVEq9m\naE7tq939IXf/P2Ku4ZJ1xNRR89F3CrcvnIRz/CT3/xrgRdUclPLBXzzqjmPk7nuA23KbnmBm4xkg\nWpR//U7Wa/dnDM3L/Z3h5nUvSvc1P8/zre5+cCIbN4muZujKqRunqR0ikqhzPAXMbJWZrRpHFcWf\n2a4dZr/PF24Xl4UezusZuuzst9x9X5XHVqs4knyiV5ybLvk8yeLPusP5A47tZ++PEQN8Si5396/m\nbv81Q6OmzzWz2bAU+IRy93uB7+Y2nW1mxdUjx+tzhdt/YWbVDAR8NZVzxSfCxwq33z+BMyDkX7+T\n8tpNv7rkV45cSuU53St5V+H2ZyekUVMg5cPnZ7WoJi1LRCaROsdT4zRiCej3mtnKUffOMbMXARcV\nNhdnryj5FEM/xJ5nZq8bZt9S/Y/n6A+WD46ljVW6H8gv+vDUSTjHdPh17v+bzOy8kXY2sycQAyzH\nxMz+hKGDMn8BvC2/T/qQ/T2GdtjfZ2b5BSvmi0sKtz9uZs8YSwVmtsbMnlOpzN1vY+jCII8EPjBK\nfacTg7Mmy38wNN/66cBl1XaQR/kCn59D+PFpcNlkKL73vCu9Rw3LzC4iWxAH4BDxWEwLM7sorVhY\n7f7PZuj0g9UuVCQik0Sd46mzgJjSZ5uZfcXMXjTSG6iZnWZmHwO+wNAVu27m6AgxAOlnxDcXNl9u\nZv9sZkNGfptZnZm9ilhOOf9B94X0E/2ESmkf+eWszzOzT5jZ08zs5MLyyrMpqlxcCvhLZva84k5m\n1mxmFxMRzcXESodVMbMzgMtym7qAl1Ya0Z7mOM7nMDYAV49hKd05wd1/yNB5oJuJmQA+YmYnD3ec\nmbWZ2UvM7GpiSr4/HOE0b2DoF74/M7PPFZ+/ZlZjZi8mfvFZwiTNQezuh4n25scovBH4blqk5ihm\n1mhmv21mX2TkFTHzC6m0AN8ws99J71PFpdHHcx9+AHwmt2kh8G0z+6NiZN7MFpvZ+4APFap52zHO\npz1R3g48lJ4LLxjutZfeg/+QWP49b9ZEvUXmKk3lNvXqidXvXgBgZvcCDxGdpUHiw/N04PgKx24D\nXjzSAhju/kkzOxd4ZdpUA7wVeIOZ/QTYQUzz9HhgeeHwOzg6Sj2RLmfo0r5/lC5F1xFzf84GnyRm\njyh1uJYBXzOzB4kvMkeIn6HPJr4gQYxOv4iY23REZraA+KWgObf5te4+7Oph7v5FM/so8Nq06RHA\nFcArqrxPc8U7iRUES/e7hnjcL0p/n9uJAY31xGviZMaQ7+nuvzaztwPvz21+OfBSM7sB2Ep0JDcR\nMxNA5NRezCTlg7v7NWb2VuBfyeb9PR/4sZntAG4hVixsJvLSH0M2R3elWXFKPgG8BWhKt89Nl0rG\nm8rxemKhjNLqoK3p/P9kZjcSXy5WA0/KtafkKne/YpznnwhNxHPh5YCb2d3AA2TTy60BHsfR09V9\n1d3/35S1UkQqUud4auwnOr/FzihEx6WaKYu+A7ymytXPXpXO+SayD6pGRu5w/hB4/mRGXNz9ajM7\nm+gczAnu3pMixd8j6wABbEiXoi5iQNadVZ7icuLLUsl/unsx37WSi4kvIqVBWb9vZt9193kzSC99\nifwDM/sV8A8MXahluL9P0Yhz5br7B9IXmHeRvdZqGfolsKSf+DI43uWsR5TatJ3oUOajlmsY+hwd\nS51bzOxColPfPMru4+LunSk96ctEx75kGbGwznA+TETKZxojBlUXB1YXXU0W1BCRaaS0iing7rcQ\nkY6nElGmnwMDVRx6hPiAeK67P6PaZYHT6kxvJqY2uobKKzOV3Ea8IZ87FT9FpnadTXyQ/YyIYs3q\nASjufidwJvFz6HCPdRfwaeAx7v6/1dRrZr/H0MGYd1J56fBKbTpC5CjnB/pcbmanVnP8XOLu/0IM\nZLyMo+cDruQu4kvJk9x91F9S0nRc5zI0bShvkHgdnuPun66q0ePk7l8g5nf+F4bmIVeyixjMN2LH\nzN2vJsZPXEqkiOxg6By9E8bd24kp+F5ORLuHM0CkKp3j7q8fx7LyE+n5xGN0A6O/tw0S7b/A3V+m\nxT9EZgZzn6vTz85sKdr0yHRZSRbh6SSivrcBt0/Eyl4p3/hcYpT8UqKjtgv4abUdbqlOmlv4XOLn\n+Sbicd4OXJ9yQmWapYFxjyF+yWkjvoS2A/cBt7n77hEOH63uk4kvpWtSvduBG91963jbPY42GZGm\n8ChgBZHq0ZXadhtwh8/wDwIzW088rquI98r9wMPE62raV8Ibjpk1AWcQvw6uJh77PmLg9L3AzdOc\nHy0iFahzLCIiIiKSKK1CRERERCRR51hEREREJFHnWEREREQkUedYRERERCRR51hEREREJFHnWERE\nREQkUedYRERERCRR51hEREREJFHnWEREREQkUedYRERERCRR51hEREREJFHnWEREREQkUedYRERE\nRCRR51hEREREJFHnWEREREQkUedYRERERCRR51hEREREJFHnWEREREQkUedYRERERCRR51hERERE\nJFHnWEREREQkUedYRERERCRR51hEREREJFHneARmtsjM3m9m95lZr5m5mW2Z7naJiIiIyOSom+4G\nzHBfBp6e/t8J7Af2TF9zRERERGQymbtPdxtmJDN7FHAr0Aec6+43THOTRERERGSSKa1ieI9K17eo\nYywiIiIyP6hzPLzmdN01ra0QERERkSmjznGBmV1iZg5cmTadlwbilS6bS/uY2ZVmVmNmrzezG82s\nPW1/bKHOx5nZZ81sq5n1mNleM/s/M3vRKG2pNbM3mdktZtZtZnvM7Otmdk4qL7Vp4yQ8FCIiIiLz\njgbkHa0L2EVEjhcTOcf7c+W9uf8bMWjv+cAAcLBYmZn9CXAF2ReRdqANeCbwTDP7LHChuw8UjqsH\nvgY8O23qJ/5eFwC/ZWYvO/a7KCIiIiKVKHJc4O7/4u6rgT9Pm37s7qtzlx/ndn8h8CzgdcBid18C\nrALuBzCzJ5N1jL8IHJ/2aQP+GnDgFcBfVmjK3xAd4wHgTbn6NwL/C3xi4u61iIiIiIA6x+PVArzR\n3a9w98MA7r7b3TtT+buIx/hHwMvcfVvap8vd/xF4b9rv7Wa2uFSpmbUAb0k3/9bd/83du9OxDxKd\n8gcn+b6JiIiIzDvqHI/PPuCTlQrMbClwfrr5nmLaRPJPwBGik/2c3PbfAhamsg8WD3L3PuD9x95s\nEREREalEnePx+bm79w9T9jgiJ9mB6yrt4O4dwE3p5pmFYwF+6e7DzZZx/RjbKiIiIiKjUOd4fEZa\nLW9Fuu4YoYMLsK2wP8DydL1jhOMeHqVtIiIiIjJG6hyPT6VUiaLGY6jXqthHSxuKiIiITDB1jidP\nKarcbGYrRthvXWH//P/XjHDcccfaMBERERGpTJ3jyfMLsuju+ZV2MLNWYFO6eXPhWIDHppkrKnnK\nuFsoIiIiIkOoczxJ3H0/8P108+1mVumxfjvQRCw88s3c9muAQ6nsz4oHmVkdcPGENlhERERE1Dme\nZO8EBomZKK4ys3UQ8xib2V8B70j7vTc3NzLufhD4QLr5D2b2BjNrTseuJxYUOWGK7oOIiIjIvKHO\n8SRKq+m9juggvxh4yMz2E0tIv5sYePc5ssVA8t5FRJDriLmOO9KxDxJzIr86t2/PZN0HERERkflE\nneNJ5u7/Djwe+DwxNVsL0AF8G3ixu7+i0gIh7t4LXECslHcr0cEeAP4fcC5ZygZEZ1tERERExsnc\nNSPYbGRmTwO+Azzo7hunuTkiIiIic4Iix7PX29L1t6e1FSIiIiJziDrHM5SZ1ZrZF83sWWnKt9L2\nR5nZF4HfAvqIfGQRERERmQBKq5ih0nRtfblNncTgvAXp9iBwkbt/bKrbJiIiIjJXqXM8Q5mZAa8l\nIsSPBlYC9cBO4AfAZe5+8/A1iIiIiMhYqXMsIiIiIpIo51hEREREJFHnWEREREQkUedYRERERCRR\n51hEREREJKmb7gaIiMxFZvYAsBjYMs1NERGZrTYCne5+wlSedM52jt/ylj92gJXLV5e3rVm/HoAH\n77kPgIceeig7wOKqvj4eksbmxnLR8uNWAlBnEWhv37Of4oED/YMA9PZmUxO7x/5WUwvA+hM2lssa\nm5tj/3QcwI5tDwJwpKsdgKVLlpTLDh85Eufxgbgv61aWy45bezwAPYNxvgfuebBcduIJpwCwal3s\ns3rV8nLZkUM9ADxp05mGiEy0xc3NzUtPO+20pdPdEBGR2eiOO+6gu7t7ys87ZzvHg30xRV13X295\nm9VFJ7X9wAEA+vv7y2WNTfUArFy5DIAVa1aUy3oGoo6e7uhMWq4r2dbaBkBn5yEAOjoOlsvq65ui\nDV1R9tCWrNNaUxsd2b6BgfK2I10dAKxaGZ3i2txf51BX1DuY9j+wp7Zc5v1xX1tXrgGgoaGhXNac\nOuE1qYO+c2fWsT/UEedj05mIzBRm9kZiju8TgCbgYne/bHpbdUy2nHbaaUtvuumm6W6HiMistGnT\nJm6++eYtU33eOds5FpHZx8xeBvwb8AvgMqAHuGFaGyUiIvOKOsciMpP8duna3R+e1pZMgFu3d7Dx\nHd+Y7maIiEyLLe+9YLqbcEzmcOc40hbqG+rLW+68/Q4ADhyInN7W1tZyWXd3FwCdHVE2OJjlDrd3\nRBpGy8JFAFjuYevtjZSL1iVRV01dlu6wd08cV1Mb2w6W0hgAG0wrE9ZlORotCyLPefHiFgAGyFYv\nrE25HP390a6Bnp5yWUOqf2FKoWhuzu7zA/ffDcDJDVF3DVku9a5duxCZYY4DmAsdYxERmZ00lZuI\nTDszu8TMHDg/3fbSJXf7WjNbbWafMLPtZjZgZhfm6lhjZh82sy1m1mtme8zsy2a2aZhztprZZWa2\nzcyOmNmdZvZmMzsxne/KKbjrIiIyw8zZyPGiFMnt781GOe7ZHpHSlkWLoyw3IG9RS0SFe4/E/nsP\n7yuXHT4UA+oaayMy29LSVC7buWMHACvXxmC4+sbsIV28JOpsa4sBdjt37s4a2BsD6/oGsgiwETNX\nNDdF/b3ZRBbU1ES9y5ZGXQsWZIPuelLUu6EhvuvUDh4pl+3bH9Hre26L202Ni8pldU1ZhFlkml2b\nri8ENgCXVthnKZF/3AV8GRgEdgGY2QnAD4nI8/eA/wKOB14MXGBmL3L3r5cqMrOmtN+ZRH7z54BW\n4K+Bp0zJp8WlAAAgAElEQVToPRMRkVllznaORWT2cPdrgWvNbDOwwd0vqbDbo4HPAK929/5C2UeJ\njvHfuPu7SxvN7CPAD4BPmdkGd+9KRW8jOsZXAS9391KE+t3AzWNpu5kNNx3FqWOpR0REZoY52zle\nsTLmAd61I5vLuPtwRIVbF8f0aw88sK1cduLGDUAWTa6pyTJOVqxYmY6LaPSRI1lkdiDt35/mNx70\nLNxbSj8+0t0JwLoNx5fLWhZFXR37smjywb0R2a5POcSDA1ldpSncamoi2ps+ywFo37cn6toTUeyD\n+7M669I8zJ0HY5+9e7I841NOPQWRWaQXeGuxY2xm64BnAg8B78uXufuPzey/gFcALwQ+nYpeSUSe\n/9JzLyZ332pmlwH/MGn3QkREZrQ52zkWkTlni7vvrrD9cen6enfvq1D+PaJz/Djg02a2GDgJ2Oru\nWyrs/8OxNMrdh8tpvomITouIyCyiAXkiMlvsHGZ7adqZHcOUl7a3pevF6Xq46Vo0jYuIyDw2ZyPH\nPT2R+tCRmz6t82CkNwymadTyK8mV1KaUhvr6bLCapWnUdqTBd319WXCqNB1cTapz7+695bKGhhhY\n15VWyDvplGww3LrTHwVk07cBbD9yGIBDnbEa3t59Wdsb0nJ5pXO7ZWkVfb0xqK99b7Svtjk31VwK\npPlg7NPXnw0A3LVzy1H3X2QG82G2l14oq4cpX1PYrzNdrxpm/+G2i4jIPDBnO8ciMm/8Il3/ppnV\nVRisd366vhnA3TvN7H5go5ltrJBa8ZsT1bAz1rZy0yydBF9EZL6as53j/WlwW0d7e3nb0iUxDVqN\nRXS4rjZbgGN/WrCjqTmiyYvbsgVC2tN0aKVxO/V1WVS5/UAMfi+Nnes80FkuW9QaWSuL0kA+ctHe\nndu2ArDt/gfK23bt2B7tS1O69RzOBv4tWbYizpM+9j23eEhDc9TfQ7SrYXE21Vx3d6RoHj4cBzYu\naC6XHTySPTYis5W7bzOzbwPPAN4E/EupzMzOBl4OHAC+kjvs08AlwHvMLD9bxfGpDhERmafmbOdY\nROaV1wI/Av7ZzJ4J/JxsnuNB4FXufjC3//uAFwAvA04xs2uI3OWXEFO/vSAdJyIi84wG5InIrOfu\n9wNnEfMdnwK8FXg28L/AOe7+tcL+3US6xeVErvLF6fY/Au9Ju3UiIiLzzpyNHHcfjsFtDQ3ZgLe2\n1uVRdijmO+7uPlQu6+yIz8GFgwsAqO/KBuvVpsF5dWk8UH9/FlCqqSsN4IvzrF2/vlzW2BTbjlu3\nDshW0QO45867ANi9MxuAv3BBCwAD/X1HtZ2UAtLQEGkRBw9lQTBPg/raFi8DYDC38t+RQwPpPDFQ\ncFFbW7lsw8aViMwk7r55mO1WaXthn+3ARWM4VzvwxnQpM7PXpP/eUW1dIiIydyhyLCLzkpkdV2Hb\n8cA7gX7g60cdJCIic96cjRyXFqpraswGoG3fHgPeetJqdotbW8plrcuWArBsRVwvaltYLhtMkeL2\njhjAZoPZwLr+rqhr776IzJ7ymDPKZX3dMaBu69YYfLegNZvK7cCBNACwKRs8V5pari+trNeaGxS4\nvz1moSot3Nc/kEWHGxbFQMOBwWjn9vuylf/274k2H+qIthzpytZQWLJgzv75RarxJTOrB24C2oGN\nwG8DC4iV87ZPY9tERGSaqHckIvPVZ4A/AF5EDMbrAn4KfMjdvzydDRMRkekzZzvH/Wlutf7+LMrb\ndTCmXVuxOtYKsNxUbqXp2VqXRk7u4bQgB0BdWhhk7fHr0pZcNspA1HGoM+UsL86iw7tStLfnSERt\n9+7JFggp5TjX5KLQpYVH6lIEuScXHW5cELnQR3p6AVizfl25bNnamObtlhtvBODhLQ+Vy6wmIuCD\nvZF7XN+Y5VLnp4oTmW/c/SPAR6a7HSIiMrMo51hEREREJFHnWEREREQkmbNpFa1LY5Ba54FsurZF\ni2KA28KFkfrQ0JhNlXb4cKQ59KS0hcFcOobVxneI0iC/wcGBcllpQN3CNNjuSFd2voOd8f9S8sbe\nnfvKZYtbIn1jYUqXAOhPU7ANEoP89u3N0jB6euPk9SnlYsmybEq2gztiwN/h3Q/Hvrkp6uobo/7B\ngahzaVppD2DF2g2IiIiISEaRYxERERGRZM5GjkvR3uUrskjpkTQAbVdaeGNxbkEMS98TamsiMtu2\nPJtGrbs7osk7duwBYEFzNj3c3j37gWxqthUrsoU1DnXFYiO1tXXpHD3lssYUtW7K1WUWMebdu3fF\nPvVZWUtLRID703Rte3ZkU7LV9cT+e3dEGx7c1lEue8RpK4ecr6enu1xWY7WIiIiISEaRYxERERGR\nZM5GjvvSQh+Hj2TR2j17IvLb0RVLL5eWhQZYs2YtAO3tkXs8mK0QzdIlsTBIV4oEL1iwuFw2MNA5\nZFvXwWx6tMOHI+LcujgiwMuWLc+VRV7w3lxe8Zo1sbx0S8qJ7unJcqLXrIvFvO648z4AdmzLlp2u\n7Y/7tTVFjg91ZVPA1ddFHauWpynqutvLZdsfyhYLERERERFFjkVEREREytQ5FhERERFJ5mxaRVta\n6e6+lIYA0HUw0ikG0jRtB/ZmKQaNDWnAWkrHKE3DBtDaGnUdtyZSGwY8m+Zt0aI0hVtKodi6LUtV\n6E+r39U3xMC3hS3ZtG3rNsQKdwdTmwA62qM97V2xkt/qVavLZZ2pbNvWWP1u58O7ymXeF4MCu1MG\nSWNjfiBfrJA30BOFDTVZKsn+vdm5RURERESRYxERERGRsjkbOS7JL7LR2BgLdixenKKpA9mou94j\nETEe8BjMVor2ArS3x0C31kUx6G7B4pZyWV1NPIQtC6OsPrewSOuSiDj390QUemFrdlzb6phibuX6\ndeVtu3bEILteIuK8c+eOctmWe+4B4PChGBTYlIsO79lfWp0k2rxoUXaeJUvjvvZ0ROT5cFd2nw9n\na5mIzGtmdi1wnrvbaPuKiMjcpsixiMgkuXV7Bxvf8Q02vuMb090UERGpkjrHIiIiIiLJnE2r+PkN\nNwHQcyib83fXrpgPuK0ttrW0ZPMV9/fHtq7uGAy3IreyXm9vDGY70hNzGDf0NWXHeaRj1NfF8Sec\ncHy5bJAYpPfTH94GQF9/1pZD/VG2JDf3cU1jDJbbsSPSKW658aZy2Yo0KHDDuqi/8+DhrH1d0a72\ngX0ALF2eSyVpijoHe6LNd/7i9nLZsnVZSofIbGFmTwDeAvwmsBzYD/wa+IS7fyHtcyHwXOBxwBqg\nL+1zhbt/NlfXRuCB3O1stC1c5+6bJ++eiIjITDRnO8ciMveY2WuAK4AB4H+Ae4CVwFnA64AvpF2v\nAG4HfgDsAJYBzwE+Y2anuPs7037twKXAhcCG9P+SLVW26aZhik6t5ngREZlZ5mzneN+eGETXeaC7\nvK11cUSKH9q6FYCNG04sl/WngXi1tTGorT8X5e3rj5FrpQF93d1ZnWc/4QkAbN8WU6zddXcWmd27\nLyK5e/fEdefe/eWy/bsjit2YploDwGIs0O2/+jUALc1ZBLg0PHDfrt2xa302YHDN2og+Ny+Mdh63\nbmm5rDS+6Ne33QtAx6GOctnpx52FyGxhZqcDHwE6gae4+22F8vxPIWe4+32F8gbgW8A7zOyj7r7d\n3duBS8xsM7DB3S+ZzPsgIiIz35ztHIvInHMR8Z71rmLHGMDdt+X+f1+F8l4z+zDwVOBpwKcnolHu\nvqnS9hRRPnMiziEiIlNnznaOTz71dADa9+4rbxvoi8jqYIrQrl67plx2pCdyePcfiP1r6xvKZQta\nlwDQmBb86EpTuwFc+73vxz4tEVU+2JktrDHQH9OmbThxAwA1uanTPC0kcu8d95a31dXFn6O+NvKE\nFzRluc2W6uo9EvnFxx2XBclWpsjx1vuHHg9wy89uBeDu2x+MtjwiO+6RjzgJkVnkien6W6PtaGbr\ngbcTneD1QHNhl7UT2zQREZkr5mznWETmnLZ0vX2knczsROBGYAlwPXAN0EHkKW8EXgk0Dne8iIjM\nb+oci8hsUVrvfS1w5wj7vZkYgPcqd78yX2Bmv0d0jkVERCqas53jVatiKrYFDVmKwa9/GYPljkvp\nFItas19aW+tjsN6K46OsZWE2GG7P7khlHByIlIYjXdmAvL07dwGwZFkaBJdbX6u/L00PdyhSLWpy\nD/fChbGK3dLcVG4H9seAvYHBSKFoaM7SKgbSOevro47GpizwNZhW+htMYwjvu39rueyuO7ek45pS\nO1eXy7oOHUFkFrmBmJXi2YzcOX5Euv5ShbLzhjlmAMDMat19wtaOPGNtKze994KJqk5ERKaAFgER\nkdniCqAfeGeauWKI3GwVW9L15kL5bwF/PEzdpcEJ68fdShERmdXmbOS4fX9Mefbw1t3lbV1psYya\n3Wmat4Nd5bKWthhst/6EGDy37aEs+lqTlgUY7I2AUk1NNo1ab28sArJz514A+vr7ymVmEdFtSgPr\nmppaymW1tfHQr1q1qrytoz1+NV6+aiUAxx2XDRg8sC0WBhlI57PBLES9e3tMC7ftwdjn/vsfzj0S\nETlfsSYixmvXP6Jc0jth8TGRyefut5vZ64CPAr8ws68R8xwvIyLKB4HzieneXgX8t5l9ichRPgN4\nFjEP8ksrVP9d4MXAl83sm0A38KC7f2Zy75WIiMw0c7ZzLCJzj7t/3MxuBd5KRIZfAOwFbgE+kfa5\nxczOB/6BWPijDvgV8EIib7lS5/gTxCIgLwP+Ih1zHaDOsYjIPDNnO8d790TEePvWbGB7Q0NEbtN6\nH+zZlS2IsXdvRG1L0eGt27LI8fFpmeXt23YCsGBBlqvcvi+i0PUNsW3xkmxJ6hXLYnD90rZWADpy\nkeqGhnjoay2LQre2xn7LV0X+ctuS1nLZwKHIOT6YppF7ePvOctnDKaq8Z3fkP1tNFlU+bn0sN73x\n5EcCsPb4jeWyjq52RGYbd/8J8KJR9vkxMZ9xJVbckPKM/ypdRERkHlPOsYiIiIhIos6xiIiIiEgy\nZ9MqWloipaG5eW952/IVka7Q1RGr4TU2ZHe/Nk351tEeZatXHlcuW5amaes6dAgAI0uFqKuPKdVK\nK+OdcvrJ5bK21hjk190Z6Qv1ua8ig309ABzOjYprbIy6Dh/sBKA9lx6xY1fcj4e3xWC79gNZSkhP\nd2+0K9V/3Pqs7etOiAF4q9fGIPyenp5yWWm1PREREREJihyLiIiIiCRzNnLcdTCmPOvtzaKjTQsi\n4rtze0RyW3MLcFhdAwAHO2PQXFtbNrDOPb5DLGqJbc35KdlqIuK8eFFsW79xbbmsuXkhAP1LIoJ8\nsONguexQGmBHLnqd1v7gSE+0YcsDD5XL7rsrBgh2tUf0urevN7uzFtHnE04+AYCzz/nNclFNmjKu\nMbVl165sarslS9sQERERkYwixyIiIiIiiTrHIiIiIiLJnE2raFm4DIAVK7LBac2NkRax7viYt7hn\nIFvNrq4hBsP190WKQl9/NlBu545YgW7r1khtWLx4SbnsSHcM4DvzzFMBWLCgqVy2bOUKAHZtj9SO\nPXuywYEN6Xx1tdngvubmmCu5ry/2v/++LK3i8OFIwygtztfcsKBctnxNpIdseuKTAHjkGb9RLtty\nz51xvwaiTqsZLJcN9GuJPBEREZE8RY5FRERERJI5Gzleu2EVAI866/Tytp9e9z0AFqYBdQubsijq\nkZ5YNq+rKwbDNS3IBt3V18egu1WrIgrd3Z1Nh1afIsD1DTGgrzdN9waw5a67Abj/vvsAGOzNBtF5\nOne/9Wfb0leVu+6+N25nzaM1rbbXPxj7r99wYrnsUY+JSPHSFREtp87LZb0pCr1wYQzIW7l6Zbns\nrtvvQkREREQyihyLiIiIiCRzNnJ8/713AHDeCc8pb1uxfDUAux7aDsDChuZyWffhmGbtUFcswGG5\nXOCFzZHf29raCsCCXFT58OGINB/pjgjt1vu3lMs6DkY+ckOKKi9Ni4kA7Nm1H4Ce7ixyvHXn9nTu\niEaXIsIAS5ZHVLilNfKmFy1uLZctXRI5xwNEqLmxsaFctnzFqnS+XQCsSRF1gEf/xmMRERERkYwi\nxyIiIiIiiTrHIjJjmNlGM3Mzu7LK/S9M+184gW3YnOq8ZKLqFBGR2WPOplXs37kTgJ9879vlbatW\nrAHgYEtMt2aWfTdYktIVHvJIbTjc1VEuq0kr0JVW2+vrywa89fXG4LymupgWblFz9pAO1MT/Fy2O\nFfL6+7KBfHv2xPRwW+7bVt7W3R91PO1ZFwCwYeNJ5bKGNEXchpNi26GubLW9rs5IBVnYEukedbXZ\ndHK9PYOpnXG9ZMmKctnJp2fT3ImIiIjIHO4ci8i88BXgBmDHdDdERETmhjnbOa7xiO5uueeO8ra+\ntGDHwztiMY+1G9eXy5rrYrq25gYDsigugPfHoLl+os5BzwbrDQ5GRHb3jhjwtvyULNrbeSQW7ti9\nazcAvUcOl8taFkU0uWlBNihw1bJoT01dDKirb2wslzU1x1RsnZ0RMW4/sK9c1prqalkUg/Q62rOo\n96GDMbVcb09MIzfYn5vmrTcbDCgyG7l7B9Ax6o4iIiJVUs6xiMxIZnaqmX3VzPab2SEz+6GZPbOw\nT8WcYzPbki6Lzez96f99+TxiM1tlZv9hZrvMrNvMfmlmr5yaeyciIjPVnI0cd6Zp1DraO8vbWhfH\ntGsLF6almy1bZWPf7ojELmqL3OPDuw+Uyw7saQfAauK7RE1aFASg62BEci1Fmk88YWO5rK8ntvUe\niahtS+uicllNmiru5NMfUd52/AmPBMDTGtE9vVl0uC6lK+/bG1Hozo72ctngmrUA1Kap6QbSUtEA\ng4M96XGIqeP27t5ZLuvJLUoiMsOcAPwEuBX4d2AN8FLgW2b2cne/uoo6GoDvAUuBa4BO4AEAM1sG\n/Bg4EfhhuqwBPpr2FRGReWrOdo5FZFY7F/gXd39baYOZfYjoMH/UzL7l7p3DHh3WALcD57n7oULZ\ne4iO8WXufnGFc1TNzG4apujUsdQjIiIzg9IqRGQm6gD+Pr/B3X8OfA5oA36nynreUuwYm1k98PvA\nQeCSYc4hIiLz1JyNHO/bF5+HtTXZ4LmurkirWLEqUid6urO0gkNppbo1J50CQHNrllZx9213R111\n8XA1NWZpFfRHKsOBfZG+sHPnnnJRa1sMoluQplhb2JKlVRxMU7G1tmUr3a1YGSvo1dSngXKepUcc\nPhzt6WiPdJH6+my6tvaOtKpfXQwmbF2UrZBnFnUtKa3OV2vlsq6DWWqGyAxzs7sfrLD9WuCVwOOA\nT41SxxHglgrbTwUWANenAX3DnaMq7r6p0vYUUT6z2npERGRmUORYRGaiXcNsLyXNtw5Tnrfb3b3C\n9tKxo51DRETmoTkbOe7sjKho6+LF5W0H0+C5RW3N6XYWNOpLQeS2FMldtnRZuWxZWjhjQYr8Ni/I\n6uw4EBHdG6/7IQA7d+4tl7UsivP09ERUub9joFxWGtw3OJh9dncfiQhwc00MFNybpoADaGhamf4X\n+zc2Z1Ho+oaIjtvgQKoz+86zsCXuz5IVC9J9z447ciibWk5khlk1zPbV6bqa6dsqdYzzx452DhER\nmYcUORaRmehMM1tUYfvmdP2LcdR9J3AYeKyZVYpAb66wTURE5gl1jkVkJmoF/ja/wczOIgbSdRAr\n4x0Td+8jBt0tojAgL3cOERGZp+ZsWsXyVZGG0LE/myv44KFIWxhMY9I6O7LxPuvWxlzBC2rj+0Jf\nXzYYbsWyGPzW2BQD8QYbWsplR/bF/MHHbTgegDtv/WW5rONADABcvmIJAD6Q/crb75ECUTeQrVK3\nY+v22P+41nS+bNBdW1v8v9aizu7u3BzItW0A1NdHukfXwaztrSk9pK4pBukN9PaUy7oOjjYTlsi0\n+QHwx2Z2NvAjsnmOa4A/rWIat9H8FfA04E2pQ1ya5/ilwDeB542zfhERmaXmbOdYRGa1B4DXAu9N\n143AzcDfu/v/jbdyd99rZucA/wg8FzgLuAu4CNjCxHSON95xxx1s2lRxMgsRERnFHXfcAbBxqs9r\nlQdzi4jIeJhZD1AL/Gq62yLzXmlBmjuntRUiYSzPx41Ap7ufMHnNOZoixyIik+NWGH4eZJGpUlrF\nUc9FmQlmw/NRA/JERERERBJ1jkVEREREEnWORUREREQSdY5FRERERBJ1jkVEREREEk3lJiIiIiKS\nKHIsIiIiIpKocywiIiIikqhzLCIiIiKSqHMsIiIiIpKocywiIiIikqhzLCIiIiKSqHMsIiIiIpKo\ncywiIiIikqhzLCJSBTNbZ2afNLOHzazHzLaY2WVmtmSM9SxNx21J9Tyc6l03WW2XuWcino9mdq2Z\n+QiXpsm8DzL7mdnvmtnlZna9mXWm581nj7GuCXmPnQh1U31CEZHZxsxOAn4MrAS+BtwJPAH4c+BZ\nZnaOu++rop5lqZ5HAt8DrgJOBV4FXGBmT3L3+yfnXshcMVHPx5xLh9neP66GynzwN8BvAF3ANuL9\nbMwm4Tk9Luoci4iM7iPEm/Yb3f3y0kYzez9wMfBu4LVV1POPRMf4A+7+5lw9bwT+LZ3nWRPYbpmb\nJur5CIC7XzLRDZR542KiU3wvcB7w/WOsZ0Kf0+Nl7j5V5xIRmXXM7ETgPmALcJK7D+bKFgE7AANW\nuvuhEepZCOwBBoE17n4wV1aTzrExnUPRY6loop6Paf9rgfPc3SatwTJvmNlmonP8OXd/xRiOm7Dn\n9ERRzrGIyMiemq6vyb9pA6QO7o+ABcATR6nnSUAz8KN8xzjVMwhck26eP+4Wy1w2Uc/HMjN7qZm9\nw8zebGbPNrPGiWuuyKgm/Dk9Xuoci4iM7JR0ffcw5fek60dOUT0yv03G8+gq4D3AvwLfBB4ys989\ntuaJjNmMe29U51hEZGSt6bpjmPLS9rYpqkfmt4l8Hn0NeC6wjvhV41Sik9wGXG1mzx5HO0WqNePe\nGzUgT0RkfEr5muMdwDFR9cj8VvXzyN0/UNh0F/BXZvYwcDkxgPRbE9s8kTGb8vdGRY5FREZWilq0\nDlO+uLDfZNcj89tUPI8+QUzj9tg0IEpkMs2490Z1jkVERnZXuh4u3+3kdD1cvtxE1yPz26Q/j9z9\nCFAaNLrwWOsRqdKMe29U51hEZGSleTufmaZcK0tRtXOAbuCGUeq5Ie13TjEal+p9ZuF8IpVM1PNx\nWGZ2CrCE6CDvPdZ6RKo06c/psVLnWERkBO5+HzHN2kbgzwrFlxKRtU/n5980s1PNbMhKUe7eBXwm\n7X9JoZ7Xp/r/T3Mcy0gm6vloZiea2dpi/Wa2HPjPdPMqd9cqeTIhzKw+PRdPym8/luf0ZNMiICIi\no6iwtOkdwNnEnMR3A0/OL21qZg5QXFyhwvLRNwKnAc8Hdqd67pvs+yOz20Q8H83sQiK3+DpiAYb9\nwHrgOUTu58+BZ7h7++TfI5mtzOwFwAvSzdXAbwH3A9enbXvd/a1p343AA8CD7r6xUM+YntOTTZ1j\nEZEqmNnxwN8TyzsvI1Zt+ipwqbvvL+xbsXOcypYCf0d8oKwB9hEzAvytu2+bzPsgc8d4n49m9mjg\nLcAm4Dhi0NNB4DbgC8C/u3vv5N8Tmc3M7BLi/Ww45Y7wSJ3jVF71c3qyqXMsIiIiIpIo51hERERE\nJFHnWEREREQkmVedYzPzdNk4DefenM69ZarPLSIiIiLVmVedYxERERGRkdRNdwOmWGkVlr5pbYWI\niIiIzEjzqnPs7qeOvpeIiIiIzFdKqxARERERSWZl59jMlprZK83sS2Z2p5kdNLNDZna7mb3fzI4b\n5riKA/LM7JK0/UozqzGz15vZjWbWnrY/Nu13Zbp9iZk1mdml6fzdZrbbzP7LzB55DPenxcxebGaf\nM7Nb03m7zexeM/uYmZ08wrHl+2Rm683s42a2zcx6zOwBM/sXM1s8yvnPMLNPpv2PpPP/yMxea2b1\nY70/IiIiIrPVbE2r+CtiZZ+STqCZWIb1NOAVZvZ0d79ljPUa8GViKdcBYrWgShqB7wNPBHqBI8AK\n4GXA88zs2e7+gzGc90Lg8tztg8QXl5PS5eVm9gJ3/84IdfwG8Elgae74jcTjdJ6ZPdndj8q1NrPX\nA/9G9kXpENACPDldXmpmF7j74THcHxEREZFZaVZGjoHtwHuBM4FF7t5KdFjPAv6P6Kh+3syOWrp1\nFC8kli18HbDY3ZcAq4h1wvMuAh4DvBJoSed/HHAzsAD4gpktGcN59xGd4ycDbe6+GGgiOvqfAxam\n+7NwhDquBH4JPDod3wL8EdBDPC6vKR5gZs9P5+0mvnCscvcW4ovGM4kBjJuBD4zhvoiIiIjMWnNu\n+WgzayQ6qacDm939ulxZ6c6e4O5bctsvIVsb/E/d/WPD1H0l0SEGeIW7f65Qvhy4k1gT/J3u/g+5\nss1EtLnimuIj3B8DrgGeDlzo7p8qlJfu023AJnfvKZRfDrwe+L67PzW3vRa4D9gAvNDdv1Lh3CcA\nvya+eKx39x3VtltERERkNpqtkeNhpc7ht9PNc8Z4+D4iNWE0DwKfr3DuvcC/p5u/O8ZzV+Tx7eUb\n6eZI9+f9xY5x8tV0fUZh+2aiY7ylUsc4nfsB4AYi/WZzlU0WERERmbVma84xZnYqERE9l8itbSFy\nhvMqDswbwc/dvb+K/a7z4UPu1xEpCmeYWYO791ZzYjNbB7yBiBCfBCzi6C8vI92fnw2zfXu6LqZ5\nPLlUp5ntHKHe1nR9/Aj7iIiIiMwJs7JzbGYvAz4NlGZSGAQ6iPxaiI7ywnQZiz1V7re9irJaokO6\na7TKzOw84OtEu0s6iIF+EDnAixn5/gw3eLBUR/FvvSZdNxB51aNZUMU+IiIiIrParEurMLMVwMeJ\njvHVxGCzJndf4u6r3X012QCysQ7IG5iIJo5p55gq7bNEx/g7RCS82d3bcvfnzcdS9yhKf/uvuLtV\ncUKBPqAAACAASURBVLlkAs8tIiIiMiPNxsjxs4mO5O3Ay919sMI+1URCx2Ok9IZSRHYAOFBFXU8C\n1gH7gecPM2XaZNyfUkT79EmoW0RERGRWmnWRY6IjCXBLpY5xmt3hqcXtE+y8KspurTLfuHR/7h5h\nLuGnV92y6v0kXZ9iZo+ahPpFREREZp3Z2DnuSNdnDDOP8WuIAW2TaaOZ/V5xo5ktBf4k3fzvKusq\n3Z+TzaypQp3PBM4/plaO7LvAQ+n/H0hTu1U0xjmbRURERGat2dg5/g7gxNRkHzSzNgAzW2xmbwM+\nTEzJNpk6gI+b2SvMrC6d/zFkC5DsBj5SZV0/Ag4TcyN/2szWpPqazezVwJeYhPuTVst7A/FYPgO4\nxszOLn3hMLM6M9tkZu/l6EVQREREROakWdc5dve7gMvSzdcDB8xsP5Gz+z4iIvrRSW7GFcTiGJ8B\nusysA/gVMTjwMPBid68m3xh3bwf+Mt18MfCwmbUTS2L/B3AvcOnENr987v8hVtHrJVJRbgAOm9le\nYpaLnwNvB9om4/wiIiIiM82s6xwDuPubifSFXxDTt9URSye/CbgAqGau4vHoIVId/p5YEKSBmAbu\nKuBMd//BWCpz9w8SS1eXosh1xEp7f0fMRzzcNG3j5u7/CZxCfOG4jXjsWolo9feBtxLzSIuIiIjM\neXNu+ejJlFs++lJNbSYiIiIy98zKyLGIiIiIyGRQ51hEREREJFHnWEREREQkUedYRERERCTRgDwR\nERERkUSRYxERERGRRJ1jEREREZFEnWMRERERkUSdYxERERGRpG66GyAiMheZ2QPAYmDLNDdFRGS2\n2gh0uvsJU3nSOds57mvfFtNwmJW3WW3c3VK4fKC/r1w2ODAQ29J1T3dvucw9ttXVx/GNTQ3lspqa\nqK2/f/CoNpTq6u+La6vJ2tLQUFdsHoODno6L654jWRv6U12NDfUA1NfV5s4U+9c3RrtqcmVm8f9B\n708nyc1OkmYqaVi6MdcKEZkgi5ubm5eedtppS6e7ISIis9Edd9xBd3f3lJ93znaOGYzOJDUVMkdq\natNV1lEcHBxM17FtMDfFXak/OdibOtO5Hm1d3dCH0OzofmapJh/MOtCljnNdbda+0rGDqe39/f1H\n1WLlfXN3J92frMm1+cJ0wqPbNzh4dIdeRCbMltNOO23pTTfdNN3tEBGZlTZt2sTNN9+8ZarPq5xj\nEREREZFEnWMRmffM7Foz04pIIiIyd9MqBlOesOXSI0oJBV5KdMilXFhNPBSDA5Hn29ubpTSUVhEs\nfXbWWJarXMq5KKUr1NbXHXVclu/gR5UNej61IdpTSnfIr15YX1dTaHKWHlGTcpm99Nk+JLXDhtTl\ng0e3QUQmx63bO9j4jm9MdzNkAmx57wXT3QQRmSKKHIuIiIiIJHO2c+yDftQlIreOpX/ulC81NTVx\nqaulpq6WvoHB8qWnPy7uhrtFZLZwcfeIxGanKV+y3ax8Ke0/MEB26R9koH+w3N7aGssutTXU1taU\n255XqrOmNi7OQHYZ7McH+zEihjw4MFC+lNssMouY2RPM7Goz225mPWa2w8yuMbOX5Pa50My+ZGb3\nm1m3mXWa2Y/M7BWFujamdIrz0m3PXa6d2nsmIiIzwZxNqxCRucfMXgNcQcy/8j/APcBK4CzgdcAX\n0q5XALcDPwB2AMuA5wCfMbNT3P2dab924FLgQmBD+n/JlirbNNx0FKdWc7yIiMwsc7Zz7ANDc4Fj\nY8q7LeX55vJvS9OnleYirsnNSdzbP3R6t9I8xAA9vTH/3sIFTbFPmqINoDYlCJembbParM5SwLY0\nB3JeX8p3HjpdW9RVzl72o3OHjdKUbrk6S7sNHh0h1lRuMpuY2enAR4BO4CnufluhfF3u5hnufl+h\nvAH4FvAOM/uou29393bgEjPbDGxw90sm8z6IiMjMN2c7xyIy51xEvGe9q9gxBnD3bbn/31ehvNfM\nPgw8FXga8OmJaJS7b6q0PUWUz5yIc4iIyNRR51hEZosnputvjbajma0H3k50gtcDzYVd1k5s00RE\nZK6Ys51jqylNfTZkfeZ0VZpiLUs/8P/P3p3H6V2V9/9/XfcyW5bJRkhIwBAQEgVZYkFFNjdE3L5q\nRa39Cta2tu5ov0WwFdq6/NS6FLWtpRarVrS41qUispSlFA0IDQSBkGFJQsg62Wa5576v3x/nfJa5\nc89kksxkZu55P33M4zNzzvmcz7nD7cyZa65zTpIO4elRclldupVbXL+Y6/KxR7sAOGbZMQC0lbKj\npdMt2ZLT7XzvPmsNTs1L6oq5NIzs9LxafF3ZWsqkrBjvy6+yrH9Oo3QMkUliVryuG66RmS0F7gJm\nA7cC1wPdhDzlJcDbgNYxG6WIiExqTTs5FpGmsz1eFwEPDtPuEsICvIvd/Zp8hZm9mTA5FhERaahp\nJ8dJpHVQpLQ6eKGb5w7gSA7HqFaqe91Xi5HfNAKciyon7Xds3QZAx4KFWV0ajR56fPnIdtK8UNx7\nh71srLZXXRIVtthBPqpcr5pbMKjIsUwydxJ2pTif4SfHx8brdxvUnT3EPVUAMyv6oBWtB+eERZ2s\n1OERIiKTStPucywiTefvgQHgL+LOFYPkdqvoitdz6urPA94xRN9b4vWogx6liIhMak0bORaR5uLu\nD5jZnwL/ANxjZj8k7HM8lxBR3gmcS9ju7WLg383su4Qc5ROAlxP2Qb6wQfe/BH4X+J6Z/RToAR5z\n96+P7asSEZGJpmknx0kaQiHu/Zsva9g+BtGrscnAQPaX1TStIk1DyNIRpnVMA2Dzug0ALFp8ZHZf\nku4QUycG7bmcjin7vFC3L7LnnlMsJm1Kg/rOj8cKey/WSyTPzo/Btc+xTDLu/k9mtgr4ECEy/Fpg\nM3AfcHVsc5+ZnQv8DeHgjxJwL/A6Qt5yo8nx1YRDQN4E/L94zy2AJsciIlNM006ORaQ5uft/A6/f\nR5s7CPsZN7LXb6kxz/iy+CEiIlNY006OrRhemhf33j4tTbUetCBt8FZn1VxdyULYNt3KLXfbrMPC\n7lL/e09X6MWyaGyxFO6rDcQnVBtEey3XWRzqQLrVXNa+5OHZ2cvJLTRMI9vJIsTsvl279gBQiPe3\nTMu2mtOCPBEREZHBtCBPRERERCRq2shxsRRzc8lyh9O82/g7QS2/rVmSf9vgAI4sEFufewytbR0A\n7NzdA0DP7j1p3fS2kI88EKPJbdOycwcqvaGsQiUbc7rFXCzLHRpSjdHk5D9YwbLfayy9xgM/PKur\nxpzrLVs2A3Bke7bVXGuhjIiIiIhkFDkWEREREYk0ORYRERERiZo2rWIk8kvW023TYmExXxl/hUgO\nrrNcSkMpFvb1h1V3GzdtSus6lswAoKUc0ilW3nVPWjf/8OkAHDZ/cVo2zIF6aV26qC/3a02ydVuy\nvVu51JbW/eY3d4Zx9vcBsGRJttUcDbaWExEREZnKFDkWEREREYmaNnJci4vmark4bG6JXX0BybkZ\nhXhgR0spOzzEYsNSOTSyQYvhwucLFh4OwKaNWeR4yXHLALj1hhsB+NV//zKte/sf/uHgsZBFfpP1\nfvmFf5V4KEkSqS7uvVUrpXIY89qutWnZDT/+PgCvesWr4gvMHYpiOgREREREJE+RYxERERGRqGkj\nx1aoPzQDatXB2bz5o5QLMaJajrubeW6bt2rM8y3FaHIhF30l6bM3bOXW9VgWtd3wVDhS+o6bfwrA\nH7zjHWndrM4FAPRX+7LxkBziEQ8iyR3vnJwL0l8I42rP/V5TtVDW1x++/tH3v5vWzZ7RDsCJp68I\n/eReV8F0CIiIiIhIniLHIiIiIiKRJsciIiIiIlETp1WE1Aez7CWah5SCWi1su5ZfjmbF0K4UUy2q\ntb1TLiwu1qvl0h169uwG4Mm1IZ3isSceSet6e0PdKc89CYBlJzw3rRuoVOOY8ifxhbJKTNXIPYZS\nKdmuLdT1V7OT9cpxq7hbbg4L/556PBvDW956MQDTOqbH52b31ZRVISIiIjKIIsciMqGY2XvN7AEz\n6zEzN7P3j/eYRERk6mjayHGyH1p+0V26X5snUeWsqhi3Z0uCtY899Nu07pGHVgOwfftmALq3b0vr\nund3h/sHQkS2bWZrWmfl8IAzz35F+LqU2wIuhoULnvv9JFkg57VBYyH3VTFu5ebFclrzWFcXAP/z\nyx8DsHz58Wnd8uecCkClP4kYZ+FiBY5lojGzNwFfAO4BPg/0AXeO66BERGRKad7JsYhMRq9Mru6+\nflxHMgpWretmyaU/Ge9hyBC6PnnBeA9BRCYgpVWIyERyBEAzTIxFRGRyatrIcbK4rZbPnYifFopJ\nWoXVVxHX7LH46MVp3cx5HQDs6t4KQPeWzWndtqefAmD3th0AdMWvAVoXzARg1YNhgdzhRx2V1rXE\n9Ij8grxyKfznaEnSL3Ir5pL9msvFlvC8PT1p3S9/8SMAjll6HACvfc2b07pC3KPZC3unmeRP4BMZ\nT2Z2BfDR3Nfpm9PdLX59C/Am4G+A84EFwB+4+zXxnoXAR4ALCJPsbuBW4GPuvrLBMzuBK4E3APOA\nLuArwA+ANcDX3P2iUX2hIiIy4TXt5FhEJpWb4/Ui4BmESWu9OYT8413A9wiJ+BsBzOxo4DbCpPhG\n4FvAkcDvAheY2evd/cdJR2bWFtudSshv/ibQCVwOnDmqr0xERCaV5p0cx0Vt1LIT4YiL7tIT7hpE\nUZMt4KbNnJXWzZw7P9wXt3vzan92XzxxLtki7etf+VJa93T3RgAeXfMgAKWbWtK6F7/orMHjBMrl\n0P/0trCor6cvOz0vaZVEfu+47fq0bvO6J8MYZsVT9zx3gl8MQhcKhUFXGLwlnch4cvebgZvN7Bzg\nGe5+RYNmJwJfB97u7gN1df9AmBh/xN0/lhSa2ZeB/wK+ZmbPcPddserPCBPja4G3ePwGYGYfA+7e\nn7Gb2V5R6WjZ/vQjIiITg3KORWSy6Ac+VD8xNrPFwMuAx4FP5evc/Q5CFHkO8Lpc1dsIv3N+2HP5\nRe7+BGGXDBERmaKaNnKcHv5RyEWHLYkYJ9cs59bqrk52WEY1HthRrcRt1Aayuko1/JxuiYdsPPvk\nU9K6h793bWhfDfc9+siDad20jnYATj89OxgkeXZLa4gw1/JzgGIbALfe+ksA1jxwb1rVGaPcLS3h\nNX/vO99K6y7+oz8MdaVyfF0ZK+h3I5lUutz96Qblyf/pbnX3SoP6G4G3xnb/amYzgWOAJ9y9q0H7\n2/ZnUO6+olF5jCifuj99iYjI+NPsSEQmi6eGKO+M1w1D1CflSa7UzHjdOET7ocpFRGQK0ORYRCaL\nobZX6Y7XBUPUL6xrtyNeDx+i/VDlIiIyBTRtWgVx8Zzlfp4WkwPo4rZmtfwZdMmpdHEBX36XszTl\nIqZheG4hXzFZ6BYXtx37rGendW0/CqkQWzeHrd9qheyf+/5V94c2be1p2SkrTgKg1BKe01Gakdbd\neNMNANxx642hjWcLDc87/+UAHL44bD93x8235l5X6KsQx1yzvapEJrt74vWFZlZqsFjv3Hi9G8Dd\nd5jZo8ASM1vSILXihaM1sBMWdbJSB02IiEwqihyLyKTm7k8CvwCWAO/P15nZ6cBbgG3A93NV/0r4\n/vcJy23+bWZH1vchIiJTS/NGjuOPO7ds/j+QhkrjNb+VWVxYV6uG9TyWC6umXcQDO3LnE1BITueI\nUehZ87K/yJ506u8AcNstIdq7ffOW7L64ZdzKX2W7QFUqod/TTg8R5NX33JPW3XV7WIg3syMs1jvu\n+OekdSeechoApdaw6O5NFx2d1hWT8cWf/8l2dOG15ra5E5nc3gncDnzazF4G/Jpsn+MacLG778y1\n/xTwWsKhIseb2fWE3OU3ErZ+ey2gvQ5FRKYgRY5FZNJz90eB5xL2Oz4e+BDhFL3/BM5w9x/Wte8h\npFtcRchV/kD8+uPAJ2KzHYiIyJTTtJHjaowKb9+W/Xw7/MgjYmXMHe7tTesqA+FgD4/btnl+mzdL\nIsbFWLf37xTJMdWl3HHQZ70i5Bquuvc3AOzYuT2t690Zglg1z/4T3HdfaLfu8XDc9EP335XWleJw\nLLY/45wXp3VJNLhaCdHvYqGc1iX50dWYcF0YtJdb7rAQkQnA3c8Zotwalde1WQf8yX48azvw3viR\nMrM/jJ+uHmlfIiLSPBQ5FpEpycyOaFB2JPAXwADw471uEhGRpte0kWMRkX34rpmVgZXAdsKCvlcC\nHYST89aN49hERGScNO3kuFwMKQOVXXvSspt+8AMAOhfOBWDJ0cekddOnhW3T2jrC9muWX4sTP/Xk\nmktHyJbjxVSNYvbX37mHhW1Xz3/9GwG44YffTut29YWFf92bN6Vls2eGfh+673/Dayhl/3k8nvQ3\nfWYY53X/fl1a98YL3wTAtGntg8YUPh/8xwHP1eYW6YtMRV8Hfh94PWEx3i7gf4Avuvv3xnNgIiIy\nfpp2ciwiMhx3/zLw5fEeh4iITCxNOzlODv9YcNSitOzxR8JCt1u+HaKu/9WWrU6bvyBEeRcedRQA\nbdNnpXXTZoa6WfNmA9DRkdW1T+sAoLUlLILrH8hFnC1Eh495Vth27Z7bbkyrep+Kf7HN1s6xaVPY\n6q29pRWAOXNmpnWHHXYkAKe+4EwAvn711Wld9+anAeicuRTIDvwI/w5BIYkg5w83Uca5iIiIyCCa\nHomIiIiIRJoci4iIiIhETZtWUUtOuMtN/1/wipcBcMrzng/Ahie60rquh8KWpo898CgAW7b+Oq3b\ntOUJAEoxXaFjRpbucPiC+QC0lkMqxFObdqV1c+aHNAzvD4sC53VkC/l6p08Pw2tpS8t2dYd798TU\njGfMmpfWPeO4EwBYuDikV7zjnX+c1k2bFsZjhb0X2Fl6QF74h9j3brEiIiIiU5cixyIiIiIiUdNG\njhNWy1agDfT2AFCe1gLA0cuOS+uWPutYADyuWLvlhlvSunVrHwDgWUeHhXWbNj+V1m3dFBbD7enb\nHZ7n2dZxG9duA6AUT6ebf+wz0roVK0Ik+P7/XZWWzZwVfleZXZgGQP9A9jrWrH0cgOOWHQ/AvAUL\n07paPA3Q48l/ydeQRdAL8Tpo9zZt5SYiIiIyiCLHIiIiIiJR00eOPb93WYyeVqt9AAz0Vwa1BCi0\nhtzhyp4sAvzk4yFqe96Fvw/AybNfktYNxPBu97ZuAL7/9X9K6174ytiuEH4HKbZmv4ssXRa2XTvy\n2OekZesfCId/bNgUtnn77fosQj27pxrGFQ8PKZWzPeCS+G9yqEf+cA+PUeQ0goyIiIiIDEVzJRER\nERGRSJNjEREREZGoadMqPC7Eq3k1LUuyDQpxvZoXs63VPEk7qIVGz33+C9O6jTGtYmB32GrNp89I\n68qFYryG+zaufyyte/ypNQCc98Y3h7FUshV2AwP9ACw+4XfSsoXHLQfgf2/9BQAbNt6W1c0KJ/GV\nS+F5Vsh+r0lOA0xK8gvyiqXwnzhJLvFclkkBLciTycfMugDcfcn4jkRERJqRIsciIiIiIlETR46T\n8HCuLEaRvRau+bhpIUaAa4T7ps+anta9+d3vju1jZ7nt4ZJPZ8ydA8AFr74wresfCFvHVftDlLgW\nrwAFD8+pVnOR7UJYZHfqi14JwLNOWpENsBy2n/NSWDBYyG/DViglL3rQl6HPGB2P19w/R8NDQ0Rk\n9Kxa182SS38y3sMYka5PXjDeQxARmRAUORYRERERiZo2cpzIB1hrtfpDMrI4atHqSmq5nN4YaSYe\nwYztvT2c10JU+MSzzso9POkryTXORZzjGJJjneODAKgMhBsLMw7L6io98XHxMI9i9p8uyT/2+LtO\nEgUfPIi9eT4BWWQCsbAf4buAPwGOAbYA3wcuH6J9K/AB4C3AscAAcC9wlbt/Z4j+3wv8MbC0rv97\nQTnNIiJTVdNPjkVkUvo8YfK6AfgKUAFeA5wOtABpjpKZtQA/B84GHgS+BHQAbwC+bWYnu/tldf1/\niTDxXh/77wdeDZwGlOPzRERkCtLkWEQmFDN7AWFivAY4zd23xvLLgZuAhcBjuVs+SJgY/wx4tbsP\nxPZXAncBHzazH7v7HbH8TMLE+CHgdHffHssvA24Ajqjrf1/jXTlE1bKR9iEiIhPH1JocxxyLQroQ\nLZfSMCgVYfApc7W40C0tyaUjZKkJIfWiVs22a0ueZ/F+z22xliyey5/gl+w6V0izN7K+asWwIK9g\nSepEftVdMjJPbsyNPUn7SNI4tAhPJryL4/VjycQYwN17zezDhAly3tsJb/5LkolxbP+0mf01cDXw\nDuCOWPW2XP/bc+37Y//ZHooiIjLlTK3JsYhMBqfG6y0N6m4l5BMDYGYzCDnG69z9wQbtb4zXU3Jl\nyeeNJsF35vsfCXdf0ag8RpRPbVQnIiITV9NOjpMIaf5AjDRmGqOvlo8WW3FQI/O9I6zJFnBea7SQ\nzWJdlqqYRmnj1T0/liQanT3H04cnY2lJ6wqEsmKD7dfS0cSg2aAIdXJASAxHK3Isk0BnvG6sr3D3\nqpltadB2wxB9JeWzDrB/ERGZYrSVm4hMNN3xenh9hZkVgbkN2i4Yoq+Fde0AduxH/yIiMsU0beRY\nRCatuwnpCGcDj9bVnUnu+5a77zSzNcBSM3umuz9c1/7cXJ+JewipFS9s0P/zGMXviycs6mSlDtcQ\nEZlUmjZy7F4LaQy13Id7+CgUwoflPiJzx9xxaulHcr/XquEj9z+SD6+CVyng6Yd5DfOsH8PTj+Qz\nCqX0o1AMHxjxw9MPK1SxQjXshWw1zMh9OGaevb7csMwMM6NQKIQP2/tDZIK5Jl4vN7M5SaGZtQGf\naND+q4T/x3w6Rn6T9vOAv8i1Sfxrrv/OXPsW4OMHPXoREZnUrFkPgqh0r3cAzx3PnGbnxrzd3M9R\nsozkvf89rC5n2HN5wtkOFrXB1/xTkzzhBrtcFAYd5pHkGu+dF5yMIZnMWu73mmTHi2SnjEE50bFZ\ncjCINTgUxDrmKRFZJhQz+zvgPYSc4evI9jneBiwC+pNDOuKk9peESPD9wE8J+xz/LjAf+JS7/3ld\n//8I/BGwDvhu7P9VhPSLRUCfuy89yNewpb29fc7y5csPphsRkSlr9erV9PT0bHX3Q5ru1rSTYxGZ\nvHIn5L2LwSfYXUaDE+xiVPkSwgl5x5CdkPcld/9Wg/4LwPsIJ+QdXdf/k8Aadz/5IF9DH1BMxisy\nASV7cTfa6UVkIjgJqLp766F8qCbHIiKRmT2TcDjIte7+5oPsayUMvdWbyHjTe1QmuvF6jyrhVESm\nHDNbEKPH+bIOwrHVEKLIIiIyBWm3ChGZit4PvNnMbibkNS8AXgwsJhxD/e/jNzQRERlPmhyLyFT0\nC0Iu28uAOYQc5YeAvwM+78o3ExGZsjQ5FpEpx91/SdjhQkREZBDlHIuIiIiIRNqtQkREREQkUuRY\nRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS5FhE\nREREJNLkWERkBMxssZl91czWm1mfmXWZ2efNbPZ+9jMn3tcV+1kf+108VmOXqWE03qNmdrOZ+TAf\nbWP5GqR5mdkbzOwqM7vVzHbE99M3DrCvUfl+PJTSaHQiItLMzOwY4A5gPvBD4EHgNOB9wMvN7Ax3\n3zKCfubGfo4DbgSuBZYBFwMXmNnz3f3RsXkV0sxG6z2ac+UQ5QMHNVCZyj4CnATsAp4kfO/bb2Pw\nXt+LJsciIvv2ZcI34ve6+1VJoZl9FvgA8DHgnSPo5+OEifHn3P2SXD/vBb4Qn/PyURy3TB2j9R4F\nwN2vGO0BypT3AcKk+BHgbOCmA+xnVN/rjZi7H8z9IiJNzcyWAmuALuAYd6/l6mYAGwAD5rv77mH6\nmQZsAmrAQnffmasrxGcsic9Q9FhGbLTeo7H9zcDZ7m5jNmCZ8szsHMLk+Jvu/tb9uG/U3uvDUc6x\niMjwXhSv1+e/EQPECe7tQAfwvH3083ygHbg9PzGO/dSA6+OX5x70iGWqGa33aMrMLjSzS83sEjM7\n38xaR2+4Igds1N/rjWhyLCIyvOPj9aEh6h+O1+MOUT8i9cbivXUt8Angb4GfAo+b2RsObHgio+aQ\nfB/V5FhEZHid8do9RH1SPusQ9SNSbzTfWz8EXgUsJvylYxlhkjwL+LaZnX8Q4xQ5WIfk+6gW5ImI\nHJwkN/NgF3CMVj8i9Ub83nL3z9UV/Ra4zMzWA1cRFpX+bHSHJzJqRuX7qCLHIiLDSyIRnUPUz6xr\nN9b9iNQ7FO+tqwnbuJ0cFz6JjIdD8n1Uk2MRkeH9Nl6HymF7ZrwOlQM32v2I1Bvz95a79wLJQtJp\nB9qPyEE6JN9HNTkWERleshfny+KWa6kYQTsD6AHu3Ec/d8Z2Z9RH3mK/L6t7nshIjdZ7dEhmdjww\nmzBB3nyg/YgcpDF/r4MmxyIiw3L3NYRt1pYA76qrvpIQRfvX/J6aZrbMzAad/uTuu4Cvx/ZX1PXz\n7tj/z7XHseyv0XqPmtlSM1tU37+ZzQP+JX55rbvrlDwZU2ZWju/RY/LlB/JeP6Dn6xAQEZHhNTiu\ndDVwOmFP4oeAF+SPKzUzB6g/SKHB8dF3AcuB1wBPx37WjPXrkeYzGu9RM7uIkFt8C+Ggha3AUcAr\nCDmevwZe6u7bx/4VSbMxs9cCr41fLgDOAx4Fbo1lm939Q7HtEmAt8Ji7L6nrZ7/e6wc0Vk2ORUT2\nzcyOBP6KcLzzXMJJTD8ArnT3rXVtG06OY90c4KOEHxILgS2E1f9/6e5PjuVrkOZ2sO9RMzsR+CCw\nAjiCsLhpJ3A/8B3gH929f+xfiTQjM7uC8L1vKOlEeLjJcawf8Xv9gMaqybGIiIiISKCcYxERERGR\nSJNjEREREZFIk2MRERERkUiT4yGYWZeZuZmds5/3XRHvu2ZsRgZmdk58RtdYPUNERERkKtLkVxV3\nEwAAIABJREFUWEREREQk0uR49G0mHG+4YbwHIiIiIiL7pzTeA2g27v5F4IvjPQ4RERER2X+KHIuI\niIiIRJocj4CZHWVmV5vZE2bWa2ZrzewzZtbZoO2QC/JiuZvZEjNbbmZfi31WzOwHdW074zPWxmc+\nYWb/ZGaLx/ClioiIiExpmhzv27GE8+T/AJgFOLCEcMTmr81s4QH0eWbs8/8SzqsfyFfGPn8dn7Ek\nPnMW8A7gbuCYA3imiIiIiOyDJsf79hmgGzjT3WcA04DXEhbeHQt87QD6/DLwK+BEd58JdBAmwomv\nxb43A68BpsVnnwXsAP72wF6KiIiIiAxHk+N9awXOd/fbANy95u4/BN4Y619qZi/czz6fjn2uin26\nu68BMLMzgZfGdm909x+5ey22uxV4OdB2UK9IRERERBrS5HjfvuPuj9QXuvtNwB3xyzfsZ59fdPee\nIeqSvu6Mz6h/7iPAt/fzeSIiIiIyApoc79vNw9TdEq+n7mef/z1MXdLXLcO0Ga5ORERERA6QJsf7\ntm4EdYftZ5+bhqlL+lo/gueKiIiIyCjS5Pjg2AHeVx2n54qIiIjIMDQ53rcjhqlLtnEbLhK8v5K+\nRvJcERERERlFmhzv29kjqLt7FJ+X9HXWCJ4rIiIiIqNIk+N9u9DMltYXmtlZwBnxy38fxeclfT0/\nPqP+uUuBC0fxeSIiIiISaXK8b/3Az8zsBQBmVjCzVwHXxfpfuPvto/WwuJ/yL+KX15nZK82sEJ99\nBvCfQN9oPU9EREREMpoc79uHgNnA7Wa2E9gF/Iiwq8QjwNvG4Jlvi30fBvwHsCs++zbCMdIfHOZe\nERERETlAmhzv2yPAc4GvEo6RLgJdhCOcn+vuG0b7gbHP3wE+CzwWn9kN/DNhH+Q1o/1MEREREQFz\n9/Eeg4iIiIjIhKDIsYiIiIhIpMmxiIiIiEikybGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiI\nSKTJsYiIiIhIpMmxiIiIiEikybGIiIiISFQa7wGIiDQjM1sLzCQcNy8iIvtvCbDD3Y8+lA9t2snx\nDY/u3Otc7EIhBspt6PsKsa5a6UvLevfsCrcl93nWgcXCWq02+Bm5smo1XC1Xlwwu3z7tPh7pXSiV\n07r26TNi+2K45gdd80H3VS0bX8XDs/t7ekPT/oHstcbrK59z+DD/IiJygGa2t7fPWb58+ZzxHoiI\nyGS0evVqenp6Dvlzm3ZyLCKHlpktAdYCX3P3i8Z1MBND1/Lly+esXLlyvMchIjIprVixgrvvvrvr\nUD+3aSfHBQb2KitaEitNY7RpncXoq8XIbLHcmta1zu6IjfZO0S7UhaGT6G0jlVo2plqtGtrXcmOI\nEWZiH7WBrH1lRzcAe3bvBqB7+/a0rr83RLlbW1oAqFay+3ZuC+12bg/39+3anQ1oIDzvlc/5oyHH\nLCIiIjKVNO3kWERkvK1a182SS38y3sOQUdT1yQvGewgiMsa0W4WIiIiISNS0keNSsTxkXZr6UMvK\nqpVQ1t+XJH5n6Q6V/kpoHlMhKr170rpd27cCsDumO8ycMTOtq8Xn9PSE9gOV/qwupkz07sr6qvaF\n+oF4rebaE9NEenaH9rt27kxrktSOllL4z2mef2HhviR9w2tZXbFYRGQsxPzjTwIvAaYDq4Ar3P3H\nde1agQ8AbwGOJbzR7wWucvfvNOhzLfA14OPAXwPnAvOAF7n7zWa2FLgUeBGwCOgB1gG3A5e7+5a6\nPt8M/BFwMtAe+/8m8Gl370NERKacpp0ci8i4eQZwF/Ao8HVgDnAh8EMze4m73wRgZi3Az4GzgQeB\nLwEdwBuAb5vZye5+WYP+jwH+B3iIMJFtB3aY2ULgV4Tt034KfBdoA44Gfh/4IpBOjs3sn4G3A08C\n3wO2A88jTLpfbGYvdfe9Fy+IiEhTa9rJ8cobfgFAX29vWlbpj5HZuGAtv3AtadcTI8D5LdY8XTwX\nrrVcRLca+0yW5eWjsUnsOYvWZtHoQvJ5bv1eOd6bLBysVatZ+/iAQvwvNn/W9LSuNS4erPSE17On\nJ3vNfXERYLKtHLlt3mr5CLPI6DmHECW+Mikws38D/hP4M+CmWPxBwsT4Z8Crk4momV1JmFx/2Mx+\n7O531PX/QuAT9RNnM3sPYSL+fnf/Ql3dNHJ/KzKziwgT4+8Dv+fuPbm6K4CPAu8CBvXTiJkNtR3F\nsn3dKyIiE49yjkVktD0G/E2+wN1/DjwOnJYrfjvh18NL8hFad3+aEL0FeEeD/jcCVzYoT+y1Kaa7\n785PgIH3EVI43l5XTnz2FuD3hnmGiIg0qaaNHK+6/VYALLe1msX4blKWj8zWGBxFzUeAkyhycuBH\nMbd9Wynm8ibtLddn8uQ0WJvb9W0g3aYtF8m15BoPDcmFlZOc4da2dgAOX7QwrXtg1QPhk0rMPW5p\nS+v29PTFMSS/B2V91mqKHMuY+I27VxuUPwE8H8DMZhByjNe5+4MN2t4Yr6c0qLt3iHzgHxFykb9k\nZucRUjZuBx7w3B6LZtYBnARsBt5v1vAMnD5geaOKeu6+olF5jCifOpI+RERk4mjaybGIjJvtQ5QP\nkP21qjNeNwzRNimf1aDuqUY3uPtjZnYacAXwcuB1seoJM/uMu/9d/Ho24bfSwwjpEyIiIimlVYjI\neOiO1wVD1C+sa5c35Ek77r7a3S8E5gLPJexcUQC+YGZ/UNfnPe5uw33s1ysSEZGm0LSR41pfWJRm\n+R+j8S+rJRucJgHZorliKaZTVPPpB9VB7au5dIRaPNUuSasYtJAvzavY+2e5N1gMV60O/kt0Sznb\nji5JAWlvCwvxtmzekdZ1bw+LCKe3h5P8Kv3ZgryWclzkV9j7NQ9UG/3lW2TsuftOM1sDLDWzZ7r7\nw3VNzo3Xuw+w/wFgJbDSzO4A/gt4LfDP7r7LzO4Hnm1mc9x96wG+jH06YVEnK3VohIjIpKLIsYiM\nl68S0hs+bWZpkr+ZzQP+ItdmRMzsNDM7vEFVUrYnV/ZZoAX4qpntlbphZrPNTPnCIiJTUNNGjrt3\nhr+clkvZwrpki7RyIUZ5i7mXn/wFtRqujf5uW0gW6eUqs4hxuObW/aQR50Jx76htskAuH2neW24x\nYXKvh/aV/qyupSVEjEvx9bSUsz6T7euyMHbueTVFjmVcfQY4H3gNcK+Z/ZSwz/HvAvOBT7n7bfvR\n31uAd5nZLcAjwDbCnsivIiyw+3zS0N2/amYrgD8F1phZspvGHMK+yGcB/wK886BeoYiITDpNOzkW\nkYnN3fvN7KXAJYSJ7XvITsh7v7t/az+7/BbQCryAsEtEO+F0vGuBv3X3VXXPf5eZ/YwwAX4JYfHf\nVsIk+dPANw7wpYmIyCTWtJPjltYWIMu1BSjHyGpalo8Ak0Ry45ZsuaU4SXQ3qSsUcrnKnhzLHK42\nKOd4cHTYc7nHVmiwv1tyqnXMYy7k62K3fb1xB6vc8dgtpXJ8XnJYSXZbqTj4P3G5nH1dHDZqLbJ/\n3L2LQW/overPaVDWS9h+7eOj0P//EE7OG7F4nPWP99lQRESmDM2OREREREQiTY5FRERERKKmTavo\nKIW0ilIpl0YQUwySVIh8WkGa8RD/aNtSbsk6i2XJorhBi+hiX5VKJdzXkt2XLNZLTsPzXB5HkqIx\nSJKiEaty2Rvp4r5imhGS5U50tLeG11pI8jKyuuQ5/XFhXrWaX+Sn341ERERE8jQ7EhERERGJmjZy\nXIxR3nIuypt8mkRPC/kFcnXLfAqFvSOs6cEduYOzklbtHWE7tSRaDNkivXJLuL9Yy0Wq0/uz51ST\nBXVW22tMJcqxLIleZ89Jo+O1CvVqtVrdffp9SERERGQomimJiIiIiERNGzmePq0dGBx9TaK6yTHN\nxVz0Ndk2LYm0tuRylZN8X9Jjp7Oc3lKMxLbG6HBloH+vsZRjX22l7HeRgZij3J8/wjnmRPf0D8Tx\nZtu1JS8kiQC3tbelVckWbpXeShxD1mcSVU5yoZP859B/g7xnERERkSlMkWMRERERkUiTYxERERGR\nqGnTKtpiGkGxmM3/S/VpBLmj5JJmyRZu+YVrSWZGcr/ltlFrjakSpXjy3EA1S1tobQtbrBViOsae\nnVvSukohpEDM6piZlrXPmA3A5i07AOjesSutGxiIiwhb4+K+3GtJtpFLtqqzcm7LuPjCSum2clna\nR/4kPRERERFR5FhEREREJNW0keP29rAgr5RbkefVEGGdFhezTZ+eLWqrESKq3du3h6/7s0VtraW4\njVosKre0p3XzDjscgM450wEolLKo7Z6enlBWCPf3Tcuet3HTRgAeW9eVls2ZHSLGs2fOAqDNsgNF\n4lo7ZsyfC0BPLuq7Z314zpy4nVwht46PQmjoHiLaHa35hYYKHYuIiIjkKXIsIiIiIhI1beS4Vg3R\n1L5cYu0zjz0GgLlz5wDw4IO/Teu27tgJZFusFXJHKxfj1mhJXUsWAKbcF6O27YcBsPnpp9K6TRs3\nAdBaDhFd9yxPeMuOEMntq2WdPfn0tnDftnCdN2dWWjf/iEXhOYeHvOSH1zyZ1rWWB+cV54+PrnoW\nyYbBudT5o7VFRERERJFjEZlAzGyJmbmZXTPC9hfF9heN4hjOiX1eMVp9iojI5KHJsYiIiIhI1LR/\nVy/HVIOjlx6blj3j6KUAPP74EwDs7s/SD3b3hc8rlZCGsGPX9rSut68v1A2EtIrcQXd0dIaFeNt2\nhm3Xup7ckNZteips3dbfG/ruy51cN0BIgSi3d2Rj2N0LQLUSUjxWr3k0rZvRGT6fP+8IANpapmdj\niHkecTc5atVsEWLyeXoqoE7Fk+byfeBOYMO+Go6HVeu6WXLpT8Z7GBNe1ycvGO8hiIikmnZyLCLN\nz927ge7xHoeIiDSPpp0cd3aGRXeds+alZWu71gHw24dDFHbbtuxnand3iBT39Mboba6vmoVocm+s\ne87yZWndsuPC53f9+i4A7l+9Oq0rx4V4ra3TAOir9KV1O3aFvkqtM9KySn/YTm56R9jCbevObBS9\nldB+RkcYy4y2bCFfazHs3TYQ+88fROLxCJNGEePk8BCRicjMlgGfBM4CWoF7gL9y9+tzbS4C/gW4\n2N2vyZV3xU+fA1wBvA5YBHzM3a+IbQ4HPg68EpgJ/Bb4HPDYmL0oERGZ8Jp2ciwik9rRwH8Dq4B/\nBBYCFwI/M7O3uPu3R9BHC3AjMAe4HtgBrAUws7nAHcBS4Lb4sRD4h9h2xMxs5RBVy4YoFxGRCax5\nJ8eF8NJW//aRtGjjlq2Drv09e7LmMTJ72JxwyAal1rRu586QAzxnWsjzfebSpWndQ2vWAvDg2scB\n2DWQ5ftaPHTE94So9O6+3qzP7hC1Xnx4FtHtaAnP7O8P97W3Z3nFR8wN45rXGSLNRcuiwwO1eLR0\ncuBJbru2YiH2H3d06+vPotfVgawPkQnmLOAz7v5nSYGZfZEwYf4HM/uZu+/YRx8LgQeAs919d13d\nJwgT48+7+wcaPENERKYo7VYhIhNRN/BX+QJ3/zXwTWAW8H9G2M8H6yfGZlYGfg/YSUi5aPSMEXP3\nFY0+gAf3px8REZkYNDkWkYnobnff2aD85ng9ZQR99AL3NShfBnQAv4kL+oZ6hoiITEFNm1axa3dI\nmdi1Jwsabd8e0imqA2HhWyG37G7+nE4AOmaERXRPb8/+YluphVSE1o5Q98DD2cl6W7aFlIkde8Lz\ntuzIfta2xS3WOlrb49fZP3clfr5rx5Zs0DFtoxIX1M2bPTutWnLUYgBaiuE+y6dO1C22s1r2daFg\n8RraF4rZfVqQJxPYxiHKkyMoO0fQx9PudUdEDr53X88QEZEpSJFjEZmIDh+ifEG8jmT7tkYT4/y9\n+3qGiIhMQU0bOe6P26K5Zwd9eDzEo9YXorxtreW07rD58wGYPTdEax989OG0bv3mzQC0TAtbsu3Z\n1ZPW7dgat4CL26jt3L0rrStb+N1j7qzQZ+e07MCPttaWOKjs5/eePeGvyLPiNnSzZsxM62rV8DoG\nYrTbc4vpkgM+zAZHiSHbfi6JLre0tKR1fX3Z4jyRCeZUM5vRILXinHi95yD6fhDYA5xsZp0NUivO\n2fuWA3PCok5W6oALEZFJRZFjEZmIOoG/zBeY2XMJC+m6CSfjHRB3rxAW3c2gbkFe7hkiIjJFNW3k\nWEQmtf8C3mFmpwO3k+1zXAD+eATbuO3LZcCLgffHCXGyz/GFwE+BVx9k/yIiMkk17eR4e9xH2MjS\nKsxDKoJXQlpEeVp7Wrdk+bMBmDdnFgD3/O+qtG5bd0iV2LhxGwC1/mwh36L5IT2xpy/02T8zS4U4\ncmGoa41pDts2Z+t/SuWQAuHlLLXDSiHlYfas0EfnjOz0vCQtotFJd0lZkl6Rl6xHSlIoknQTgJ6e\nnr3ai0wQa4F3Ek7IeyfhhLy7CSfk/fxgO3f3zWZ2BuGEvFcBzyWckPcnQBeaHIuITFlNOzkWkcnH\n3bsAyxW9Zh/trwGuaVC+ZATPegp4+xDVNkS5iIg0uaadHFf6k23KsgVvbW1hQV1He7i2tGeR41Wr\nHgCgGiOrbeVpad2Jy04A4Bm7Q6S1VMr+2WbPDIvnyqWwbZuRbY9mHj4vxYV5Oxdna4t6+sNCuZpn\nP4NLpRBFnjUzRK/T0+2AStx+bqBBdDhdkBe/rtVqe9fFxXoD1WwhX35xnoiIiIhoQZ6IiIiISKp5\nI8cxIlsuZdHRWtzWrRaDyZWe3HZoO0M+cUsxRG8t909TIERw58ZDOTz3K0UlBmlLhRA5zkdte3pj\npNhjvnAp28ptZkfIK7bcTqyVGLUe6Avjslxgd0/ckq1cDuOq5p6THOZRSLZysywaPRD7LMXc5mRL\nOGgchRYRERGZyhQ5FhERERGJNDkWEREREYmaNq1iIKZV5NMckrV5yfZmtVxaQTH+nlBK0jBqWb5D\nJZ5Gl2zXRmv2O8X0GSFVwgvhedu6c9uvetw2LaZVkN9GLR5OV8gtii/HhX7VuP1cpTdL++iPqRP9\ncWFesngv97Koxc+KuQWD1MJYazF/o6W9LXvNNaVViIiIiOQpciwiIiIiEjVt5LgUo6+F3IK3JKJa\nSg/NyKLKHhfrlUox4jyQOzzEQoS1vxIWxVlu+7XCtLBIjxiFbS3mFvIVWkPf1dC+mts51WKkOR+9\nJh4W0pFsMZffaTUGnZPDPGq5LeoKpfB6BmKEu9KbHe6RLM5Luqrm7qvktnUTEREREUWORURERERS\nTRs5bm+JUeJ8/m2MolYHQrS2XMxCs2lqssUIckv2e0OpLURyewZC5Hj7juwwj46OEKWdNzsc9ewD\n2f5r1Rh9LreHsopnUdtkW7neSnZoSBLm7o3btlU9iypXqskhICHaW8j9XtNaChHqYtzmLXcOCS3x\n9Se51z17sqhyVTnHIiIiIoMociwiIiIiEmlyLCIiIiISNW1aRTEuuvNcKoOnuROhrGDFtM5i+kFv\nf1jwVvXcYr3kk7h9WndPb1rXsiekWLS0hdSJPbu2Z3UxtaMSj9Hz3CK/ae3TQpvpM7IxxGs5nma3\np2d3WleJa+eSxYR5yaI+i2kj6YI+oBh7TbaCa29rR0REREQaU+RYRCYUM+sys67xHoeIiExNTRs5\nTuzZsyf9vFBIDvqIL9uySO5ADA9bMUSAPbfNWSVGZvf0hujrlu3daV175ywAdvWG51gpi+z2xj5q\nccu49kK2WC/Zyo1cZDuJdifXjo6OtK61LUSTk+hyLXdf0kcSHc7t1kb/QCU28UFXyCLNIiIiIhIo\nciwiIiIiEjVt5DiJvs6cOTMt64/HNyfbmg3ktlEzC/8UxZhX3NqaHbM8EPOPLW591jkjyxNOtkbr\naw+5yvM6s+clB4sM1EIEua2Y2+YtRqPzx1snnyfR3XIuCo0VBtVVevtyVSEC3BLHnhwUUv85ZFvI\nhc4QkTG0al03Sy79yXgPY9R1ffKC8R6CiMiYUeRYRA45C95tZvebWa+ZrTOzL5pZ5zD3vNnMbjKz\nbfGe1Wb2ETNrHaL9MjO7xsyeMLM+M9toZv9mZsc3aHuNmbmZLTWz95jZfWbWY2Y3j+LLFhGRSaBp\nI8ciMqF9HngvsAH4CuHomtcApwMtpAemB2b2z8DbgSeB7wHbgecBfw282Mxe6u4DufYvj+3KwH8A\njwCLgdcBF5jZue5+d4NxfQE4E/gJ8FNAJ+WIiEwxTTs57okL8Qaq2c+2JJUhWZhXzp2e19aSpC2E\nNsXcwrVC/HxWWwhQLVkwP63bsGlLeF48Nc+mTc/uI6QwtCXpEaUsUN/fH1I6kpQLgFJM7UhO8OvL\nqiD2VasN3rYNskV6lbj4Lr9Yr6UcUzlie8+lVZjpDwdy6JnZCwgT4zXAae6+NZZfDtwELAQey7W/\niDAx/j7we+7ek6u7Avgo8C7CxBYzmw18C9gDnOXuD+TaPxv4H+Bq4NQGwzsVOMXd1+7H61k5RNWy\nkfYhIiITh2ZHInKoXRyvH0smxgDu3gt8uEH79wEDwNvzE+Por4EtwO/lyv4vMAv4aH5iHJ9xP/BP\nwClm9qwGz/rU/kyMRUSk+TRt5DhZpGbV/JZnMWoaiwqFLPpaiX89rcbt10qeLYZriYdytLaEKOyC\nttlp3a7eEDF+ujsc/jGwNUt/nN0R2s8It9NRnJbWlcvJISVZeLivN4l2xy3garlt15Jt6IqlQWPK\njzk58KOYOyikEiPhhRg5rnn2mmtV/cVYxkUSsb2lQd2thIkwAGbWAZwEbAbeP8T2g33A8tzXz4/X\nk2Jkud5x8boceKCu7q7hBt6Iu69oVB4jyo2i0yIiMoE17eRYRCasZNHdxvoKd6+a2ZZc0WxC5tBh\nhPSJkZgbr3+4j3bTG5Q9NcJniIhIk2reybHvvU9ZspVbW1tbbJK16esNR0InB2mUWrLI7LZ4RHRy\nGMjO3PHRG7aEA0HWbdkGwO61T6R15fiv2xoDuTNzW8AlQey+/mzdURLJLcTIbz6y3RLzlpPjn1tz\n4yvEaHASVZuWq2tP7otHSucjzv25rexEDqHkFJ3DgUfzFWZWJExu19W1vcfdRxqFTe45yd3v28+x\naYNDEZEpTjnHInKoJbtEnN2g7kxyv7S7+y7gfuDZZjZnhP3fmetLRERkvzRv5FhEJqprgHcAl5vZ\nD3O7VbQBn2jQ/rPAPwNfNbOL3H17vjLuTnF0bmu2fwEuBz5qZr9y97vq2hcIu1jcPIqvqaETFnWy\nUgdmiIhMKk07OU5OxhsYyBa8JakFHhe6FXMn0LUVQxC9Etv39GWpE8nCuFJMd8ifdDe/Izzn8M4Q\n1KrlFgwNxK3Vki3W9vRnp9UlKRrtuZP4kkVzxbjFXLGQD+zH0/PiKXolyy0YjO1L8TqjLRvfjNby\noLpyLq2iVGi4uElkTLn77WZ2FfAeYJWZXUe2z/E2wt7H+fZfNbMVwJ8Ca8zs58DjwBzgaOAswoT4\nnbH9FjN7A2HrtzvN7JeE6HMNOIqwYG8u0IaIiEidpp0ci8iE9j7gIcL+xH9M2I7t+8BlwL31jd39\nXWb2M8IE+CWErdq2EibJnwa+Udf+l2b2HOBDwHmEFIt+YD1wI/DdMXlVgy1ZvXo1K1Y03MxCRET2\nYfXq1QBLDvVzzRssXBMRkYNjZn1AkQaTfZEJIjmo5sFxHYXI0E4Cqu7eus+Wo0iRYxGRsbEKht4H\nWWS8Jac76j0qE9UwJ5COKe1WISIiIiISaXIsIiIiIhJpciwiIiIiEmlyLCIiIiISaXIsIiIiIhJp\nKzcRERERkUiRYxERERGRSJNjEREREZFIk2MRERERkUiTYxERERGRSJNjEREREZFIk2MRERERkUiT\nYxERERGRSJNjEREREZFIk2MRkREws8Vm9lUzW29mfWbWZWafN7PZ+9nPnHhfV+xnfex38ViNXaaG\n0XiPmtnNZubDfLSN5WuQ5mVmbzCzq8zsVjPbEd9P3zjAvkbl+/FQSqPRiYhIMzOzY4A7gPnAD4EH\ngdOA9wEvN7Mz3H3LCPqZG/s5DrgRuBZYBlwMXGBmz3f3R8fmVUgzG633aM6VQ5QPHNRAZSr7CHAS\nsAt4kvC9b7+NwXt9L5oci4js25cJ34jf6+5XJYVm9lngA8DHgHeOoJ+PEybGn3P3S3L9vBf4QnzO\ny0dx3DJ1jNZ7FAB3v2K0ByhT3gcIk+JHgLOBmw6wn1F9rzdi7n4w94uINDUzWwqsAbqAY9y9lqub\nAWwADJjv7ruH6WcasAmoAQvdfWeurhCfsSQ+Q9FjGbHReo/G9jcDZ7u7jdmAZcozs3MIk+Nvuvtb\n9+O+UXuvD0c5xyIiw3tRvF6f/0YMECe4twMdwPP20c/zgXbg9vzEOPZTA66PX5570COWqWa03qMp\nM7vQzC41s0vM7Hwzax294YocsFF/rzeiybGIyPCOj9eHhqh/OF6PO0T9iNQbi/fWtcAngL8Ffgo8\nbmZvOLDhiYyaQ/J9VJNjEZHhdcZr9xD1SfmsQ9SPSL3RfG/9EHgVsJjwl45lhEnyLODbZnb+QYxT\n5GAdku+jWpAnInJwktzMg13AMVr9iNQb8XvL3T9XV/Rb4DIzWw9cRVhU+rPRHZ7IqBmV76OKHIuI\nDC+JRHQOUT+zrt1Y9yNS71C8t64mbON2clz4JDIeDsn3UU2ORUSG99t4HSqH7ZnxOlQO3Gj3I1Jv\nzN9b7t4LJAtJpx1oPyIH6ZB8H9XkWERkeMlenC+LW66lYgTtDKAHuHMf/dwZ251RH3mL/b6s7nki\nIzVa79EhmdnxwGzCBHnzgfYjcpDG/L0OmhyLiAzL3dcQtllbAryrrvpKQhTtX/N7aprZMjMbdPqT\nu+8Cvh7bX1HXz7tj/z/XHseyv0brPWpmS81sUX3/ZjYP+Jf45bXurlPyZEyZWTm+R4/Jlx/Ie/2A\nnq9DQEREhtfguNLVwOmEPYkfAl6QP67UzByg/iCFBsdH3wUsB14DPB37WTPWr0eaz2iIhFpAAAAg\nAElEQVS8R83sIkJu8S2Egxa2AkcBryDkeP4aeKm7bx/7VyTNxsxeC7w2frkAOA94FLg1lm129w/F\ntkuAtcBj7r6krp/9eq8f0Fg1ORYR2TczOxL4K8LxznMJJzH9ALjS3bfWtW04OY51c4CPEn5ILAS2\nEFb//6W7PzmWr0Ga28G+R83sROCDwArgCMLipp3A/cB3gH909/6xfyXSjMzsCsL3vqGkE+HhJsex\nfsTv9QMaqybHIiIiIiKBco5FRERERCJNjkVEREREIk2OD5KZefxYMt5jEREREZGDo8mxiIiIiEik\nybGIiIiISKTJsYiIiIhIpMmxiIiIiEikyfE+mFnBzN5jZveaWY+ZbTKz/zCz54/g3lPM7Btm9oSZ\n9ZnZZjP7uZm9fh/3Fc3s/WZ2X+6ZPzazM2K9FgGKiIiIjAEdAjIMMysB1xGOdgUYAHYBs+LnFwLf\njXVHu3tX7t4/Av6e7BeQ7cAMoBi//gZwkbtX655ZJhyHeP4Qz3xTHNNezxQRERGRg6PI8fD+nDAx\nrgF/BnS6+2xgKXAD8NVGN5nZC8gmxtcBR8b7ZgGXAw68Ffhwg9s/QpgYV4H3AzPjvUuA/yScey8i\nIiIiY0CR4yGY2TRgPeFs+Svd/Yq6+lbgbuBZsSiN4prZL4EXAbcDZzeIDn+cMDHeBSxy9x2xfDrw\nFDANuNzdP153Xxn4FXBS/TNFRERE5OApcjy0lxEmxn3A5+or3b0P+Ex9uZnNAc6NX36ifmIc/X9A\nLzAdeEWu/DzCxLgX+LsGz6wAn92vVyEiIiIiI6bJ8dBOjdffuHv3EG1uaVB2CmCE1IlG9cT+VtY9\nJ7k3eeauIZ5565AjFhEREZGDosnx0A6L1/XDtFk3zH3dw0xwAZ6saw8wL143DHPfcOMRERERkYOg\nyfHYaT2Ae2wEbZQkLiIiIjJGNDke2qZ4PWKYNo3qkvvazeywBvWJxXXt858v3M9nioiIiMgo0OR4\naHfH68lmNnOINmc3KLuHLLp7boN6zKwTWFH3nOTe5JnTh3jmmUOUi4iIiMhB0uR4aD8HdhDSI95X\nX2lmLcAH68vdfStwU/zyz82s0b/xnwNthK3cfporvx7YHeve1eCZJeAD+/UqRERERGTENDkegrvv\nAT4Vv/yomV1iZu0A8djm7wNHDnH7XxAODjkVuNbMFsf7ppvZZcClsd0nkz2O4zN3km0b9zfx2Ork\nmUcRDhQ5enReoYiIiIjU0yEgwzjI46P/GPgy4RcQJxwfPZPs+OhvAm9rcEBIC/AfhH2WASrxmbPj\n5xcC34t1R7j7cDtbiIiIiMh+UOR4GO4+ALweeC9wH2FCXAV+Qjj57nvD3PuPwO8A/0bYmm060A38\nAvhdd39rowNC3L0fuICQsrGKEIGuEibMZ5GlbECYcIuIiIjIKFHkeJIxsxcDNwCPufuScR6OiIiI\nSFNR5Hjy+bN4/cW4jkJERESkCWlyPMGYWdHMrjOzl8ct35LyZ5vZdcB5hNzjvxu3QYqIiIg0KaVV\nTDBxEWAlV7QDKAEd8esa8Cfu/pVDPTYRERGRZqfJ8QRjZga8kxAhPhGYD5SBp4D/Aj7v7ncP3YOI\niIiIHChNjkVEREREIuUci4iIiIhEmhyLiIiIiESaHIuIiIiIRJoci4iIiIhEmhyLiIiIiESl8R6A\niEgzMrO1wEyga5yHIiIyWS0Bdrj70YfyoU07OX7nsW9xgGKhlpZtfnozAH0tIWBeMEvrrBg+r3k1\n1BWKad2M1nL4JB7N4bSmdXNL/QDs6N25131GbyhrmwnA9M75aV3SanffnrSse2c3AGULz+vpzepq\nxfA6Shb+k7XGNgDVgb4wrlrYlq9A9rqKhPsq8b90y0BbWuex7uqnr89uEJHRMrO9vX3O8uXL54z3\nQEREJqPVq1fT09NzyJ/btJPjU846CYDdC7N533/8/VUAtC84AoBC7tUbYZJrcTJ92IwZad3cBWFS\nu6cvzI779/SmdcccNReAjY+sAaBogzoFYP4xxwBw8stel1a1trcD8OBDq9KyH/zoujCuYuijb/vW\ntK6toyU8uy9M3vf0DmTPGajGZ4cHVivZAXvtrWEybDPC1X1XNryKsmpk8jGzLgB3XzK+I9mnruXL\nl89ZuXLleI9DRGRSWrFiBXfffXfXoX6uZkciIiIiIlHTRo5FRMbbqnXdLLn0J+M9DJGm1/XJC8Z7\nCNJEmnZy7ItCml/7CdPSso2tIYd3WmdISSgUstSEpZ0hzeHEpUsBWDQ7SxNsL4a0hSc2PQ1AZXeW\nx9wxN6QwTNsRUi3KlgXjB+LR3J2Lwz/z0c9ZnNa1dYRx9ZQ2Z4P+RcirqcXuZ8/N+poxI6RVuIey\nhx/tSut6LeQcz5gW+qzFNAuAakyP9vbQaUs5qyv2ZXnLIiIiIqK0ChGZgCx4t5ndb2a9ZrbOzL5o\nZp1DtG81s0vN7D4z22NmO8zsVjN74zD9v8/MHqjv38y6krxmERGZepo2clyIC9ZacrtHlIsh+mp9\nIdK6fOnhad15Jy4PZQuPBGDWjCxyXCyFqOvxO7YBsHFTtlCu8vRaAErlWaGgZ3c2iFqIJls1RGsH\ncjtTVOOvJb3dO7Ix94XnFGZMB+CJp9andR19oa/lz35O6POpXMS5O7zWSiXcb54tQqwOhDLvDa+5\nr5pFvVuqihzLhPV54L3ABuArhL1iXgOcDrRAXEELmFkL8HPgbOBB4EtAB/AG4NtmdrK7X1bX/5eA\nPwHWx/77gVcDpwFl0r1p9s3Mhlpxt2ykfYiIyMTRtJNjEZmczOwFhInxGuA0d98ayy8HbgIWAo/l\nbvkgYWL8M+DV7j4Q218J3AV82Mx+7O53xPIzCRPjh4DT3X17LL8MuAE4oq5/ERGZQpp2ctxaC4Gl\nOW1ZdLQ17us7z8L1JcuPSeuetXB2qOsMW551dE5P69rLIfo8N0Z0Z82eldb1L+gAoGdhyCd+cvW9\naV2pFvY+rraEf+YNW9eldW0tIRl4V9wfGWDWvBCtfmx9yG3u3pUGx/BaGMOq1Y8CUCu0pHWz54Wt\n5p5+amMYb0u2D3NbOUaRY8S4ktsDuZaLIotMIBfH68eSiTGAu/ea2YcJE+S8twMOXJJMjGP7p83s\nr4GrgXcAd8Sqt+X6355r3x/7v21/BuvuKxqVx4jyqfvTl4iIjD/lHIvIRJNMKG9pUHcrkE6AzWwG\ncCyw3t0fbND+xng9JVeWfN5oEnxnvn8REZl6NDkWkYkmWXS3sb7C3avAlgZtNwzRV1I+K1e2P/2L\niMgU07RpFd3VsIDNvS8tayuFlIKzTjgBgGMXZAvfW4ph/c2M2aGs1JIds2yxj0IpbM122PxsId+e\n2SEdo9YafgZPH8jSJNpbYtpCS9hibW3XfWlduRT+6fv7s3U/c+eHFI1H1oRjpKe35FJCSiGNYqAn\ntG/ryMbX3x9OvSu2hbJKLduujdhFqRjSMoq5oJjn24lMHN3xejjwaL7CzIrAXGBdXdsFQ/S1sK4d\nQLIKdiT9i4jIFNO0k2MRmbTuJqRWnE3d5BU4k9z3LXffaWZrgKVm9kx3f7iu/bm5PhP3EFIrXtig\n/+cxit8XT1jUyUodTiAiMqk07eS4tTVEgMuzsijvcc9cAsBzT3o2AEXLbaMWI7PllhC9LRWzyGx1\nIERrrRizUHLr2FrjFm6lQtharbNzdlrXUozbqMV/5upAtsCuXAhR6Jb/v717D470Ku88/n26JbXu\no9vMaC4ea8bG2MSUjU05dkjW3IxxWDZUAmFNsrVAZXcJrANxshsvCRt7k0A2kAAhBDZLHAqSjUni\nZJ1N7ABFbBYMLGDAxvYY2zPMTXMf3aVutVp99o/n9HvasjRXaS6t36dqqkfvefu8p6W31KcfPec5\nhZTZcvmlPr6WuNju2R170hiCR36LJe9zeiaVjOvs9DG/9MWeSrnzqZR6OTnm4+rt9NfTnCrbYTll\n1ch56dP4ArpfN7P76qpVtAIfWOT8u4HfAT5oZj8TUyMwswHgfXXn1HwGX8RX6388nt8CvH8FXo+I\niFxAGnZyLCIXphDCw2b2MeA24HEz+xtSneNRnp9f/CHgltj+qJndj9c5fhOwDvi9EMJX6/r/spn9\nCfDvgSfM7N7Y/+vx9Iv9POcjsIiIrCYKHYrI+ejd+OR4HPgPwK34Rh+vpm4DEPASbMBNwK/HQ7fh\n5dqeAd4SQvi1Rfr/ReB2YAp4B/AWvMbxTUA3KS9ZRERWmYaNHO8e97SDIw9/Pzt20cZ+ANav9XrF\nR4b3ZW3tBU9lyMUd9fKF9qytgi9cm5sdj49pp7ude3yHvN4eP7+zKy3ys6ov5Gtr6/J+qmnxXa3G\ncC6XfgRNfT6GtQObANhy8das7cAhT494ZscuAKaLaeHf0KW+EdfV1/yon7s/7Z43Menfh6m4Q16h\no+7zUDUgcj4KIQTgj+K/hYYWOb+Ep0ScVFpECKEKfDj+y5jZC4BOYPupjVhERBqFIscisuqY2aCZ\n5RYca8e3rQb4u7M/KhEROR80bOT46cO+sdaz46mC0xWxzFqOIpDKmwFMTXk0OB8X5o1PTmVtzzz7\nNAADa7zNqz25nTs8crx120UADPZ1pEFU/L23s9sX7VXnUxm1uVmP5FZD2rEub95/iNHroa1DWdvA\net8Fr3+tR7+/893Hs7ahoW0AzMzGyHRzW9b20ht+HIBvffWLABTLqXxbW92iQ5FV5j3ArWb2EJ7D\nPAi8CtiMb0P91+duaCIici417ORYROQ4vghcBbwG6MN3xXsa+EPgIzGtQ0REVqGGnRxPzHmEtGN9\nX3Zsa+tGAA7u3w1AKKcNQro7Pbqbi5tzPPZ42rDj3r/xINLNr3yZ9z2Vco4vvuRFAAxu8MhuKKXN\ntVpaPIIbsuyVFHFujjnNoS5y3Nwc/x8j2nPVtGC+OW5K0tHVDUBrW4oO55u8ryee9gj35VdclrUN\nxE1NvvX1WKqukMZgVWXVyOoUQvgS8KVzPQ4RETn/aHYkIiIiIhJpciwiIiIiEjVsWkWIi+YqIZVP\n6yh4asL0pJcwHejpydoGBz3lYqZYAqC3N7W97ubXALB2rR97/MFsPwEqTZ62cMklQwBMTB7M2vp7\nfLc8yzUDMBtSGkc+7k6Xs/QjqC2ez8XzW+sWDJbjLn35Oe9jbX/aiW/PHi9Jt3eXp1WsXb8xaxsb\n8fFs2rwFgK2DXVnbaF3JNxERERFR5FhEREREJNOwkeMqvtg8zKdFbTNFL6XW3OafCdo704YdU9O+\nWUal6gvqrnrRFVnbtos2AzAyMebP70jR1137PGo7PuHR6ObmtHlIrqngY5mP5dPqIsG5ptzzxlet\nbUASy8k15QupLUbAq3ERXUdbKsPWZN62Ya2/nn379mZthVbf8KQ47RHnH+5MG391tdSVnRMRERER\nRY5FRERERGoaN3Lc4lHXIqXs2K69owBcv8W3ZbZCZ9Y2Pu0bg3R2evS2s61++2i3ptn73DyUtnWe\nmPSNRSoVP6u7O0WjifnEIcQecumziOVi2ba6yHGo5Rw3ec5xS3P9Jh0eTS6XPUrcHM/xMftYq+Uy\nAIf3H87aSrOeV1yJm460kPKeN61bj4iIiIgkihyLiIiIiESaHIuIiIiIRA2bVoF5ukK+I73ElnI/\nAK0tvmtea9dA1pYL/jmhs8d3ujsyOp61FeNOevmCp1VsGxrK2kolT9tY0+0pGh0daREdxEV0cSNa\nq/t2z897WkUIdTvWxdSJUsmvl6vbUa8l7pDXXPZ0irGxSta2b58vIty95wgA05OpfF05LuCr1u3E\nVzM9U3zeMREREZHVTJFjEVn1zOwhMwvnehwiInLuNWzkuPXYMABbNqUFcldcfSUAfYMeOe7pW5e1\nNeX8W5GPi/Tm5lL0dWLSy7y1xMVwmwbTQrbZWY8c963x8m45UkR3Li6Cs7z3XSikRX7NcbFdLpd+\nBKWSL6ibKU4BMD45mrV1dnb79eb8nH11i+6e/MEeAEZGPRJslhbrERcDVmP4eq6aFgDOzaWxioiI\niEgDT45FRM61x4fHGbrjH8/1MGQZ7Prd153rIYjIWaK0ChG5oJjZdWb2OTMbNrNZMztgZl8ws5+t\nO+etZnavme00s6KZTZjZw2b28wv6GorpFDfGr0Pdv4fO7isTEZHzQcNGjlsP/BCArZenmsRr1r8Q\ngLaetQB0dPZlbbXFby0tnpJQKKT0g84uT2nI5X1Rm9V9pCjOeApEvuopFHOz81nbTNFTLioVT4WY\nn0wL4FriznVd3b1pzK2edjE27rWJi7NTWdt00VM7Ro55XeW9w4eytslpT/eYnvZr1++6V5nzazbH\n1I62lrTIr6lJn43kwmJm/w74BDAP/D3wDLAOeCnwTuCv4qmfAJ4E/i9wAOgHfhL4rJm9MITwvnje\nGHAX8Fbg4vj/ml0r+FJEROQ81bCTYxFpLGb2IuCPgQngJ0IITyxo31z35ZUhhB0L2luAB4A7zOyT\nIYThEMIYcKeZvRy4OIRw52mM65Elmi4/1b5EROTca9jJ8Xxxwh+r09mxXJtHa6uzHgEul9Kiu/Y2\nXyDXVvDHfF1QNcQyaNPTk/HrtJAtxKjwXNmjxMP792ZtO3f6e/PBwwe9z3yK2r74JdcBMDKRxtdZ\n8Kj1bIwYTxVTObm2tg7/T6zIlsu3ZG3TRY9aT8Vd/qyaoteDgx71Xtvpz58cP5a1WUjniVwAfhH/\nnfVbCyfGACGEfXX/37FIe9nMPg68EngV8JkVHKuIiFygGnZyLCIN5/r4+MCJTjSzLcCv4ZPgLUDb\nglM2LdegQgjXLjGGR4Brlus6IiJydjTs5LgSS6Qdni5nx5qavKxbU96jw0eOpZzetvjWWal4NLW1\nNZVDm41l3UozHjlubUrlUIvTHt3du9tznB977LtZWymWcpuamvGvSynnuKdvAwCj42kMxVi67SXX\n/AgAIZ8i1PmKh4xnSj6+yakUcc42Dcl5W39vR9b2Ezf4e/PMUS/9tv1oFlyDasP++KUx9cTH4eOd\nZGbbgG8CvcBXgC8A43ie8hDwb4HCUs8XEZHVTbMjEblQjMXHTcBTxznvdnwB3ttCCJ+ubzCzW/HJ\nsYiIyKJUrkBELhTfiI+3nOC8S+PjvYu03bjEc+YBzCy/RLuIiKwSDRs5zuViObNiWnRmM55iUe7y\ntAOjM2sbnfFUienZmDrRmt4jq8HTKqziqQzj5Yms7YnvfweAQ0eOADAxnna1Y95TIcZHPJ1ifDIt\nsPunf/gikMq9+TX9/L5+H9fFWweztmLRxzU24eeXZlKKxqb1vtNfd8HHvK43va4De56NY4gL8ep2\nyO3tbEXkAvIJ4B3A+8zs8yGEJ+sbzWxzXJS3Kx56OfB/6tpvBn5hib5rK1W3AD9crgFfuWkNj2jz\nCBGRC0rDTo5FpLGEEJ40s3cCnwS+a2b34XWO+/E6x5PAK/Byb28D/trM7sVzlK8EXovXQX7zIt1/\nCXgT8Ldmdj9QBHaHED67sq9KRETONw07Oe5p9YyR7ta0qG360H4Aeps9cpyVRwM62j3aGmJ5s7nZ\nFOWdnBwBYGLkAAA7tn8vazu03xe41UqrFYszaRAVj9KOjfqiuyOjqc/Zsl+npSWVZJureuQ4xGyX\nQmtaMzQ3PxeP+ULBqelUkq2jxftq6vAfZ5hNUeWnn/mBXyf21bymO42vZeECfpHzWwjhf5rZ48Cv\n4pHhNwBHgceAT8VzHjOzVwC/jW/80QQ8Cvw0nre82OT4U/gmIP8a+M/xOV8GNDkWEVllGnZyLCKN\nKYTwdeBnTnDO1/B6xouxRc6fB94b/4mIyCrWsJPjoU39APRvXpMdq4x6ObO2zdsAsPlU5m2+7OXQ\nWlo9D3d2PkWcy9OeY7xn504Ajhw6mrXlzL+Fs0WP7M5MphzifIwAF2N+cGtrygUm59eubS0N0Nrm\nY+3q7olfp8g2cZOR7jUead64MW19fSBGh6dH4mL++ZQv3dnukeLZJj82mk8l6kjpxyIiIiKCqlWI\niIiIiGQ0ORYRERERiRo2raK14OkHc6W0k9zk6A4AypdeBkBHYSBryzXFBWt5T3OYGjuUtR075Ivu\nRg4fBJ5bRs2qXjKuMu85CrmQUhpqO+K1xEyGtub07S7N+CI/KrPZsQ0DFwGQt1pKZF0KRCzB1tTi\nj/39KV3k2W/74rymWL0u1P1Yrcn7KDf796NUSKkaM5VU5k5EREREFDkWEREREck0bOR43yHflKPQ\nWs2OjU15Kba1u7cA8OKB67O29vZ2AGamPWL89PZvZW2Hhz1yfCT2WSmnxXq1CHU5Lpibm0vR2Gpc\n1NdaiCXa5tLmIYPdfuyiLVuzY2vWeGm1sRGPKq/bfFF6QeaR7XxTbbxpcd/6desBGN3vG5CMjqdy\ncjN5//xT6YkL+Fq6Up8hLR4UEREREUWORUREREQymhyLiIiIiEQNm1YxOuYL8dr7Ur3/+aqnPDy1\n/TsA9Pal3eKGtg0BcDguvtvx1KNZ23isH9yU975am1qzttZYPzgfvC3fltryeU/VyJunV3Q2p7F0\nd3maxNR0SrUYPbgHgJZYm3jk6GjW1tPndZtDrE1s+bSzXucaT5kYHY51jvPpx9rc630VY/3mynxK\n+5gP+mwkIiIiUk+zIxERERGRqGEjx6HkC/GmxqayY00Fj/KWR3ynvKe+/qWsbX70UgAK3b5g7bKt\nF2dtc+vXeZ9h3J8/k3a1q5T880U5VnAb3NSbtXV0ebm1XbuGAZidHM/apvMeAp6YSoviuuKOeKHs\nJeCKdefnYkk2ch59npmtW/jX4hHqYm0xYFtb1rb16msAODLpi/R27B/O2spNqeyciIiIiChyLCIi\nIiKSadjI8WLz/nLJo7RtBX/ZldJk1vbM978HQEev5+9Wc3U5vW0emc3legBoSlXU2LdnPwCzRY9Q\nT46laOz+YW87eszzilvrhtTS4hHgrlhCDoAY+a2VcmvuTTnH05WYr5zzTorTKapcqngUeq7L+9ow\ndGnW1jPgG520d3ne85HxkXS5UipzJyIiIiKKHIuIiIiIZDQ5FpFlYWZDZhbM7NPneiwiIiKnq2HT\nKqr5uPhuPmTHOmOZtUrVUwxKc7NZW1PZUxqmJr0EXK6Q0iqOmH+GWD/gO9a1t6Vv2+AmT8Po7vGF\nfKOjKW1hctJTLZqbLF6/kLXlYkm1UqmYHWsteHt7TPs4un9vGvuAj7mp3RfbjRw6nLUVZ32BYKXf\nx9J7yVDWlo8L+frior3N69dnbbt270dEREREEkWORURWyOPD4wzd8Y/nehgiInIKGjZyPFX2aGpx\npi46bL4Arb3Zo6lzlVQObe2Ab7LR37vWzy2khXWzlTkA2lp9cdvhIyniWiz7ornZWY9QVyppkVt3\nl2/AsX6dP69aTiXgjh4+AkBX55rsWL7g45qa8uj1ZDEturPY1lyNiwPrNhQJ1h5fsy843HP0WNZ2\nxdZL/JxZjzxv3rAhazt2OC34ExERERFFjkVkBcT843vM7KiZlczs22b2Lxc5r2Bmd5jZY2Y2Y2YT\nZvYVM/vZJfoMZvZpM7vMzD5nZofNrGpmL4/nbDOzPzGzZ82saGYjZvZ9M/ukmfUv0uetZvagmY3G\ncW43s98ws8LCc0VEZHVo2MgxTZ4znGtO73HVuH30bNEjwQfjxhgAHS2ey9vW4ZHf6mzanKM65+eV\nY+mzcn2ucrNHbdf0eJT46e3PpLZ87dGjvIfGU+m4sgdyWbcmvV/vHz0KwMxcjD53pO2tD037NedK\n5Xjd9LpaOjzXuKngec8HDxzN2i4bGvLXgL/27s6OrG2w/3lzBZHlcDHwTWAn8FmgD3gzcJ+ZvTqE\n8CCAmbUAnwduBJ4CPg60A28EPmdmV4cQ3rtI/5cA/w94GvgLoA2YMLMNwLeAbuB+4F6gFdgK/Bvg\nj4Dszypm9qfA24F9wN8CY8D1wG8BrzKzm0IIlWX6noiIyAWicSfHInKuvBy4M4RwV+2Amf0v4J+A\n/wQ8GA//Cj4xfgD4V7WJqJndhU+u/4uZ/UMI4WsL+v9x4AMLJ85mdhs+EX9PCOGjC9o6gGrd12/F\nJ8Z/B/xcCKFY13Yn8JvAu4Dn9LMYM3tkiabLT/RcERE5/yitQkSW227gt+sPhBA+D+wBrqs7/HYg\nALfXR2hDCIfx6C3ALyzS/yHgrkWO1xQXHgghTNdPgIF3AxXg7QuOE699DPi541xDREQaVMNGjivm\nqQzjxZQC0dHpnwX6unynu9HRsaxtJJZds4KXYqulQgC05v19u73bA0+l0ZSO0Rw8raIU0zByTenz\nxsBATHeIpdxmiuk9eHLK/797777sWOiMfbX6Ir2OdVuytn37fRFgKc4h2uq22xuMqSD93X694b27\nsrbDR73k2+CALzQsx8WFABs2DCKyAr4XQphf5Phe4AYAM+sCLgWGQwhPLXLuP8fHlyzS9mgIYXaR\n438PvB/4uJndjKdsPAw8GULIajqaWTtwFXAUeI+ZLdIVs8AVizUsFEK4drHjMaJ8zcn0ISIi54+G\nnRyLyDkztsTxCumvVbUyLQeWOLd2vGeRtoOLPSGEsNvMrgPuBF4L/HRs2mtmHwoh/GH8uhcwYC2e\nPiEiIpJp2Mlxe7u/9850pkjpxLQvVNsQ/P25OZ+irxYjxRtjNLU4lRbk7dm3B4DxKY/QVuZTubbN\nXb4IrlzxaHKrNWdt42MejZ6PUeXWQnvW1rLR3/M3bro4HVu/EYD7v/0YAMNHUym32Zz/qHLUSsyl\naFdpxq+zccDLtE32pvJww8MeOb6o319XyVJAr9CWFvyJnGW1m3upP19sWHBevbDIMW8IYTvwZjNr\nwqPDrwZuAz5qZtMhhD+t6/O7IQRFdkVE5DmUcywiZ10IYRLYAWwysxcscsor4uN3TrP/SgjhkRDC\nfwdujYffENumgCeAHzGzvtPp/2RduWkNu373dSt5CRERWWaaHIvIuXI3/ieQD8wZ2doAAAadSURB\nVJpZtuuOmQ0A76s756SY2XVmtn6RptqxmbpjfwC0AHeb2fNSN8ys18wUVRYRWYUaNq2iF093aK22\nZMfGRj3FYqoUPxPMp7SC2QlPh9hf9FrE03VpFVNF76PSElMb6hbwTE/7/0sVf5w6kD5vzMx7TeLZ\nTn9eoaM3a2vv9ffjcDSN4eBB3xkvt8ev11SXvtGV87lDbWO8nKW/LIfgY51d438tbquk1I7ilKdc\nHBnd7Qea0vMqcw3745cLw4eAW4CfAh41s/vxOsdvAtYBvxdC+Oop9PcW4F1m9mXgWWAUr4n8enyB\n3UdqJ4YQ7jaza4F3AjvMrFZNow+vi/wvgD8D3nFGr1BERC44mh2JyDkRQiib2U3A7fjE9jZ80d6j\neK3ivzzFLv8SKAA/hleJaAOGgXuA3w8hPL7g+u8yswfwCfCr8cV/I/gk+YPAn5/mS6sZ2r59O9de\nu2gxCxEROYHt27cDDJ3t61pdhSMREVkmZjYL5PHJvsj5qLZRzWLlFEXOB1cB8yGEwgnPXEaKHIuI\nrIzHYek6yCLnWm13R92jcr46zg6kK0oL8kREREREIk2ORUREREQiTY5FRERERCJNjkVEREREIk2O\nRUREREQilXITEREREYkUORYRERERiTQ5FhERERGJNDkWEREREYk0ORYRERERiTQ5FhERERGJNDkW\nEREREYk0ORYRERERiTQ5FhE5CWa22czuNrP9ZjZrZrvM7CNm1nuK/fTF5+2K/eyP/W5eqbHL6rAc\n96iZPWRm4Tj/WlfyNUjjMrM3mtnHzOwrZjYR76c/P82+luX38VKalqMTEZFGZmaXAF8D1gH3AU8B\n1wHvBl5rZi8LIRw7iX76Yz+XAf8M3ANcDrwNeJ2Z3RBC2Lkyr0Ia2XLdo3XuWuJ45YwGKqvZbwBX\nAVPAPvx33ylbgXv9eTQ5FhE5sT/GfxH/UgjhY7WDZvYHwC8DvwO84yT6eT8+Mf5wCOH2un5+Cfho\nvM5rl3Hcsnos1z0KQAjhzuUeoKx6v4xPip8FbgQePM1+lvVeX4y2jxYROQ4z2wbsAHYBl4QQqnVt\nXcABwIB1IYTp4/TTARwBqsCGEMJkXVsuXmMoXkPRYzlpy3WPxvMfAm4MIdiKDVhWPTN7OT45/osQ\nws+fwvOW7V4/HuUci4gc3yvj4xfqfxEDxAnuw0A7cP0J+rkBaAMerp8Yx36qwBfil6844xHLarNc\n92jGzN5sZneY2e1mdouZFZZvuCKnbdnv9cVociwicnwvjI9PL9H+THy87Cz1I7LQStxb9wAfAH4f\nuB/YY2ZvPL3hiSybs/J7VJNjEZHjWxMfx5dorx3vOUv9iCy0nPfWfcDrgc34XzouxyfJPcDnzOyW\nMxinyJk6K79HtSBPROTM1HIzz3QBx3L1I7LQSd9bIYQPLzj0A+C9ZrYf+Bi+qPSB5R2eyLJZlt+j\nihyLiBxfLRKxZon27gXnrXQ/IgudjXvrU3gZt6vjwieRc+Gs/B7V5FhE5Ph+EB+XymF7QXxcKgdu\nufsRWWjF760QQgmoLSTtON1+RM7QWfk9qsmxiMjx1WpxviaWXMvECNrLgCLwjRP084143ssWRt5i\nv69ZcD2Rk7Vc9+iSzOyFQC8+QT56uv2InKEVv9dBk2MRkeMKIezAy6wNAe9a0HwXHkX7TH1NTTO7\n3Myes/tTCGEK+Gw8/84F/fzH2P/nVeNYTtVy3aNmts3MNi3s38wGgD+LX94TQtAuebKizKw53qOX\n1B8/nXv9tK6vTUBERI5vke1KtwM/itckfhr4sfrtSs0sACzcSGGR7aO/CVwB/BRwOPazY6VfjzSe\n5bhHzeyteG7xl/GNFkaALcBP4jme3wZuCiGMrfwrkkZjZm8A3hC/HARuBnYCX4nHjoYQfjWeOwT8\nENgdQhha0M8p3eunNVZNjkVETszMLgL+G769cz++E9P/Bu4KIYwsOHfRyXFs6wN+E3+T2AAcw1f/\n/9cQwr6VfA3S2M70HjWzFwO/AlwLbMQXN00CTwB/BfyPEEJ55V+JNCIzuxP/3beUbCJ8vMlxbD/p\ne/20xqrJsYiIiIiIU86xiIiIiEikybGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiISKTJsYiI\niIhIpMmxiIiIiEikybGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiISKTJsYiIiIhIpMmxiIiI\niEikybGIiIiISKTJsYiIiIhIpMmxiIiIiEikybGIiIiISPT/AS0BDwr+Q21FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96958a6da0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
